{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import scipy.stats as sp\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "import json\n",
    "import copy\n",
    "import yaml\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import shutil \n",
    "import subprocess\n",
    "import traceback\n",
    "import time\n",
    "from datetime import datetime\n",
    "import humanize \n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.getLogger(\"arviz\").setLevel(logging.CRITICAL)\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def set_logging_level(level_name):\n",
    "    level = getattr(logging, level_name.upper(), logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # remove all existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    handler= logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "    \n",
    "\n",
    "def get_git_tag():\n",
    "        try:\n",
    "            tag = subprocess.check_output([\"git\", \"describe\", \"--tags\"], stderr=subprocess.DEVNULL).strip().decode()\n",
    "            return tag\n",
    "        except subprocess.CalledProcessError:\n",
    "            return \"No tag found\"\n",
    "        \n",
    "def create_directories(*paths):\n",
    "    \"\"\"Creates multiple directories if they don't exist.\"\"\"\n",
    "    for path in paths:\n",
    "        os.makedirs(path)\n",
    "\n",
    "def rel_to_root(path, root):\n",
    "    \"\"\"Return *path* relative to *root* so that HTML <img> links keep working\n",
    "    no matter where the whole experiment folder is moved.\"\"\"\n",
    "\n",
    "    return os.path.relpath(path, start=root)\n",
    "\n",
    "def adjust_dimension_of_kwargs(posterior_type, kwargs_dict_copy, kwargs_dict, target_dim, required_parameters):\n",
    "    \"\"\"\n",
    "    Adjusts only the required vector-like entries in the dictionary to match the given dimension.\n",
    "    For mixtures, this is applied recursively to each component.\n",
    "\n",
    "    Parameters:\n",
    "    - posterior_type: str (e.g., \"Normal\", \"MvNormal\", \"Mixture\")\n",
    "    - kwargs_dict: dict to be modified in-place\n",
    "    - target_dim: int, desired length of vector-like parameters\n",
    "    - required_parameters: dict mapping posterior types to required param keys\n",
    "    \"\"\"\n",
    "\n",
    "    if posterior_type == \"Mixture\":\n",
    "        # Recursive call for each component\n",
    "        component_types = kwargs_dict[\"component_types\"]\n",
    "        component_params = kwargs_dict[\"component_params\"]\n",
    "        component_params_copy = kwargs_dict_copy[\"component_params\"]\n",
    "\n",
    "        for i, comp_type in enumerate(component_types):\n",
    "            adjust_dimension_of_kwargs(\n",
    "                posterior_type=comp_type,\n",
    "                kwargs_dict_copy= component_params_copy[i],\n",
    "                kwargs_dict=component_params[i],\n",
    "                target_dim=target_dim,\n",
    "                required_parameters=required_parameters\n",
    "            )\n",
    "        return \n",
    "\n",
    "    # Get the required keys for the current posterior type\n",
    "    required_keys = required_parameters.get(posterior_type, [])\n",
    "\n",
    "    # Exclude parameters that do not depend on dimension\n",
    "    dimension_invariant_keys = {\"nu\"} \n",
    "    adaptable_keys = [k for k in required_keys if k not in dimension_invariant_keys]\n",
    "\n",
    "    for key in adaptable_keys:\n",
    "        # get the original paramter dims \n",
    "        value = kwargs_dict_copy.get(key)\n",
    "\n",
    "        # Skip missing keys\n",
    "        if value is None:\n",
    "            continue\n",
    "\n",
    "        if target_dim >= 2 and not isinstance(value, list):\n",
    "            raise ValueError(\n",
    "                f\"Parameter '{key}' must be a list when varying dimension ≥ 2 (got scalar: {value}).\"\n",
    "            )\n",
    "\n",
    "        if isinstance(value, list):\n",
    "            if all(isinstance(v, (int, float)) for v in value):\n",
    "                # e.g., mu: [1.0, 2.0, 3.0, 4.0] → [1.0, 2.0]\n",
    "                # Check length before trimming\n",
    "                if len(value) < target_dim:\n",
    "                    raise ValueError(f\"'{key}' too short: expected ≥{target_dim}, got {len(value)}\")\n",
    "                kwargs_dict[key] = value[:target_dim]\n",
    "            elif all(isinstance(v, list) and all(isinstance(x, (int, float)) for x in v) for v in value):\n",
    "                # e.g., cov: 5x5 matrix → 3x3 matrix\n",
    "                # Check matrix size before trimming\n",
    "                if len(value) < target_dim or any(len(row) < target_dim for row in value[:target_dim]):\n",
    "                    raise ValueError(f\"'{key}' matrix too small for target_dim={target_dim}\")\n",
    "                trimmed_matrix = [row[:target_dim] for row in value[:target_dim]]\n",
    "                kwargs_dict[key] = trimmed_matrix\n",
    "\n",
    "\n",
    "def adjust_circle_layout(num_components, component_type, radius, cov, weight_type):\n",
    "    mus = [\n",
    "        [\n",
    "            radius * np.cos(2 * np.pi * k / num_components),\n",
    "            radius * np.sin(2 * np.pi * k / num_components),\n",
    "        ]\n",
    "        for k in range(num_components)\n",
    "    ]\n",
    "\n",
    "    if weight_type == \"equal\":\n",
    "        weights = [1.0 / num_components] * num_components\n",
    "\n",
    "    component_params = [{\"mu\": mu, \"cov\": cov} for mu in mus]\n",
    "\n",
    "    component_types = [component_type] * num_components\n",
    "\n",
    "    return component_params, component_types, weights\n",
    "\n",
    "\n",
    "def get_scalar_rhat_and_ess(trace):\n",
    "    posterior_vars = [v for v in trace.posterior.data_vars if v.startswith(\"posterior\")]\n",
    "    if not posterior_vars:\n",
    "        raise ValueError(\"No posterior variables found.\")\n",
    "    return (\n",
    "        az.rhat(trace, var_names=posterior_vars).to_array().max().item(),\n",
    "        az.ess(trace, var_names=posterior_vars).to_array().min().item()\n",
    "    )\n",
    "\n",
    "def parse_mu_entry(mu_entry):\n",
    "    if not isinstance(mu_entry, str):\n",
    "        return mu_entry\n",
    "\n",
    "    if mu_entry.startswith(\"ZERO_\"):\n",
    "        n = int(mu_entry.split(\"_\")[1])\n",
    "        return [0.0] * n\n",
    "\n",
    "    raise ValueError(f\"Unknown mu specifier: {mu_entry}\")\n",
    "\n",
    "\n",
    "def parse_cov_entry(cov_entry):\n",
    "\n",
    "    if not isinstance(cov_entry, str):\n",
    "        return cov_entry\n",
    "    \n",
    "    if cov_entry.startswith(\"ID_\"):\n",
    "        n = int(cov_entry.split(\"_\")[1])\n",
    "        return np.eye(n).tolist()\n",
    "    \n",
    "    if cov_entry.startswith(\"HIGH_\"):\n",
    "            n = int(cov_entry.split(\"_\")[1])\n",
    "            rho = 0.9\n",
    "            cov = np.full((n, n), rho)\n",
    "            np.fill_diagonal(cov, 1.0)\n",
    "            return cov.tolist()\n",
    "\n",
    "    raise ValueError(f\"Unknown covariance specifier: {cov_entry}\")\n",
    "    \n",
    "def extract_varying_value_from_json(png_path):\n",
    "    json_path = png_path.replace(\".png\", \".json\")\n",
    "   \n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        v = data.get(\"varying_value\")\n",
    "\n",
    "        if isinstance(v, (int, float)):\n",
    "            return v\n",
    "        if isinstance(v, (list, tuple)) and len(v) == 1:\n",
    "            return v[0]\n",
    "        return tuple(v) if isinstance(v, (list, tuple)) else str(v)\n",
    "\n",
    " \n",
    "def build_correlation_cov_matrix(dim, rho):\n",
    "    cov = np.full((dim, dim), rho)\n",
    "    np.fill_diagonal(cov, 1.0)\n",
    "    return cov.tolist() \n",
    "\n",
    "def load_config_file(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "\n",
    "    group_name = data[\"group_name\"]\n",
    "    configs = data[\"configs\"]\n",
    " \n",
    "    for cfg in configs:\n",
    "\n",
    "        if cfg.get(\"posterior_type\") == \"Circle\":\n",
    "            num_components = cfg[\"component_number\"]\n",
    "            radius = cfg[\"radius\"]\n",
    "            cov = cfg.get(\"cov\", [[1.0, 0.0], [0.0, 1.0]])\n",
    "            weight_type = cfg.get(\"weights\", \"equal\")\n",
    "            component_type = cfg.get(\"component_type\", \"MvNormal\")\n",
    "\n",
    "            component_params, _, weights = adjust_circle_layout(num_components, component_type, radius, cov, weight_type)\n",
    "\n",
    "            cfg[\"posterior_type\"] = \"Mixture\"\n",
    "            cfg[\"component_params\"] = component_params\n",
    "            cfg[\"component_types\"] = [component_type] * num_components\n",
    "            cfg[\"weights\"] = weights\n",
    "\n",
    "            cfg[\"radius\"] = radius\n",
    "            cfg[\"component_number\"] = num_components\n",
    "            cfg[\"component_type\"] = component_type\n",
    "            cfg[\"cov\"] = cov\n",
    "            cfg[\"weight_type\"] = weight_type \n",
    "\n",
    "            print(f\"Transformed Circle config '{cfg['config_descr']}' into Mixture with {len(cfg['component_params'])} components\")\n",
    "            print(\"First few component means:\")\n",
    "            for i, cp in enumerate(cfg[\"component_params\"][:3]):\n",
    "                print(f\"  Component {i}: mu = {cp['mu']}, cov = {cp['cov']}\")\n",
    "        \n",
    "        #Top-level convert mu/loc (for SinglePosterior)\n",
    "        for key in (\"mu\", \"loc\"):\n",
    "            if key in cfg:\n",
    "                cfg[key] = parse_mu_entry(cfg[key])\n",
    "\n",
    "        # Top-level convert cov/scale (for SinglePosterior)\n",
    "        for key in (\"cov\", \"scale\"):\n",
    "            if key in cfg:\n",
    "                cfg[key] = parse_cov_entry(cfg[key])\n",
    "                \n",
    "        # cov/scale inside component_params (for MixturePosterior)\n",
    "        if \"component_params\" in cfg:\n",
    "            for component in cfg[\"component_params\"]:\n",
    "                for key in (\"cov\", \"scale\"):\n",
    "                    if key in component:\n",
    "                        component[key] = parse_cov_entry(component[key])\n",
    "\n",
    "        if \"component_params\" in cfg:\n",
    "            for component in cfg[\"component_params\"]:\n",
    "                for key in (\"mu\", \"loc\"):\n",
    "                    if key in component:\n",
    "                        component[key] = parse_mu_entry(component[key])\n",
    "\n",
    "        if \"varying_values\" in cfg:\n",
    "            cfg[\"varying_values\"] = [\n",
    "                tuple(v) if isinstance(v, list) else v\n",
    "                for v in cfg[\"varying_values\"]\n",
    "            ]\n",
    "    return group_name, configs\n",
    "\n",
    "def load_experiment_settings(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "    \n",
    "def get_experiment_paths(group_names, base_dir):\n",
    "    return [os.path.join(base_dir, f\"{name}.yaml\") for name in group_names]\n",
    "\n",
    "def load_default_values(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)[\"defaults\"]\n",
    "\n",
    "def apply_defaults_to_config(config, defaults):\n",
    "    for key, value in defaults.items():\n",
    "        if key not in config:\n",
    "            config[key] = value\n",
    "    return config\n",
    "\n",
    "def safe_json_dump(obj, path):\n",
    "    def convert_numpy(o):\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "        elif isinstance(o, np.generic):\n",
    "            return o.item()\n",
    "        return o\n",
    "\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f, indent=4, default=convert_numpy)\n",
    "\n",
    "def ensure_2d(arr):\n",
    "    \"\"\"Ensures array shape is (N, d), even if 1D.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr[:, np.newaxis]\n",
    "    else:\n",
    "        return arr.reshape(-1, arr.shape[-1])\n",
    "\n",
    "def save_adjusted_posterior_config(posterior_kwargs, folder, dim_value):\n",
    "    json_path = os.path.join(folder, f\"posterior_config_dim_{dim_value}.json\")\n",
    "    safe_json_dump(posterior_kwargs, json_path)\n",
    "\n",
    "    \n",
    "def get_uniform_prior_bounds(means_array, expansion_factor=0.25, unimodal_init_margin=None):\n",
    "    \n",
    "    min_mode = np.min(means_array, axis=0)\n",
    "    max_mode = np.max(means_array, axis=0)\n",
    "\n",
    "    if len(means_array) == 1 and unimodal_init_margin is not None:\n",
    "        # For unimodal: use a fixed margin\n",
    "        border = unimodal_init_margin\n",
    "        low = min_mode - border\n",
    "        high = max_mode + border\n",
    "    else:\n",
    "        # For multimodal: compute bounding box\n",
    "        diff = (max_mode - min_mode)\n",
    "        min_margin = np.where(diff > 40, diff, 40.0)\n",
    "        border = expansion_factor * min_margin\n",
    "        low = min_mode - border\n",
    "        high = max_mode + border\n",
    " \n",
    "    return low, high, min_mode, max_mode, border\n",
    "\n",
    "def get_logp_func(weights, components):\n",
    "    def logp_func(x):\n",
    "        logps = [pt.log(w) + pm.logp(comp, x).sum() for w, comp in zip(weights, components)]\n",
    "        return pm.math.logsumexp(pt.stack(logps))\n",
    "    return logp_func\n",
    "\n",
    "\n",
    "def get_posterior_dim(posterior_type, params):\n",
    "    \"\"\"\n",
    "    Robustly determines the dimensionality of a posterior from its parameters.\n",
    "    \"\"\"\n",
    "    if posterior_type == \"Mixture\":\n",
    "        # Check only the first component (assuming all have same dimension)\n",
    "        comp_type = params[\"component_types\"][0]\n",
    "        comp_params = params[\"component_params\"][0]\n",
    "        return get_posterior_dim(comp_type, comp_params)\n",
    "\n",
    "    if \"mu\" in params:\n",
    "        mu = np.array(params[\"mu\"])\n",
    "        return mu.shape[0] if mu.ndim > 0 else 1\n",
    "    elif \"loc\" in params:\n",
    "        loc = np.array(params[\"loc\"])\n",
    "        return loc.shape[0] if loc.ndim > 0 else 1\n",
    "    elif posterior_type == \"Cauchy\" and \"alpha\" in params:\n",
    "        alpha = np.array(params[\"alpha\"])\n",
    "        return alpha.shape[0] if alpha.ndim > 0 else 1\n",
    "    elif posterior_type == \"Beta\":\n",
    "        a = np.array(params[\"a\"])\n",
    "        return a.shape[0] if a.ndim > 0 else 1\n",
    "    elif (posterior_type == \"MvNormal\" or posterior_type== \"MvStudentT\")and \"mu\" in params:\n",
    "        return len(params[\"mu\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine dimensionality for posterior type '{posterior_type}' with parameters: {params}\")\n",
    "    \n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"Checks if the config correctly defines one varying attribute and all other attributes are fixed.\"\"\"\n",
    "    \n",
    "    REQUIRED_ATTRIBUTES = {\n",
    "    \"config_descr\",\n",
    "    \"posterior_type\",\n",
    "    \"runs\",\n",
    "    \"num_samples\",\n",
    "    \"num_chains\",\n",
    "    \"varying_attribute\",\n",
    "    \"varying_values\",\n",
    "    }\n",
    "\n",
    "    # Posterior-specific required attributes\n",
    "    POSTERIOR_ATTRIBUTES = {\n",
    "        \"Cauchy\": {\"alpha\", \"beta\"},\n",
    "        \"Beta\": {\"a\", \"b\"},\n",
    "        \"Normal\": {\"mu\", \"sigma\"},\n",
    "        \"SkewNormal\": {\"mu\", \"sigma\", \"alpha\"},\n",
    "        \"StudentT\": {\"nu\", \"mu\", \"sigma\"},\n",
    "        \"Laplace\": {\"mu\", \"b\"},\n",
    "        \"SkewStudentT\": {\"a\", \"b\", \"mu\", \"sigma\"},\n",
    "        \"Mixture\": {\"component_types\", \"component_params\", \"weights\"},\n",
    "        \"MvNormal\": {\"mu\", \"cov\"},\n",
    "        \"MvStudentT\": {\"nu\", \"mu\", \"scale\"},\n",
    "        \"Custom\": {\"logp_func\"}\n",
    "    }\n",
    "\n",
    "    OPTIONAL_ATTRIBUTES = {\"base_random_seed\", \"init_scheme\", \"varying_component\", \"dimension\", \"correlation\", \"circle_radius\", \"circle_modes\"}\n",
    "\n",
    "    if \"config_descr\" not in config:\n",
    "        raise ValueError(\"Config is missing 'config_descr'.\")\n",
    "    \n",
    "    config_descr = config[\"config_descr\"]\n",
    "\n",
    "    if \"varying_attribute\" not in config:\n",
    "        raise ValueError(f\"Config '{config_descr}' is missing 'varying_attribute'.\")\n",
    "    \n",
    "    varying_attr = config[\"varying_attribute\"]\n",
    "\n",
    "    # Ensure all required attributes are present\n",
    "    missing_attrs = REQUIRED_ATTRIBUTES - config.keys() - {varying_attr}\n",
    "\n",
    "    if missing_attrs:\n",
    "        raise ValueError(f\"Config '{config_descr}' is missing required attributes: {missing_attrs}.\")\n",
    "    \n",
    "    posterior_type = config[\"posterior_type\"]\n",
    "\n",
    "    if posterior_type not in POSTERIOR_ATTRIBUTES:\n",
    "        raise ValueError(f\"Config '{config_descr}' has an invalid 'posterior_type': '{posterior_type}'.\")\n",
    "\n",
    "    if posterior_type == \"Mixture\" and \"varying_component\" in config:\n",
    "        varying_index = config[\"varying_component\"]\n",
    "        varying_component = config[\"component_types\"][varying_index]\n",
    "        all_valid_attributes = REQUIRED_ATTRIBUTES.union(POSTERIOR_ATTRIBUTES[posterior_type], POSTERIOR_ATTRIBUTES[varying_component], OPTIONAL_ATTRIBUTES)\n",
    "        \n",
    "    else:\n",
    "        # Ensure varying_attribute is a recognized attribute\n",
    "        all_valid_attributes = REQUIRED_ATTRIBUTES.union(POSTERIOR_ATTRIBUTES[posterior_type], OPTIONAL_ATTRIBUTES)\n",
    "\n",
    "    if varying_attr not in all_valid_attributes:\n",
    "        raise ValueError(f\"Config '{config_descr}' has an invalid 'varying_attribute': '{varying_attr}'.\")\n",
    "    \n",
    "    if varying_attr == \"dimension\":\n",
    "        max_dim = max(config[\"varying_values\"])\n",
    "\n",
    "        for key in [\"mu\", \"sigma\", \"cov\", \"scale\"]: \n",
    "            val = config.get(key)\n",
    "\n",
    "            if val is None:\n",
    "                continue\n",
    "\n",
    "            if max_dim >= 2 and not isinstance(val, list):\n",
    "                raise ValueError(\n",
    "                    f\"Parameter '{key}' in config '{config['config_descr']}' must be a list when varying dimension ≥ 2 (got scalar: {val}).\"\n",
    "                )\n",
    "\n",
    "            if isinstance(val, list) and len(val) < max_dim:\n",
    "                raise ValueError(f\"Parameter '{key}' in config '{config['config_descr']}' is too short for max dimension {max_dim}\")\n",
    "\n",
    "        \n",
    "    if posterior_type == \"Mixture\" and varying_attr not in (\"num_samples\", \"num_chains\", \"init_scheme\", \"weights\", \"dimension\", \"correlation\", \"circle_radius\", \"circle_modes\"):\n",
    "        if \"varying_component\" not in config:\n",
    "            raise ValueError(\n",
    "                f\"Config '{config_descr}' must have 'varying_component' defined \"\n",
    "                f\"when varying '{varying_attr}' for a Mixture.\"\n",
    "            )\n",
    "        \n",
    "    vc = config.get(\"varying_component\")    \n",
    "    if vc is not None and not (0 <= vc < len(config[\"component_types\"])):\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_descr}' has invalid 'varying_component' index {vc}, \"\n",
    "            f\"but 'component_types' has length {len(config['component_types'])}.\"\n",
    "        )\n",
    "    \n",
    "    VALID_INIT_SCHEMES = {\"equal_per_mode\",\"all_in_middle\", \"all_near_mode\", \"thesis_scheme\", \"None\"} \n",
    "\n",
    "    if \"init_scheme\" in config:\n",
    "        if config[\"init_scheme\"] not in VALID_INIT_SCHEMES and not config[\"init_scheme\"].startswith(\"all_near_mode_\"):\n",
    "            raise ValueError(\n",
    "                f\"Config '{config_descr}' has invalid 'init_scheme': \"\n",
    "                f\"'{config['init_scheme']}'. Must be one of {VALID_INIT_SCHEMES} \"\n",
    "                \"or 'all_near_mode_<int>'.\"\n",
    "            )\n",
    "\n",
    "\n",
    "def generate_html_report(experiment_root_folder, report_pngs_folder, experiments, output_path):\n",
    "    \"\"\"\n",
    "    Generates a single HTML report for the entire experiment (all groups and configs).\n",
    "    \"\"\"\n",
    "\n",
    "    template_path = \".\"  \n",
    "    env = Environment(loader=FileSystemLoader(template_path))\n",
    "    template = env.get_template(\"report.html\")\n",
    "\n",
    "    def rel(p): \n",
    "        return os.path.relpath(p, start=experiment_root_folder)\n",
    "    \n",
    "    def collect_metric_pngs(base, metrics):\n",
    "        out = {}\n",
    "        for m in metrics:\n",
    "            full_p = os.path.join(base, f\"{m}_global_plot_shaded.png\")\n",
    "            if os.path.exists(full_p):\n",
    "                out[m] = rel(full_p)\n",
    "        return out\n",
    "\n",
    "    def collect_glass_pngs(base): \n",
    "        out = {}\n",
    "        for k in (\"ws\", \"mmd\"):\n",
    "            full_p = os.path.join(base, f\"glass_delta_{k}.png\")\n",
    "            if os.path.exists(full_p):\n",
    "                out[k] = rel(full_p)\n",
    "        return out\n",
    "\n",
    "    metrics = [\"wasserstein_distance\", \"mmd_rff\", \"r_hat\", \"ess\", \"runtime\"]\n",
    "\n",
    "    groups_data = []\n",
    "\n",
    "    for group_name, configs in experiments:\n",
    "        config_entries = []\n",
    "\n",
    "        for config in configs:\n",
    "            config_descr = config[\"config_descr\"]\n",
    "            \n",
    "            png_base = os.path.join(experiment_root_folder, \"results\", \"z_html_pngs\", group_name, config_descr)\n",
    "            pooled_png_base = os.path.join(png_base, \"pooled_global_plots\")\n",
    "            chain_png_base = os.path.join(png_base, \"chain_global_plots\")\n",
    "\n",
    "            # Glob all KDE plots for this config\n",
    "            kde_path = os.path.join(report_pngs_folder, group_name, config_descr,\"IID_KDE_and_Histograms\", \"iid_hist_kde_*.png\")\n",
    "            kde_plots = sorted(glob(kde_path), key=extract_varying_value_from_json)\n",
    "\n",
    "            # Turn absolute paths into relative paths for <img src=...> in HTML\n",
    "            rel_kde_paths = [rel(p) for p in kde_plots]\n",
    "\n",
    "            pooled_init_path = os.path.join(report_pngs_folder, group_name, config_descr, \"pooled_init\", \"init_*.png\")\n",
    "            pooled_init_plots = sorted(glob(pooled_init_path), key=extract_varying_value_from_json) \n",
    "\n",
    "            rel_pooled_init_paths = [rel(p) for p in pooled_init_plots]\n",
    "\n",
    "            chain_init_path = os.path.join(report_pngs_folder, group_name, config_descr, \"chain_init\", \"init_*.png\")\n",
    "            chain_init_plots = sorted(glob(chain_init_path), key=extract_varying_value_from_json)\n",
    "\n",
    "            rel_chain_init_paths = [rel(p) for p in chain_init_plots]\n",
    "\n",
    "            # Load metadata\n",
    "            metadata_path = os.path.join(\n",
    "                experiment_root_folder, \"results\", group_name, config_descr, f\"metadata_config_{config_descr}.json\"\n",
    "            )\n",
    "            with open(metadata_path, \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "\n",
    "            entry = {\n",
    "                \"config_descr\": config_descr,\n",
    "                \"posterior_type\": metadata.get(\"posterior_type\"),\n",
    "                \"varying_attribute\": metadata.get(\"varying_attribute\"),\n",
    "                \"runs\": metadata.get(\"runs\"),\n",
    "                \"git_tag\": metadata.get(\"git_tag\"),\n",
    "\n",
    "                # batch\n",
    "                \"metric_plot_paths_pooled\" : collect_metric_pngs(pooled_png_base, metrics),\n",
    "                \"glass_plot_paths_pooled\"  : collect_glass_pngs(pooled_png_base),\n",
    "\n",
    "                # chain  (may be None)\n",
    "                \"metric_plot_paths_chain\" : collect_metric_pngs(chain_png_base, metrics),\n",
    "                \"glass_plot_paths_chain\"  : collect_glass_pngs(chain_png_base),\n",
    "\n",
    "                \"iid_kde_plot_paths\": rel_kde_paths,\n",
    "                \"pooled_init_plot_paths\": rel_pooled_init_paths,\n",
    "                \"chain_init_plot_paths\": rel_chain_init_paths,\n",
    "                \"kde_init_triples\": list(zip(rel_kde_paths, rel_pooled_init_paths, rel_chain_init_paths)),\n",
    "                #\"metrics\": metrics\n",
    "            }\n",
    "\n",
    "            config_entries.append(entry)\n",
    "\n",
    "        groups_data.append({\n",
    "            \"name\": group_name,\n",
    "            \"configs\": config_entries\n",
    "        })\n",
    "\n",
    "    html = template.render(\n",
    "        experiment_name=os.path.basename(experiment_root_folder),\n",
    "        groups=groups_data,\n",
    "        metrics=metrics\n",
    "    )\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "    print(f\"Experiment-level HTML report saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def plot_and_save_all_metrics(df_results, sampler_colors, varying_attribute, varying_attribute_for_plot, csv_folder, plots_folder, run_id, config_descr):\n",
    "    \"\"\"\n",
    "    Generates and saves multiple metric plots for different samplers.\n",
    "\n",
    "    Parameters:\n",
    "    - df_results: DataFrame containing experiment results.\n",
    "    - sampler_colors: Dictionary mapping sampler names to colors.\n",
    "    - varying_attribute: The attribute that varies.\n",
    "    - varying_attribute_for_plot: The attribute used for plotting.\n",
    "    - plots_folder: Folder where plots should be saved.\n",
    "    - run_id: ID of the current run.\n",
    "    - config_descr: Description of the configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define metric labels\n",
    "    metrics = [\"wasserstein_distance\", \"mmd_rff\", \"r_hat\", \"ess\", \"runtime\"]\n",
    "\n",
    "    # Initialize plots for all metrics\n",
    "    fig_ax_pairs = {key: plt.subplots(figsize=(10, 6)) for key in metrics}\n",
    "\n",
    "    # Iterate over samplers and plot all metrics\n",
    "    for sampler in df_results[\"sampler\"].unique():\n",
    "        df_sampler = df_results[df_results[\"sampler\"] == sampler]\n",
    "        csv_filename = os.path.join(csv_folder, f\"{sampler}_results.csv\")\n",
    "        df_sampler.to_csv(csv_filename, index=False)\n",
    "\n",
    "        for metric in metrics:\n",
    "            fig, ax = fig_ax_pairs[metric]\n",
    "            ax.plot(df_sampler[varying_attribute_for_plot], df_sampler[metric], \n",
    "                    marker=\"o\", linestyle=\"-\", label=sampler, \n",
    "                    color=sampler_colors.get(sampler, \"black\"))\n",
    "\n",
    "    # Set dynamic axis labels and save plots\n",
    "    attribute_label = varying_attribute.replace(\"_\", \" \").title()\n",
    "\n",
    "    for metric in metrics:\n",
    "        fig, ax = fig_ax_pairs[metric]\n",
    "        finalize_and_save_plot(fig,ax, attribute_label, metric, \n",
    "                               f\"{metric} for Samplers (config =_{config_descr})\",\n",
    "                               os.path.join(plots_folder, f\"{metric}_run_{run_id}.pdf\"))\n",
    "        \n",
    "\n",
    "def compute_and_save_global_metrics(df_all_runs, sampler_colors, varying_attribute, varying_values, runs, num_chains, config_descr, global_results_folder, global_plots_folder, png_folder, iid_ref_stats_dict, scatter_overlay, save_extra_scatter):\n",
    "    \"\"\"\n",
    "    Computes and saves global metric plots (averaged across runs) for different samplers.\n",
    "\n",
    "    Parameters:\n",
    "    - df_all_runs: DataFrame containing results from all runs.\n",
    "    - sampler_colors: Dictionary mapping sampler names to colors.\n",
    "    - varying_attribute: The attribute that varies.\n",
    "    - runs: Number of experiment runs.\n",
    "    - config_descr: Configuration description.\n",
    "    - global_results_folder: Folder to save CSVs.\n",
    "    - global_plots_folder: Folder to save plots.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define metrics for aggregation\n",
    "    metrics = [\"wasserstein_distance\", \"mmd_rff\",\"r_hat\", \"ess\", \"runtime\"]\n",
    "\n",
    "    attribute_label = varying_attribute.replace(\"_\", \" \").title()\n",
    "\n",
    "\n",
    "    # New figure set (line + fill)\n",
    "    fig_ax_pairs_shaded = {metric: plt.subplots(figsize=(10, 6)) for metric in metrics}\n",
    "    fig_g, ax_g = plt.subplots(figsize=(10, 6))  # Glass delta for wasserstein_distance\n",
    "    fig_g_mmd, ax_g_mmd = plt.subplots(figsize=(10, 6))  # Glass delta for mmd\n",
    "\n",
    "    global_avg_dfs = {}\n",
    "    scatter_data = {}\n",
    "\n",
    "    # Load IID reference statistics\n",
    "    iid_means_dict_swd = {}\n",
    "    iid_stds_dict_swd = {}\n",
    "    iid_medians_dict_swd = {}\n",
    "    iid_q25_dict_swd = {}\n",
    "    iid_q75_dict_swd = {}\n",
    "    iid_means_dict_mmd = {}\n",
    "    iid_stds_dict_mmd = {}\n",
    "    iid_medians_dict_mmd = {}\n",
    "    iid_q25_dict_mmd = {}\n",
    "    iid_q75_dict_mmd = {}\n",
    "\n",
    "    for key in df_all_runs[varying_attribute].unique():\n",
    "        k = tuple(key) if isinstance(key, np.ndarray) else key\n",
    "        iid_entry = iid_ref_stats_dict.get(k)\n",
    "        if iid_entry is None:\n",
    "            raise KeyError(f\"Missing IID reference stats for varying attribute value: {k}\")\n",
    "        iid_means_dict_swd[k] = iid_entry[\"mean_swd\"]\n",
    "        iid_stds_dict_swd[k] = iid_entry[\"std_swd\"]\n",
    "        iid_medians_dict_swd[k] = iid_entry[\"median_swd\"]\n",
    "        iid_q25_dict_swd[k] = iid_entry[\"q25_swd\"]\n",
    "        iid_q75_dict_swd[k] = iid_entry[\"q75_swd\"]\n",
    "        iid_means_dict_mmd[k] = iid_entry[\"mean_mmd\"]\n",
    "        iid_stds_dict_mmd[k] = iid_entry[\"std_mmd\"]\n",
    "        iid_medians_dict_mmd[k] = iid_entry[\"median_mmd\"]\n",
    "        iid_q25_dict_mmd[k] = iid_entry[\"q25_mmd\"]\n",
    "        iid_q75_dict_mmd[k] = iid_entry[\"q75_mmd\"]\n",
    "\n",
    "\n",
    "    for metric in metrics:\n",
    "        fig_shaded, ax_shaded = fig_ax_pairs_shaded[metric]\n",
    "\n",
    "        # For each sampler, plot its line for this metric\n",
    "        for sampler in df_all_runs[\"sampler\"].unique():\n",
    "            df_sampler = df_all_runs[df_all_runs[\"sampler\"] == sampler]\n",
    "            color = sampler_colors.get(sampler, \"black\")\n",
    "\n",
    "            # Pivot: rows = varying_attribute, columns = run_id, values = metric\n",
    "            df_pivot = df_sampler.pivot_table(\n",
    "                index=varying_attribute, columns=\"run_id\", values=metric\n",
    "            )\n",
    "\n",
    "            if df_pivot.empty or df_pivot.shape[1] == 0:\n",
    "                print(f\"No data for sampler '{sampler}' and metric '{metric}' – skipping.\")\n",
    "                ax_shaded.annotate(\"'DEMetropolis' r-hat skipped due to invalid values\", \n",
    "                       xy=(0.98, 0.02), xycoords='axes fraction',\n",
    "                       ha=\"right\", va=\"bottom\", fontsize=9, color=\"red\")\n",
    "                continue\n",
    "\n",
    "            if metric == \"r_hat\":\n",
    "                if df_pivot.isnull().values.any() or  (df_pivot > 1000).any().any():\n",
    "                    logger.warning(f\"Skipping r_hat plot for sampler {sampler} due to extremely high values.\")                    \n",
    "                    ax_shaded.annotate(\"'DEMetropolis' r-hat skipped due to >1000\", \n",
    "                       xy=(0.98, 0.02), xycoords='axes fraction',\n",
    "                       ha=\"right\", va=\"bottom\", fontsize=9, color=\"red\")\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            # Compute mean+std and median+quantiles \n",
    "            means = df_pivot.mean(axis=1)\n",
    "            medians = df_pivot.median(axis=1)\n",
    "            q25 = df_pivot.quantile(0.25, axis=1)\n",
    "            q75 = df_pivot.quantile(0.75, axis=1)\n",
    "\n",
    "            # Custom ordering based on config (only if needed)\n",
    "            if isinstance(medians.index[0], str):\n",
    "                custom_order = [str(t) for t in varying_values]\n",
    "                means = means.reindex(custom_order) \n",
    "                medians = medians.reindex(custom_order)\n",
    "                q25 = q25.reindex(custom_order)\n",
    "                q75 = q75.reindex(custom_order)\n",
    "                \n",
    "            # Plot median line\n",
    "            ax_shaded.plot(medians.index, medians, \"o-\", label=sampler, color=color)\n",
    "\n",
    "            # Plot uncertainty: interquartile range (q25–q75)\n",
    "            if len(medians.index) > 1:\n",
    "                ax_shaded.fill_between(medians.index, q25, q75, color=color, alpha=0.2)\n",
    "            else:\n",
    "                lower_err = medians - q25\n",
    "                upper_err = q75 - medians\n",
    "                yerr = [lower_err, upper_err]\n",
    "                ax_shaded.errorbar(medians.index, medians, yerr=yerr, fmt=\"o\", color=color, capsize=5)\n",
    "\n",
    "\n",
    "            if scatter_overlay or save_extra_scatter:\n",
    "\n",
    "                # ---------------- plot_mode == 'scatter' ------------------\n",
    "                # We want *all* points: x = v.attr value repeated per run\n",
    "                xs, ys = np.broadcast_to(\n",
    "                    df_pivot.index.to_numpy()[:, None], df_pivot.shape\n",
    "                ).ravel(), df_pivot.to_numpy().ravel()\n",
    "\n",
    "                if scatter_overlay:\n",
    "                    ax_shaded.scatter(xs, ys, alpha=0.55, s=30,\n",
    "                                    color=color, rasterized=True,\n",
    "                                    label=f\"{sampler} (runs)\")\n",
    "\n",
    "                # save data for a per-sampler scatter figure later\n",
    "                if save_extra_scatter:\n",
    "                    scatter_data.setdefault(metric, []).append(\n",
    "                        (sampler, xs, ys, color)\n",
    "                    )\n",
    "\n",
    "\n",
    "            # Save global avg for CSV\n",
    "            if sampler not in global_avg_dfs:\n",
    "                global_avg_dfs[sampler] = {}\n",
    "            global_avg_dfs[sampler][metric] = (medians, q25, q75)\n",
    "\n",
    "            # Compute glass delta for wasserstein_distance only\n",
    "            if metric == \"wasserstein_distance\":\n",
    "                # Get IID mean and std for this varying attribute value\n",
    "\n",
    "                iid_mean_swd = pd.Series(\n",
    "                    [iid_means_dict_swd[k] for k in means.index],\n",
    "                    index=means.index,                   \n",
    "                    name=\"iid_mean_swd\"\n",
    "                )\n",
    "                iid_std_swd = pd.Series(\n",
    "                    [iid_stds_dict_swd[k] for k in means.index],\n",
    "                    index=means.index,\n",
    "                    name=\"iid_std_swd\"\n",
    "                )\n",
    "\n",
    "                # Glass Δ\n",
    "                glass_delta = (means - iid_mean_swd) / iid_std_swd.replace(0, np.nan)\n",
    "        \n",
    "                global_avg_dfs[sampler][\"ws_dist_glass_delta\"] = glass_delta\n",
    "                global_avg_dfs[sampler][\"ws_dist_mcmc_mean\"]   = means          \n",
    "                global_avg_dfs[sampler][\"ws_dist_iid_mean\"]    = iid_mean_swd   \n",
    "                global_avg_dfs[sampler][\"ws_dist_iid_std\"]     = iid_std_swd \n",
    "\n",
    "                # Plot glass delta for this sampler\n",
    "                ax_g.plot(means.index, glass_delta, \"o-\", label=sampler, color=color)\n",
    "            \n",
    "            elif metric == \"mmd_rff\":\n",
    "                # Get IID mean and std for this varying attribute value\n",
    "\n",
    "\n",
    "                iid_mean_mmd = pd.Series(\n",
    "                    [iid_means_dict_mmd[k] for k in means.index],\n",
    "                    index=means.index,                    \n",
    "                    name=\"iid_mean_mmd\"\n",
    "                )\n",
    "\n",
    "                iid_std_mmd = pd.Series(\n",
    "                    [iid_stds_dict_mmd[k] for k in means.index],\n",
    "                    index=means.index,\n",
    "                    name=\"iid_std_mmd\"\n",
    "                )\n",
    "\n",
    "                # Glass Δ\n",
    "                glass_delta = (means - iid_mean_mmd) / iid_std_mmd.replace(0, np.nan)\n",
    "\n",
    "                global_avg_dfs[sampler][\"mmd_rff_glass_delta\"] = glass_delta\n",
    "                global_avg_dfs[sampler][\"mmd_rff_mcmc_mean\"] = means\n",
    "                global_avg_dfs[sampler][\"mmd_rff_iid_mean\"]  = iid_mean_mmd\n",
    "                global_avg_dfs[sampler][\"mmd_rff_iid_std\"]   = iid_std_mmd\n",
    "\n",
    "                # Plot glass delta for this sampler\n",
    "                ax_g_mmd.plot(means.index, glass_delta, \"o-\", label=sampler, color=color)\n",
    "\n",
    "\n",
    "        # Only for wasserstein_distance and mmd: Plot IID baseline once\n",
    "        if metric == \"wasserstein_distance\":\n",
    "            \n",
    "            iid_medians = np.array([iid_medians_dict_swd[k] for k in medians.index])\n",
    "            iid_q25 = np.array([iid_q25_dict_swd[k] for k in medians.index])\n",
    "            iid_q75 = np.array([iid_q75_dict_swd[k] for k in medians.index])\n",
    "\n",
    "            ax_shaded.plot(medians.index, iid_medians, \"o--\", label=\"IID Reference\", color=\"black\")\n",
    "            ax_shaded.fill_between(\n",
    "                medians.index,\n",
    "                iid_q25,\n",
    "                iid_q75,\n",
    "                color=\"black\",\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "        elif metric == \"mmd_rff\":\n",
    "\n",
    "            iid_medians = np.array([iid_medians_dict_mmd[k] for k in medians.index])\n",
    "            iid_q25 = np.array([iid_q25_dict_mmd[k] for k in medians.index])\n",
    "            iid_q75 = np.array([iid_q75_dict_mmd[k] for k in medians.index])\n",
    "\n",
    "            ax_shaded.plot(medians.index, iid_medians, \"o--\", label=\"IID Reference\", color=\"black\")\n",
    "            ax_shaded.fill_between(\n",
    "                medians.index,\n",
    "                iid_q25,\n",
    "                iid_q75,\n",
    "                color=\"black\",\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "        # ── optional scatter-only twin ──────────────────────────────\n",
    "        if save_extra_scatter and metric in scatter_data:\n",
    "            fig_sc, ax_sc = plt.subplots(figsize=(10, 6))\n",
    "            for s, xs, ys, c in scatter_data[metric]:\n",
    "                ax_sc.scatter(xs, ys, alpha=0.55, s=30,\n",
    "                                color=c, rasterized=True, label=s)\n",
    "                \n",
    "            finalize_and_save_plot(\n",
    "                fig_sc, ax_sc,\n",
    "                attribute_label,\n",
    "                metric,\n",
    "                title=(f\"All runs {metric.replace('_', ' ').title()} \"\n",
    "                        f\"({runs} Runs, config = {config_descr})\"),\n",
    "                save_path=os.path.join(global_plots_folder,\n",
    "                                        f\"{metric}_global_plot_scatter.pdf\"),\n",
    "                save_path_png=os.path.join(png_folder,\n",
    "                                            f\"{metric}_global_plot_scatter.png\"),\n",
    "            )\n",
    "\n",
    "    # Save Global Averages per Sampler to CSV\n",
    "    for sampler, metrics_dict in global_avg_dfs.items():\n",
    "        # Fill missing metrics with NaNs so CSV is complete\n",
    "        for m in metrics:\n",
    "            if m not in metrics_dict:\n",
    "                nan_series = pd.Series(np.nan, index=metrics_dict[\"wasserstein_distance\"][0].index)\n",
    "                metrics_dict[m] = (nan_series, nan_series, nan_series)\n",
    "\n",
    "        df_global_avg = pd.DataFrame({\n",
    "            varying_attribute: metrics_dict[\"wasserstein_distance\"][0].index,\n",
    "            **{f\"global_median_{metric}\": metrics_dict[metric][0].values for metric in metrics},\n",
    "            **{f\"global_q25_{metric}\": metrics_dict[metric][1].values for metric in metrics},\n",
    "            **{f\"global_q75_{metric}\": metrics_dict[metric][2].values for metric in metrics},\n",
    "            \"ws_mcmc_mean\":  metrics_dict[\"ws_dist_mcmc_mean\"].values,\n",
    "            \"ws_iid_mean\":   metrics_dict[\"ws_dist_iid_mean\"].values,\n",
    "            \"ws_iid_std\":    metrics_dict[\"ws_dist_iid_std\"].values,\n",
    "            \"mmd_mcmc_mean\": metrics_dict[\"mmd_rff_mcmc_mean\"].values,\n",
    "            \"mmd_iid_mean\":  metrics_dict[\"mmd_rff_iid_mean\"].values,\n",
    "            \"mmd_iid_std\":   metrics_dict[\"mmd_rff_iid_std\"].values,\n",
    "        })\n",
    "\n",
    "        if \"ws_dist_glass_delta\" in metrics_dict:\n",
    "            df_global_avg[\"ws_dist_glass_delta\"] = metrics_dict[\"ws_dist_glass_delta\"].values\n",
    "        if \"mmd_rff_glass_delta\" in metrics_dict:\n",
    "            df_global_avg[\"mmd_rff_glass_delta\"] = metrics_dict[\"mmd_rff_glass_delta\"].values\n",
    "\n",
    "        csv_filename = os.path.join(global_results_folder, f\"Global_results_{sampler}.csv\")\n",
    "        df_global_avg.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Save plots\n",
    "    for metric in metrics:\n",
    "\n",
    "        title = (f\"Averaged {metric.replace('_', ' ').title()} \"\n",
    "                f\"({runs} Runs, config = {config_descr})\")   \n",
    "        fig_shaded, ax_shaded = fig_ax_pairs_shaded[metric]\n",
    "        pdf_path = os.path.join(global_plots_folder, f\"{metric}_global_plot_shaded.pdf\")\n",
    "        png_path = os.path.join(png_folder, f\"{metric}_global_plot_shaded.png\")\n",
    "\n",
    "        finalize_and_save_plot(fig_shaded, ax_shaded, attribute_label, metric,\n",
    "                               title, save_path=pdf_path, save_path_png=png_path)\n",
    "        \n",
    "\n",
    "    # Plot Glass's Δ for wasserstein_distance\n",
    "    pdf_path = os.path.join(global_plots_folder, \"glass_delta_ws_dist.pdf\")\n",
    "    png_path = os.path.join(png_folder, \"glass_delta_ws.png\")\n",
    "    title_ws= f\"Glass's Δ for Wasserstein Distance ({runs} Runs, config = {config_descr})\"\n",
    "\n",
    "    finalize_and_save_plot(fig_g, ax_g, xlabel=attribute_label, ylabel=\"Glass's Δ\", title=title_ws,\n",
    "                            save_path=pdf_path, save_path_png=png_path)\n",
    "\n",
    "    # Plot Glass's Δ for MMD\n",
    "    pdf_path = os.path.join(global_plots_folder, \"glass_delta_mmd.pdf\")\n",
    "    png_path = os.path.join(png_folder, \"glass_delta_mmd.png\")\n",
    "    title_mmd = f\"Glass's Δ for MMD-RFF ({runs} Runs, config = {config_descr})\"\n",
    "\n",
    "    finalize_and_save_plot(fig_g_mmd, ax_g_mmd, xlabel=attribute_label, ylabel=\"Glass's Δ\", title=title_mmd,\n",
    "                           save_path=pdf_path, save_path_png=png_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def finalize_and_save_plot(fig, ax, xlabel, ylabel, title, save_path, save_path_png=None):\n",
    "    \"\"\"\n",
    "    Finalizes the plot with labels, grid, and saves it to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - fig: Matplotlib figure\n",
    "    - ax: Matplotlib axis\n",
    "    - xlabel: Label for x-axis\n",
    "    - ylabel: Label for y-axis\n",
    "    - title: Title of the plot\n",
    "    - save_path: Path to save the figure.\n",
    "    \"\"\"\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=\"Sampler\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    # store as pdf\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    if save_path_png:\n",
    "    # store as well as png\n",
    "        fig.savefig(save_path_png, dpi=150, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def plot_histogram(samples, title, save_path=None, save_path_png=None, posterior_type=None, value=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram and KDE of the given samples.\n",
    "\n",
    "    Parameters:\n",
    "    - samples: 1D or 2D array of samples.\n",
    "    - title: Title of the plot.\n",
    "    - save_path: If provided, saves the figure to this path.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    if samples.ndim == 2:\n",
    "        # Handle multivariate case\n",
    "        if samples.shape[1] == 2:\n",
    "            plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3, label=\"2D Samples\")\n",
    "            plt.xlabel(\"Dimension 1\")\n",
    "            plt.ylabel(\"Dimension 2\")\n",
    "            plt.title(title)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "        elif (posterior_type == \"MvNormal\" or posterior_type == \"MvStudentT\") and samples.shape[1] > 2:\n",
    "            logger.info(f\"Skipping plotting: Multivariate Normal with dimension {samples.shape[1]}.\")\n",
    "            return\n",
    "        \n",
    "    else:\n",
    "        # Standard 1D histogram + KDE\n",
    "        plt.hist(samples, bins=50, alpha=0.5, density=True, color='blue', edgecolor='black', label=\"Histogram\")\n",
    "        sns.kdeplot(samples, color='red', lw=2, label=\"KDE\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Sample Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.savefig(save_path_png, bbox_inches=\"tight\")\n",
    "\n",
    "        metadata_path = save_path_png.replace(\".png\", \".json\")\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump({\"varying_value\": value}, f)\n",
    "\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def handle_trace_plots(trace, sampler_name, varying_attribute, value, save_path=None, show=False, save_individual=False):\n",
    "    \"\"\"\n",
    "    Handles both displaying and saving trace plots.\n",
    "\n",
    "    Parameters:\n",
    "    - trace: the ArviZ InferenceData object\n",
    "    - sampler_name: name of the sampler (e.g. \"HMC\")\n",
    "    - varying_attribute: the name of the varying parameter (e.g. \"mu\")\n",
    "    - value: the current value of the varying parameter\n",
    "    - save_path: path to save the full trace plot (if any)\n",
    "    - show: if True, show plot in notebook\n",
    "    - save_individual: if True and dim > 1, save individual dim plots\n",
    "    \"\"\"\n",
    "    posterior_array = trace.posterior[\"posterior\"]\n",
    "    dim = posterior_array.shape[-1] if posterior_array.ndim == 3 else 1\n",
    "\n",
    "    if posterior_array.ndim == 3 and dim > 1:\n",
    "        # Plot combined\n",
    "        fig = az.plot_trace(trace, compact=True)\n",
    "        if show:\n",
    "            plt.suptitle(f\"Trace Plot ({sampler_name}, {varying_attribute} = {value})\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "        # Plot per dimension\n",
    "        if save_individual or show:\n",
    "            for i in range(dim):\n",
    "                dim_i = posterior_array[..., i]\n",
    "                fig = az.plot_trace({f\"posterior_{i}\": dim_i})\n",
    "                title = f\"Trace Plot of posterior[{i}] ({sampler_name}, {varying_attribute} = {value})\"\n",
    "                if show:\n",
    "                    plt.suptitle(title)\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                if save_path and save_individual:\n",
    "                    filename = save_path.replace(\".pdf\", f\"_dim_{i}.pdf\")\n",
    "                    plt.suptitle(title)\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(filename, bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "\n",
    "    else:\n",
    "        fig = az.plot_trace(trace, compact=True)\n",
    "        if show:\n",
    "            plt.suptitle(f\"Trace Plot ({sampler_name}, {varying_attribute} = {value})\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        if save_path:\n",
    "            plt.suptitle(f\"Trace Plot ({sampler_name}, {varying_attribute} = {value})\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def extract_means_from_posterior(posterior_type, posterior_kwargs):\n",
    "    \"\"\"\n",
    "    Generalized function to extract central tendency (mean/loc) for initialization.\n",
    "    - For Mixture: returns list of all component means.\n",
    "    - For single-posteriors: returns list with one mean value or vector.\n",
    "    \"\"\"\n",
    "    if posterior_type == \"Mixture\":\n",
    "        return extract_means_from_components(posterior_type, posterior_kwargs[\"component_params\"])\n",
    "\n",
    "    elif \"mu\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"mu\"]]\n",
    "\n",
    "    elif \"loc\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"loc\"]]\n",
    "\n",
    "    elif posterior_type == \"Cauchy\" and \"alpha\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"alpha\"]] \n",
    "\n",
    "    elif posterior_type == \"Beta\":\n",
    "        a = posterior_kwargs[\"a\"]\n",
    "        b = posterior_kwargs[\"b\"]\n",
    "        # Expected value\n",
    "        return [a / (a + b)]  \n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot extract central location (mu or loc) for posterior type '{posterior_type}'.\")\n",
    "\n",
    "\n",
    "def extract_means_from_components(posterior_type, component_params):\n",
    "    \"\"\"\n",
    "    Extracts central tendency (mu or loc) from each component's parameters.\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    for params in component_params:\n",
    "        if \"mu\" in params:\n",
    "            means.append(params[\"mu\"])\n",
    "        elif \"loc\" in params:\n",
    "            means.append(params[\"loc\"])\n",
    "\n",
    "        elif posterior_type == \"Cauchy\" and \"alpha\" in params:\n",
    "            means.append(params[\"alpha\"])\n",
    "            \n",
    "        elif posterior_type == \"Beta\":\n",
    "            a = params[\"a\"]\n",
    "            b = params[\"b\"]\n",
    "             # Expected value\n",
    "            means.append([a / (a + b)]) \n",
    "        else:\n",
    "            raise ValueError(\"Component missing a central tendency parameter (mu or loc).\")\n",
    "    return means\n",
    "\n",
    "\n",
    "def get_initvals(init_scheme, means, eval_mode, num_chains, rng=None, run_id=None, init_folder=None, png_folder=None, varying_attribute=None, value=None, unimodal_init_margin = None):\n",
    "    \"\"\"Generates initialization values based on the chosen scheme.\"\"\" \n",
    "\n",
    "    if np.isscalar(means[0]):\n",
    "        dim = 1\n",
    "        means_array = np.array(means)[:, None]  # shape (n_modes, 1)\n",
    "    else:\n",
    "        means_array = np.array(means)\n",
    "        dim = means_array.shape[1]\n",
    "\n",
    "\n",
    "    if init_scheme == \"thesis_scheme\":\n",
    "        # If multimodal posterior, use the means of the components, else spawn them randomly around the mean\n",
    "        if len(means_array) >= 2:\n",
    "            # Multimodal case\n",
    "            # Compute bounding box across all dimensions\n",
    "            low, high, min_mode, max_mode, border  = get_uniform_prior_bounds(means_array=means_array, expansion_factor=0.25)\n",
    "            #single_init = rng.uniform(low, high).item() if dim == 1 else rng.uniform(low, high)\n",
    "            #initvals = [{\"posterior\": single_init} for _ in range(num_chains)]\n",
    "            initvals = [{\"posterior\": rng.uniform(low, high).item() if dim == 1 else rng.uniform(low, high)} for _ in range(num_chains)]\n",
    "\n",
    "            if run_id == 1:\n",
    "                init_info = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"case\": \"multimodal\",\n",
    "                    \"dim\": dim,\n",
    "                    \"means_array\": means_array.tolist(),\n",
    "                    \"min_mode\": min_mode,\n",
    "                    \"max_mode\": max_mode,\n",
    "                    \"border\": border,\n",
    "                    \"low\": low,\n",
    "                    \"high\": high,\n",
    "                    \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "                }  \n",
    "        else:\n",
    "           \n",
    "            low, high,_,_,_ = get_uniform_prior_bounds(means_array=means_array, expansion_factor=0.25, unimodal_init_margin=unimodal_init_margin)\n",
    "            #single_init = rng.uniform(low, high).item() if dim == 1 else rng.uniform(low, high)\n",
    "            #initvals = [{\"posterior\": single_init} for _ in range(num_chains)]\n",
    "            initvals = [{\"posterior\": rng.uniform(low, high).item() if dim == 1 else rng.uniform(low, high)} for _ in range(num_chains)]\n",
    "\n",
    "            if run_id == 1:          \n",
    "                init_info = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"case\": \"unimodal\",\n",
    "                    \"dim\": dim,\n",
    "                    \"low\": low,\n",
    "                    \"high\": high,\n",
    "                    \"means_array\": means_array.tolist(),\n",
    "                    \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "                }\n",
    "\n",
    "    elif init_scheme == \"equal_per_mode\":\n",
    "        noise = 0.5\n",
    "        initvals =[]\n",
    "        for i in range(num_chains):\n",
    "            mean = means_array[i % len(means_array)]\n",
    "            center = mean + rng.normal(scale=noise)\n",
    "            if dim == 1:\n",
    "                center = center.item()\n",
    "            initvals.append({\"posterior\": center})\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": \"equal_per_mode\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    elif init_scheme == \"all_in_middle\":\n",
    "        middle_point = np.mean(means_array, axis=0)\n",
    "        middle_point = middle_point.item() if dim == 1 else middle_point\n",
    "        noise = 0.5\n",
    "        initvals = [{\"posterior\": middle_point + rng.normal(scale=noise)} for _ in range(num_chains)]\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": \"all_in_middle\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"middle_point\": middle_point.tolist() if hasattr(middle_point, \"tolist\") else middle_point,\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    elif init_scheme.startswith(\"all_near_mode_\"):\n",
    "\n",
    "        mode_index = int(init_scheme.split(\"_\")[-1])\n",
    "        if mode_index >= len(means):\n",
    "            raise IndexError(f\"Mode index {mode_index} out of bounds for available means.\")\n",
    "        \n",
    "        target_mode = means_array[mode_index]\n",
    "        target_mode = target_mode.item() if dim == 1 else target_mode\n",
    "        noise = 0.5\n",
    "        initvals = [{\"posterior\": target_mode + rng.normal(scale=noise)} for _ in range(num_chains)]\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": f\"all_near_mode{mode_index}\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"mode_index\": mode_index,\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    if run_id == 1:\n",
    "        parent_folder= os.path.join(init_folder, f\"initvals_{eval_mode}\")\n",
    "        create_directories(parent_folder)\n",
    "\n",
    "        chain_info_path = os.path.join(parent_folder, f\"init_{varying_attribute}_{value}_{eval_mode}.json\")\n",
    "        chain_info_plot_path = os.path.join(parent_folder, f\"init_{varying_attribute}_{value}_{eval_mode}.pdf\")\n",
    "        chain_info_png_path = os.path.join(png_folder, f\"init_{varying_attribute}_{value}_{eval_mode}.png\")\n",
    "        \n",
    "        save_sample_info(init_info, chain_info_path, chain_info_plot_path, chain_info_png_path, varying_attribute, value,\"Init Values\", init_info[\"case\"])\n",
    "\n",
    "    logger.debug(f\"Generated initvals: {initvals}\")\n",
    "    return initvals\n",
    "\n",
    "\n",
    "def save_sample_info(sample_info, json_path, plot_path, png_path=None, varying_attribute=None, value=None, label=\"Samples\", case=None):\n",
    "    \"\"\"\n",
    "    General utility to save sample info (e.g., init values, warmup samples) as JSON and plot if dim ≤ 2.\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_info: dict containing\n",
    "        - \"samples\": list of dicts like [{\"posterior\": ...}, ...]\n",
    "        - \"means_array\": list of means (e.g. from init or components)\n",
    "        - \"dim\": int, dimensionality\n",
    "        - optionally: \"low\", \"high\", \"case\"\n",
    "    - json_path: path to save JSON info\n",
    "    - plot_path: path to save the plot\n",
    "    - label: label for sample points (e.g., \"Init Values\", \"Warmup Samples\")\n",
    "    - case: override case type (for optional bounding box display)\n",
    "    \"\"\"\n",
    "\n",
    "    safe_json_dump(sample_info, json_path)\n",
    "\n",
    "    dim = sample_info[\"dim\"]\n",
    "    means_array = np.array(sample_info.get(\"means_array\", []))\n",
    "\n",
    "    if label == \"Init Values\":\n",
    "        samples = np.array([list(v.values())[0] for v in sample_info[\"samples\"]])\n",
    "    elif label == \"Samples\":\n",
    "        samples = np.array(sample_info[\"samples\"])\n",
    "\n",
    "    if dim > 2:\n",
    "        return\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 2) if dim == 1 else (8, 6))\n",
    "\n",
    "    # 1D case\n",
    "    if dim == 1:\n",
    "        samples_flat = samples.flatten()\n",
    "        ax.scatter(samples_flat, np.zeros_like(samples_flat), color='blue', label=label, alpha=0.7)\n",
    "        means_flat = means_array.flatten()\n",
    "        ax.scatter(means_flat, np.zeros_like(means_flat), color='red', marker='x', s=100, label='Means')\n",
    "\n",
    "        if case == \"multimodal\" or case == \"unimodal\":\n",
    "            # Handle scalar or list storage\n",
    "            low = sample_info[\"low\"]\n",
    "            high = sample_info[\"high\"]\n",
    "\n",
    "            ax.axvline(low, color=\"black\", linestyle=\"--\", label=\"Init Box\")\n",
    "            ax.axvline(high, color=\"black\", linestyle=\"--\")  \n",
    "\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Value\")\n",
    "\n",
    "\n",
    "    # 2D case\n",
    "    elif dim == 2:\n",
    "        ax.scatter(samples[:, 0], samples[:, 1], color='blue', label=label, alpha=0.7)\n",
    "        ax.scatter(means_array[:, 0], means_array[:, 1], color='red', marker='x', s=100, label='Means')\n",
    "        \n",
    "        if case == \"multimodal\" or case == \"unimodal\":\n",
    "            low = np.array(sample_info[\"low\"])\n",
    "            high = np.array(sample_info[\"high\"])\n",
    "\n",
    "            rect = plt.Rectangle(low, *(high - low), linewidth=1, edgecolor='black',\n",
    "                                    facecolor='none', linestyle='--', label='Init Box')\n",
    "            ax.add_patch(rect)\n",
    "        ax.set_xlabel(\"Dim 1\")\n",
    "        ax.set_ylabel(\"Dim 2\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "\n",
    "    if label == \"Init Values\":\n",
    "        ax.set_title(f\"{label} & Means ({varying_attribute} = {value})\")\n",
    "    elif label == \"Samples\":\n",
    "        sampler = sample_info.get(\"sampler\", \"Unknown\")\n",
    "        case = sample_info.get(\"case\", \"Unknown\")\n",
    "        ax.set_title(f\"First {case} from {sampler}\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    if dim == 1:\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "        fig.subplots_adjust(right=0.75)  \n",
    "    else:\n",
    "        ax.legend()  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    if png_path:\n",
    "        plt.savefig(png_path, bbox_inches=\"tight\")\n",
    "        metadata_path = png_path.replace(\".png\", \".json\")\n",
    "        with open(metadata_path, \"w\") as f:\n",
    "            json.dump({\"varying_value\": value}, f)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def compute_mean_mahalanobis_distance(component_params):\n",
    "    means = []\n",
    "    covs = []\n",
    "\n",
    "    for comp in component_params:\n",
    "        mu = np.atleast_1d(comp[\"mu\"])\n",
    "        sigma = comp[\"sigma\"]\n",
    "\n",
    "        if np.isscalar(sigma):\n",
    "            cov = np.diag([sigma**2] * mu.shape[0])\n",
    "        else:\n",
    "            # Ensure covariance matrix\n",
    "            cov = np.atleast_2d(sigma) ** 2  \n",
    "\n",
    "        means.append(mu)\n",
    "        covs.append(cov)\n",
    "\n",
    "    pairwise_distances = []\n",
    "    for (i, j) in combinations(range(len(means)), 2):\n",
    "        pooled_cov = (covs[i] + covs[j]) / 2\n",
    "        try:\n",
    "            inv_cov = np.linalg.inv(pooled_cov)\n",
    "            d = mahalanobis(means[i], means[j], inv_cov)\n",
    "            pairwise_distances.append(d)\n",
    "        except np.linalg.LinAlgError:\n",
    "            # skip singular cases\n",
    "            continue  \n",
    "\n",
    "    return np.mean(pairwise_distances) if pairwise_distances else 0.0\n",
    "\n",
    "\n",
    "def sliced_wasserstein_distance(X, Y, L=None, rng=None):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein distance (SWD_p) between two sets of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: numpy array of shape (N, d) -> first sample set\n",
    "    - Y: numpy array of shape (N, d) -> second sample set\n",
    "    - L: int, number of random projections\n",
    "    - p: int, order of Wasserstein distance (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "    - SWD_p: float, the sliced Wasserstein distance\n",
    "    \"\"\"\n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "\n",
    "     # Assuming X and Y have the same shape\n",
    "    N, d = X.shape \n",
    "    # Accumulation variable\n",
    "    S = 0  \n",
    "\n",
    "    for _ in range(L):\n",
    "        # Sample a random unit vector (projection direction)\n",
    "        theta = rng.standard_normal(d)\n",
    "        norm = np.linalg.norm(theta)\n",
    "        if norm == 0:\n",
    "            continue\n",
    "        # Normalize to unit sphere\n",
    "        theta /= norm  \n",
    "\n",
    "        # Compute projections\n",
    "        alpha = X @ theta\n",
    "        beta = Y @ theta\n",
    "\n",
    "        # Compute 1D Wasserstein distance\n",
    "        W_i = sp.wasserstein_distance(alpha, beta)\n",
    "\n",
    "        # Accumulate\n",
    "        S += W_i\n",
    "\n",
    "    # Compute final SWD\n",
    "    SWD_p = (S / L) \n",
    "\n",
    "    return SWD_p\n",
    "\n",
    "def compute_mmd_rff(X, Y, D=None, sigma=1.0, rng=None):\n",
    "    \"\"\"\n",
    "    Computes the approximate Maximum Mean Discrepancy (MMD) using Random Fourier Features (RFF)\n",
    "    between two sample sets X and Y.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.ndarray of shape (n, d) – sample set from distribution p(x)\n",
    "    - Y: np.ndarray of shape (m, d) – sample set from distribution q(x)\n",
    "    - D: int – number of random Fourier features\n",
    "    - sigma: float – bandwidth of the Gaussian kernel\n",
    "    - rng: np.random.Generator, optional – random number generator for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - mmd_rff: float – approximate MMD value\n",
    "    \"\"\"\n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "\n",
    "    n, d = X.shape\n",
    "    m, _ = Y.shape\n",
    "\n",
    "    # Step 1: Generate random frequencies and offsets\n",
    "    omega = rng.normal(loc=0.0, scale=1.0 / sigma, size=(D, d))\n",
    "    b = rng.uniform(0, 2 * np.pi, size=D)\n",
    "\n",
    "    # Step 2: Compute random Fourier features\n",
    "    def z(x):\n",
    "        projection = np.dot(x, omega.T) + b\n",
    "        return np.sqrt(2.0 / D) * np.cos(projection)\n",
    "\n",
    "    Z_X = z(X)  # shape (n, D)\n",
    "    Z_Y = z(Y)  # shape (m, D)\n",
    "\n",
    "    # Step 3: Calculate mean embeddings\n",
    "    mu_p = Z_X.mean(axis=0)\n",
    "    mu_q = Z_Y.mean(axis=0)\n",
    "\n",
    "    # Step 4: Calculate MMD^2 (Euclidean distance between embeddings)\n",
    "    mmd_rff = np.linalg.norm(mu_p - mu_q)\n",
    "\n",
    "    return mmd_rff\n",
    "\n",
    "\n",
    "def generate_iid_samples(posterior_type = None, num_samples=None, rng=None,**params):\n",
    "    \"\"\"\n",
    "    Generate IID samples from a given posterior type.\n",
    "\n",
    "    Parameters:\n",
    "    - posterior_type: String specifying the type of the posterior (e.g., \"Normal\", \"Mixture\").\n",
    "    - num_samples: Number of samples to generate.\n",
    "    - rng: Optional random number generator.\n",
    "    - **params: Additional parameters depending on posterior_type.\n",
    "        - For \"Mixture\":\n",
    "            - component_types: list of strings.\n",
    "            - component_params: list of parameter dicts.\n",
    "            - weights: list of floats.\n",
    "        - For others: distribution-specific parameters.\n",
    "    Returns:\n",
    "    - iid_samples: Array of generated IID samples.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "\n",
    "    # Mapping from string names to scipy sampling functions\n",
    "    scipy_distributions = {\n",
    "        \"Normal\": lambda p: sp.norm.rvs(loc=p[\"mu\"], scale=p[\"sigma\"], size=num_samples, random_state=rng),\n",
    "        \"SkewNormal\": lambda p: sp.skewnorm.rvs(a=p[\"alpha\"], loc=p[\"mu\"], scale=p[\"sigma\"], size=num_samples, random_state=rng),\n",
    "        \"StudentT\": lambda p: sp.t.rvs(df=p[\"nu\"], loc=p[\"mu\"], scale=p[\"sigma\"], size=num_samples, random_state=rng),\n",
    "        \"Beta\": lambda p: sp.beta.rvs(a=p[\"a\"], b=p[\"b\"], size=num_samples, random_state=rng),\n",
    "        \"Cauchy\": lambda p: sp.cauchy.rvs(loc=p[\"alpha\"], scale=p[\"beta\"], size=num_samples, random_state=rng),\n",
    "        \"Laplace\": lambda p: sp.laplace.rvs(loc=p[\"mu\"], scale=p[\"b\"], size=num_samples, random_state=rng),\n",
    "        \"MvNormal\": lambda p: sp.multivariate_normal.rvs(mean=np.array(p[\"mu\"]), cov=np.array(p[\"cov\"]), size=num_samples, random_state=rng),\n",
    "        \"MvStudentT\": lambda p: sp.multivariate_t.rvs(df=p[\"nu\"], loc=np.array(p[\"mu\"]), shape=np.array(p[\"scale\"]), size=num_samples, random_state=rng),\n",
    "    }\n",
    "\n",
    "    # Handle Skewed Student-T (which needs PyMC)\n",
    "    if posterior_type == \"SkewStudentT\":\n",
    "        with pm.Model():\n",
    "            skewed_t = pm.SkewStudentT.dist(a=params[\"a\"], b=params[\"b\"], mu=params[\"mu\"], sigma=params[\"sigma\"])\n",
    "            return pm.draw(skewed_t, draws=num_samples, random_seed=rng)\n",
    "\n",
    "    # Handle single distributions\n",
    "    if posterior_type in scipy_distributions:\n",
    "        logger.debug(f\"Generating {posterior_type} samples with parameters: {params}\")\n",
    "        return scipy_distributions[posterior_type](params)\n",
    "\n",
    "    elif posterior_type == \"Mixture\":\n",
    "        component_types = params[\"component_types\"]\n",
    "        component_params = params[\"component_params\"]\n",
    "        weights = params[\"weights\"]\n",
    "\n",
    "        if len(component_types) != len(component_params):\n",
    "            raise ValueError(\"Each component type must have a corresponding parameter dictionary.\")\n",
    "\n",
    "        # normalize weights\n",
    "        weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "        # Choose which component each sample belongs to based on weights\n",
    "        chosen_components = rng.choice(len(component_types), size=num_samples, p=weights)\n",
    "\n",
    "        posterior_dim = get_posterior_dim(\"Mixture\", {\n",
    "            \"component_types\": component_types,\n",
    "            \"component_params\": component_params,\n",
    "            \"weights\": weights\n",
    "        })\n",
    "\n",
    "        if posterior_dim > 1:\n",
    "            iid_samples = np.empty((num_samples, posterior_dim)) \n",
    "        else:\n",
    "            iid_samples = np.empty(num_samples)\n",
    "\n",
    "        for i, (comp_type, comp_params) in enumerate(zip(component_types, component_params)):\n",
    "            # Select samples for this component\n",
    "            mask = chosen_components == i  \n",
    "            num_selected = mask.sum()\n",
    "            if num_selected > 0:\n",
    "                if comp_type in scipy_distributions or comp_type == \"SkewStudentT\":\n",
    "                    iid_samples[mask] = generate_iid_samples(posterior_type=comp_type, num_samples=num_selected, rng=rng, **comp_params)\n",
    "\n",
    "        return iid_samples\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported posterior type: {posterior_type}\")\n",
    "\n",
    "\n",
    "def generate_all_iid_batches(\n",
    "    posterior_type,\n",
    "    posterior_kwargs,\n",
    "    iid_kwargs_original,\n",
    "    iid_kwargs,\n",
    "    iid_posteriors_folder,\n",
    "    varying_attribute,\n",
    "    varying_values,\n",
    "    num_total_iid_batches,\n",
    "    num_iid_vs_iid_batches,\n",
    "    num_samples,\n",
    "    num_chains,\n",
    "    rng,\n",
    "    group_folder,\n",
    "    png_folder,\n",
    "    required_parameters\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates all IID batches for the given posterior type and varying attribute.\n",
    "    \n",
    "    Returns:\n",
    "    - iid_batches_dict: Dictionary of generated IID batches.\n",
    "    - iid_ref_stats_dict: Dictionary of reference statistics for SWD and MMD.\n",
    "    \"\"\"\n",
    "    \n",
    "    iid_histogram_folder = os.path.join(group_folder, \"KDE and Histograms of IID Samples\")\n",
    "    create_directories(iid_histogram_folder)\n",
    "\n",
    "    # === Handle Precomputed IID Samples for Varying Attributes ===\n",
    "    # Dictionary to store generated IID batches and reference statistics\n",
    "    iid_batches_dict = {}\n",
    "    iid_ref_stats_dict = {}\n",
    "\n",
    "    component_index = posterior_kwargs.get(\"varying_component\", None)\n",
    "\n",
    "    if posterior_type == \"Mixture\":\n",
    "\n",
    "        is_studentt = posterior_kwargs[\"component_types\"][0] == \"MvStudentT\"\n",
    "        cov_param_key = \"scale\" if is_studentt else \"cov\" \n",
    "\n",
    "        # Loop through all varying values for Mixture posterior\n",
    "        for value in varying_values:\n",
    "            \n",
    "            if varying_attribute == \"weights\":\n",
    "                    iid_kwargs[\"weights\"] = value\n",
    "            elif varying_attribute == \"dimension\":\n",
    "                adjust_dimension_of_kwargs(posterior_type, iid_kwargs_original, iid_kwargs, target_dim=value, required_parameters=required_parameters)\n",
    "                save_adjusted_posterior_config(\n",
    "                    iid_kwargs,\n",
    "                    folder=iid_posteriors_folder,\n",
    "                    dim_value=value\n",
    "                )\n",
    "\n",
    "            elif varying_attribute == \"num_samples\":\n",
    "                num_samples = value\n",
    "            elif varying_attribute == \"num_chains\":\n",
    "                num_chains = value\n",
    "            elif varying_attribute == \"circle_radius\":\n",
    "                iid_kwargs[\"component_params\"], \\\n",
    "                iid_kwargs[\"component_types\"], \\\n",
    "                iid_kwargs[\"weights\"] = adjust_circle_layout(\n",
    "                    posterior_kwargs[\"component_number\"],\n",
    "                    posterior_kwargs[\"component_type\"],\n",
    "                    value,\n",
    "                    posterior_kwargs[\"cov\"],\n",
    "                    posterior_kwargs[\"weight_type\"]\n",
    "                )\n",
    "\n",
    "                save_adjusted_posterior_config(\n",
    "                    iid_kwargs,\n",
    "                    folder=iid_posteriors_folder,\n",
    "                    dim_value=value\n",
    "                )\n",
    "            elif varying_attribute == \"circle_modes\":\n",
    "                iid_kwargs[\"component_params\"], \\\n",
    "                iid_kwargs[\"component_types\"], \\\n",
    "                iid_kwargs[\"weights\"] = adjust_circle_layout(\n",
    "                    value,\n",
    "                    posterior_kwargs[\"component_type\"],\n",
    "                    posterior_kwargs[\"radius\"],\n",
    "                    posterior_kwargs[\"cov\"],\n",
    "                    posterior_kwargs[\"weight_type\"]\n",
    "                )\n",
    "\n",
    "                save_adjusted_posterior_config(\n",
    "                    iid_kwargs,\n",
    "                    folder=iid_posteriors_folder,\n",
    "                    dim_value=value\n",
    "                )\n",
    "            elif varying_attribute == \"correlation\":\n",
    "                \n",
    "                posterior_dim = get_posterior_dim(posterior_type, iid_kwargs)\n",
    "\n",
    "                for i, comp_params in enumerate(iid_kwargs[\"component_params\"]):\n",
    "                    \n",
    "                    iid_kwargs[\"component_params\"][i][cov_param_key] = build_correlation_cov_matrix(posterior_dim, value)\n",
    "\n",
    "                save_adjusted_posterior_config(\n",
    "                        iid_kwargs,\n",
    "                        folder=iid_posteriors_folder,\n",
    "                        dim_value=value\n",
    "                )\n",
    "            else:\n",
    "                # Vary only the selected component's parameter\n",
    "                iid_kwargs[\"component_params\"][component_index][varying_attribute] = value\n",
    "\n",
    "            samples_per_chain = num_samples // num_chains\n",
    "            num_samples = samples_per_chain*num_chains\n",
    "\n",
    "            iid_batches = [generate_iid_samples(\n",
    "                posterior_type=posterior_type,\n",
    "                component_types=iid_kwargs[\"component_types\"],\n",
    "                component_params=iid_kwargs[\"component_params\"], \n",
    "                weights=iid_kwargs[\"weights\"],\n",
    "                num_samples= num_samples,\n",
    "                rng=rng) for _ in range(num_total_iid_batches)]\n",
    "\n",
    "            iid_batches_dict[value] = iid_batches\n",
    "\n",
    "            compute_and_store_iid_stats(\n",
    "                iid_batches=iid_batches,\n",
    "                value=value,\n",
    "                num_iid_vs_iid_batches=num_iid_vs_iid_batches,\n",
    "                iid_ref_stats_dict=iid_ref_stats_dict,\n",
    "                iid_histogram_folder=iid_histogram_folder,\n",
    "                png_folder=png_folder,\n",
    "                varying_attribute=varying_attribute,\n",
    "                posterior_type=posterior_type,\n",
    "                rng=rng\n",
    "            )\n",
    "\n",
    "\n",
    "    # Single posterior case\n",
    "    else:\n",
    "        is_studentt =  posterior_type == \"MvStudentT\"\n",
    "        cov_param_key = \"scale\" if is_studentt else \"cov\" \n",
    "\n",
    "        for value in varying_values:\n",
    "            \n",
    "            if varying_attribute == \"dimension\":\n",
    "                adjust_dimension_of_kwargs(posterior_type, iid_kwargs_original, iid_kwargs, target_dim=value, required_parameters=required_parameters)\n",
    "                save_adjusted_posterior_config(\n",
    "                    iid_kwargs,\n",
    "                    folder=iid_posteriors_folder,\n",
    "                    dim_value=value\n",
    "                )\n",
    "            elif varying_attribute == \"num_samples\":\n",
    "                num_samples = value\n",
    "            elif varying_attribute == \"num_chains\":\n",
    "                num_chains = value\n",
    "            elif varying_attribute == \"correlation\":\n",
    "                posterior_dim = get_posterior_dim(posterior_type, iid_kwargs)\n",
    "                iid_kwargs[cov_param_key] = build_correlation_cov_matrix(posterior_dim, value)\n",
    "                save_adjusted_posterior_config(\n",
    "                        iid_kwargs,\n",
    "                        folder=iid_posteriors_folder,\n",
    "                        dim_value=value\n",
    "                )\n",
    "            else:\n",
    "                iid_kwargs[varying_attribute] = value  \n",
    "\n",
    "            samples_per_chain = num_samples // num_chains\n",
    "            num_samples = samples_per_chain*num_chains\n",
    "            \n",
    "            iid_batches = [generate_iid_samples(    \n",
    "                posterior_type=posterior_type,\n",
    "                **iid_kwargs,\n",
    "                num_samples= num_samples,\n",
    "                rng=rng) for _ in range(num_total_iid_batches)]\n",
    "\n",
    "            iid_batches_dict[value] = iid_batches\n",
    "\n",
    "            compute_and_store_iid_stats(\n",
    "                iid_batches=iid_batches,\n",
    "                value=value,\n",
    "                num_iid_vs_iid_batches=num_iid_vs_iid_batches,\n",
    "                iid_ref_stats_dict=iid_ref_stats_dict,\n",
    "                iid_histogram_folder=iid_histogram_folder,\n",
    "                png_folder=png_folder,\n",
    "                varying_attribute=varying_attribute,\n",
    "                posterior_type=posterior_type,\n",
    "                rng=rng\n",
    "            )\n",
    "\n",
    "    return iid_batches_dict, iid_ref_stats_dict\n",
    "\n",
    "\n",
    "def compute_and_store_iid_stats(\n",
    "    iid_batches,\n",
    "    value,\n",
    "    num_iid_vs_iid_batches,\n",
    "    iid_ref_stats_dict,\n",
    "    iid_histogram_folder,\n",
    "    png_folder,\n",
    "    varying_attribute,\n",
    "    posterior_type,\n",
    "    rng\n",
    "):\n",
    "\n",
    "    ref_swd_values = []\n",
    "    ref_mmd_values = []\n",
    "\n",
    "    # get dimension of the first batch\n",
    "    dim = ensure_2d(iid_batches[0]).shape[1]\n",
    "    projections = 1 if dim == 1 else max(50, min(10*dim, 500))\n",
    "\n",
    "    # Pairwise comparison for SWD/MMD stats\n",
    "    for i in range(0, num_iid_vs_iid_batches, 2): \n",
    "        x = ensure_2d(iid_batches[i])\n",
    "        y = ensure_2d(iid_batches[i + 1])\n",
    "\n",
    "        swd = sliced_wasserstein_distance(x, y, L=projections, rng=rng)\n",
    "        mmd_rff = compute_mmd_rff(x, y, D=500, sigma=1.0, rng=rng)\n",
    "        ref_swd_values.append(swd)\n",
    "        ref_mmd_values.append(mmd_rff)\n",
    "\n",
    "\n",
    "    iid_ref_stats_dict[value] = {\n",
    "        \"mean_swd\": np.mean(ref_swd_values),\n",
    "        \"std_swd\": np.std(ref_swd_values, ddof=1),\n",
    "        \"median_swd\": np.median(ref_swd_values),\n",
    "        \"q25_swd\": np.quantile(ref_swd_values, 0.25),\n",
    "        \"q75_swd\": np.quantile(ref_swd_values, 0.75),\n",
    "        \"mean_mmd\": np.mean(ref_mmd_values),\n",
    "        \"std_mmd\": np.std(ref_mmd_values, ddof=1),\n",
    "        \"median_mmd\": np.median(ref_mmd_values),\n",
    "        \"q25_mmd\": np.quantile(ref_mmd_values, 0.25),\n",
    "        \"q75_mmd\": np.quantile(ref_mmd_values, 0.75)\n",
    "    }\n",
    "\n",
    "    plot_histogram(\n",
    "        samples=iid_batches[0],\n",
    "        title=f\"IID Samples Histogram & KDE ({varying_attribute}={value})\",\n",
    "        save_path=os.path.join(iid_histogram_folder, f\"iid_hist_kde_{varying_attribute}_{value}.pdf\"),\n",
    "        save_path_png=os.path.join(png_folder, f\"iid_hist_kde_{varying_attribute}_{value}.png\"),\n",
    "        posterior_type=posterior_type,\n",
    "        value=value\n",
    "    )\n",
    "\n",
    "def eval_trace(\n",
    "        trace,\n",
    "        runtime,\n",
    "        eval_level,           \n",
    "        run_id,\n",
    "        sampler_name,\n",
    "        value,\n",
    "        posterior_type,\n",
    "        iid_batch,\n",
    "        experiment_settings,\n",
    "        folders,\n",
    "        varying_attribute,\n",
    "        results,\n",
    "        rng\n",
    "):\n",
    "    \n",
    "    # Plot trace plots in notebook if requested\n",
    "    if experiment_settings.get(\"plot_traces_in_notebook\", False):\n",
    "        handle_trace_plots(\n",
    "            trace=trace,\n",
    "            sampler_name=sampler_name,\n",
    "            varying_attribute=varying_attribute,\n",
    "            value=value,\n",
    "            show=True,\n",
    "            save_path=None,\n",
    "            save_individual=False,\n",
    "        )\n",
    "\n",
    "    trace_plot_mode = experiment_settings.get(\"trace_plots\", \"none\")\n",
    "\n",
    "    # Save trace plots to PDF if requested\n",
    "    if trace_plot_mode == \"all\" or (trace_plot_mode == \"first_run_only\" and run_id == 1):\n",
    "\n",
    "        save_path = os.path.join(folders[\"var_attr_folder\"], f\"{sampler_name}_trace_plot.pdf\")\n",
    "\n",
    "        handle_trace_plots(\n",
    "            trace=trace,\n",
    "            sampler_name=sampler_name,\n",
    "            varying_attribute=varying_attribute,\n",
    "            value=value,\n",
    "            show=False,\n",
    "            save_path=save_path,\n",
    "            save_individual=experiment_settings.get(\"save_individual_traceplots_per_dim\", False)\n",
    "        )\n",
    "    \n",
    "    # Save trace to NetCDF file if requested\n",
    "    if experiment_settings.get(\"save_traces\", False):\n",
    "        trace_filename = os.path.join(folders[\"var_attr_folder\"], f\"{sampler_name}_trace.nc\")\n",
    "\n",
    "        az.to_netcdf(trace, trace_filename)\n",
    "\n",
    "\n",
    "    # poolwise evaluation of posterior samples (multiple chains)\n",
    "    posterior_samples = trace.posterior[\"posterior\"].values\n",
    "\n",
    "    # Ensure posterior_samples always has shape (N, dims)\n",
    "    if posterior_samples.ndim == 2:\n",
    "        posterior_samples = posterior_samples.reshape(-1, 1) \n",
    "    else:\n",
    "        posterior_samples = posterior_samples.reshape(-1, posterior_samples.shape[-1])\n",
    "\n",
    "    # Compute Wasserstein distance and MMD if not custom posterior\n",
    "    if posterior_type != \"Custom\":\n",
    "\n",
    "        dim = posterior_samples.shape[1]\n",
    "        projections = 1 if dim == 1 else max(50, min(10*dim, 500))\n",
    "\n",
    "        mcmc_vs_iid_swd = sliced_wasserstein_distance(posterior_samples, iid_batch, L=projections, rng=rng)\n",
    "        mmd_rff_value = compute_mmd_rff(posterior_samples, iid_batch, D=500, sigma=1.0, rng=rng)\n",
    "\n",
    "    else:\n",
    "        mcmc_vs_iid_swd = np.nan\n",
    "        mmd_rff_value = np.nan\n",
    "\n",
    "    if eval_level == \"pooled\":\n",
    "        # Compute R-hat and ESS\n",
    "        r_hat, ess = get_scalar_rhat_and_ess(trace)\n",
    "    else:\n",
    "        # For single chain, we can only compute ESS\n",
    "        r_hat = np.nan\n",
    "        ess = np.nan\n",
    "        #ess = az.ess(trace)\n",
    "\n",
    "    #print(f\"R-hat for sampler {sampler_name}: {r_hat}\")\n",
    "    #print(f\"ESS for sampler {sampler_name}: {ess}\")\n",
    "\n",
    "    results.append({\n",
    "        \"eval_level\": eval_level,\n",
    "        \"run_id\": run_id,\n",
    "        varying_attribute: value,\n",
    "        \"sampler\": sampler_name,\n",
    "        \"wasserstein_distance\": mcmc_vs_iid_swd,\n",
    "        \"mmd_rff\": mmd_rff_value,\n",
    "        \"r_hat\": r_hat,\n",
    "        \"ess\": ess,\n",
    "        \"runtime\": runtime\n",
    "    })\n",
    "\n",
    "class PosteriorExample:\n",
    "    \"\"\"Base class for different posterior types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None  # Placeholder for the PyMC model\n",
    "    \n",
    "    def _define_posterior(self):\n",
    "        \"\"\"Subclasses should implement this method to define the posterior.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _define_posterior()\")\n",
    "\n",
    "    def run_sampling(self, sampler_name, num_samples=2000, tune=1000, num_chains=2, eval_mode=None, initvals=None, run_id=None, plot_first_sample=None, init_folder=None, value=None, means=None, posterior_type=None, run_random_seed=None):\n",
    "        \"\"\"Runs MCMC sampling using the chosen sampler.\"\"\"\n",
    "\n",
    "        with self.model:\n",
    "\n",
    "            if sampler_name == \"SMC\":\n",
    "                trace = pm.sample_smc(num_samples, chains=num_chains, progressbar=False, random_seed=run_random_seed)\n",
    "            else:\n",
    "                \n",
    "                # Define which sampler to use\n",
    "                if sampler_name == \"Metro\":\n",
    "                    sampler = pm.Metropolis()\n",
    "                elif sampler_name == \"HMC\":\n",
    "                    sampler = pm.NUTS()\n",
    "                elif sampler_name == \"DEMetro\":\n",
    "                    sampler = pm.DEMetropolis()\n",
    "                elif sampler_name == \"DEMetro_Z\":\n",
    "                    sampler = pm.DEMetropolisZ()\n",
    "                elif sampler_name == \"Slice\":\n",
    "                    sampler = pm.Slice()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampler: {sampler_name}\")\n",
    "\n",
    "                if run_id == 1:\n",
    "                    discard_tuned_samples = False\n",
    "                else:\n",
    "                    discard_tuned_samples = True\n",
    "\n",
    "                if initvals is not None:\n",
    "                    trace = pm.sample(num_samples, tune=tune, step=sampler,initvals=initvals, chains=num_chains, return_inferencedata=True, discard_tuned_samples=discard_tuned_samples, progressbar=False, random_seed=run_random_seed)   \n",
    "                else:\n",
    "                    trace = pm.sample(num_samples, tune=tune, step=sampler, chains=num_chains, return_inferencedata=True, discard_tuned_samples=discard_tuned_samples, progressbar=False, random_seed=run_random_seed)\n",
    "\n",
    "                if run_id == 1 and plot_first_sample and eval_mode == \"pooled\":\n",
    "                    first_warmup_samples = trace.warmup_posterior[\"posterior\"].isel(draw=0).values\n",
    "                    dim = first_warmup_samples.shape[1] if first_warmup_samples.ndim > 1 else 1\n",
    "\n",
    "                    warmup_info = {\n",
    "                        \"sampler\": sampler_name,\n",
    "                        \"value\": value,\n",
    "                        \"means_array\": means,\n",
    "                        \"case\": \"Warmup Samples\",\n",
    "                        \"dim\": dim,\n",
    "                        \"samples\": first_warmup_samples.tolist(),\n",
    "                    }\n",
    "\n",
    "                    # Define file paths\n",
    "                    parent_folder = os.path.join(init_folder, f\"{sampler_name}\")\n",
    "                    create_directories(parent_folder)\n",
    "                    warmup_base = os.path.join(parent_folder, \"first warm up samples\")\n",
    "                    warmup_json_path = f\"{warmup_base}.json\"\n",
    "                    warmup_plot_path = f\"{warmup_base}.pdf\"\n",
    "\n",
    "                    save_sample_info(sample_info=warmup_info, json_path=warmup_json_path, plot_path=warmup_plot_path, label=\"Samples\")\n",
    "\n",
    "                    # also plot first posterior sample\n",
    "                    first_posterior_samples = trace.posterior[\"posterior\"].isel(draw=0).values\n",
    "                    posterior_info = {\n",
    "                        \"sampler\": sampler_name,\n",
    "                        \"value\": value,\n",
    "                        \"means_array\": means,\n",
    "                        \"case\": \"Posterior Samples\",\n",
    "                        \"dim\": dim,\n",
    "                        \"samples\": first_posterior_samples.tolist(),\n",
    "                    }\n",
    "                    \n",
    "                    # Define file paths\n",
    "                    posterior_base = os.path.join(parent_folder, \"first posterior samples\")\n",
    "                    posterior_json_path = f\"{posterior_base}.json\"\n",
    "                    posterior_plot_path = f\"{posterior_base}.pdf\"\n",
    "                    save_sample_info(sample_info=posterior_info, json_path=posterior_json_path, plot_path=posterior_plot_path, label=\"Samples\")\n",
    "                    \n",
    "        return trace\n",
    "\n",
    "\n",
    "class SinglePosterior(PosteriorExample):\n",
    "    def __init__(self, dist_name, dist_params, low=None, high= None, use_smc=False):\n",
    "        \"\"\"\n",
    "        A flexible class for defining unimodal posteriors.\n",
    "\n",
    "        Parameters:\n",
    "        - dist_name: String specifying the name of the PyMC distribution (e.g., \"Normal\", \"StudentT\").\n",
    "        - dist_params: Dictionary containing the parameters for the distribution.\n",
    "        \"\"\"\n",
    "        self.dist_name = dist_name\n",
    "        self.dist_params = dist_params\n",
    "        self.use_smc = use_smc\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        super().__init__()\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "    def _define_posterior(self):\n",
    "       \n",
    "        dim = get_posterior_dim(self.dist_name, self.dist_params)\n",
    "        shape = (dim,) if dim > 1 else ()\n",
    "       \n",
    "        # Retrieve the distribution class from PyMC\n",
    "        dist_class = getattr(pm, self.dist_name)   \n",
    "        dist = dist_class.dist(**self.dist_params, shape=shape)\n",
    "        logp_func = lambda x: pm.logp(dist, x).sum() \n",
    "\n",
    "        if dim == 1 and self.low is not None and self.high is not None:\n",
    "            low = self.low.item() if isinstance(self.low, np.ndarray) else self.low\n",
    "            high = self.high.item() if isinstance(self.high, np.ndarray) else self.high\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "\n",
    "        with pm.Model() as model:\n",
    "            if self.use_smc:\n",
    "                x = pm.Uniform(\"posterior\", lower=self.low, upper=self.high, shape=shape)\n",
    "                pm.Potential(\"logp\", logp_func(x))\n",
    "            else:\n",
    "                dist_class(\"posterior\", **self.dist_params, shape=shape)\n",
    "\n",
    "        return model\n",
    "        \n",
    "\n",
    "class MixturePosterior(PosteriorExample):\n",
    "    def __init__(self, component_types, component_params, weights=None, varying_component=None, low=None, high=None, use_smc=False): \n",
    "        \"\"\"\n",
    "        A flexible mixture posterior allowing any number of components and arbitrary distributions.\n",
    "\n",
    "        Parameters:\n",
    "        - component_types: List of strings specifying the type of each component (e.g., [\"normal\", \"beta\"]).\n",
    "        - component_params: List of dictionaries, where each dictionary contains the parameters for the corresponding distribution.\n",
    "        - weights: List of weights for the mixture components (defaults to uniform).\n",
    "        \"\"\"\n",
    "        if len(component_types) != len(component_params):\n",
    "            raise ValueError(\"Each component type must have a corresponding parameter dictionary.\")\n",
    "\n",
    "        if weights is None:\n",
    "            weights = np.ones(len(component_types))  # Default: Equal weights\n",
    "\n",
    "        if len(weights) != len(component_types):\n",
    "            raise ValueError(\"Number of weights must match number of components.\")\n",
    "\n",
    "        self.component_types = component_types\n",
    "        self.component_params = component_params\n",
    "        self.weights = weights\n",
    "        self.use_smc = use_smc\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "        # Normalize weights\n",
    "        self.weights = np.array(self.weights) / np.sum(self.weights)\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "\n",
    "    def _define_posterior(self):\n",
    "\n",
    "        first_type = self.component_types[0]\n",
    "        first_params = self.component_params[0]\n",
    "\n",
    "        dim = get_posterior_dim(first_type, first_params)\n",
    "        shape = (dim,) if dim > 1 else ()\n",
    "\n",
    "        if dim == 1 and self.low is not None and self.high is not None:\n",
    "            low = self.low.item() if isinstance(self.low, np.ndarray) else self.low\n",
    "            high = self.high.item() if isinstance(self.high, np.ndarray) else self.high\n",
    "            self.low = low\n",
    "            self.high = high\n",
    "        \n",
    "        # Construct component distributions dynamically\n",
    "        components = []\n",
    "        for dist_type, params in zip(self.component_types, self.component_params):\n",
    "                dist_class = getattr(pm, dist_type)  \n",
    "                components.append(dist_class.dist(**params, shape=shape)) \n",
    "        \n",
    "        # Construct logp_func for mixtures\n",
    "        tensor_weights = pt.as_tensor_variable(self.weights)\n",
    "        logp_func = get_logp_func(tensor_weights, components)\n",
    "\n",
    "        # Define the mixture model    \n",
    "        with pm.Model() as model:\n",
    "            # Mixture model\n",
    "            if self.use_smc:\n",
    "                x = pm.Uniform(\"posterior\", lower=self.low, upper=self.high, shape=shape)\n",
    "                pm.Potential(\"logp\", logp_func(x))\n",
    "            else:\n",
    "                pm.Mixture(\"posterior\", w=self.weights, comp_dists=components, shape=shape) \n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "class CustomPosterior(PosteriorExample):\n",
    "    \"\"\"\n",
    "    A flexible class to define custom posteriors using a user-specified log-probability function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logp_func):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - logp_func: Callable function that defines the log-probability.\n",
    "                     Must accept PyMC symbolic variables.\n",
    "        - param_names: List of parameter names required by logp_func.\n",
    "        - initvals: Optional dictionary for initial values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logp_func = logp_func\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "    def _define_posterior(self):\n",
    "        with pm.Model() as model:\n",
    "\n",
    "            # Define the custom distribution using pm.CustomDist\n",
    "            pm.CustomDist(\"posterior\", logp=self.logp_func)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    results_folder,\n",
    "    png_folder,\n",
    "    experiment_settings,\n",
    "    posterior_type,\n",
    "    config_descr,\n",
    "    runs,\n",
    "    varying_attribute, \n",
    "    varying_values,      \n",
    "    num_samples,\n",
    "    num_chains,\n",
    "    init_scheme=None,\n",
    "    base_random_seed=None,\n",
    "    unimodal_init_margin=None,\n",
    "    progress_bar=None,\n",
    "    group_name=\"default\",\n",
    "    **posterior_kwargs\n",
    "):\n",
    "    \n",
    "    set_logging_level(experiment_settings.get(\"logging_level\", \"INFO\"))\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    logger.info(f\"===== Config {config_descr} started! =====\")\n",
    "\n",
    "    # Initialize random number generator\n",
    "    rng = np.random.default_rng(base_random_seed)\n",
    "\n",
    "    samples_per_chain = \"varies\" if varying_attribute in [\"num_samples\", \"num_chains\"] else num_samples // num_chains\n",
    "    # Adjust total to match per-chain sample count\n",
    "    num_samples = samples_per_chain*num_chains\n",
    "\n",
    "    component_index = posterior_kwargs.get(\"varying_component\")\n",
    "\n",
    "    # Number of IID batches for the IID vs IID comparison\n",
    "    num_iid_vs_iid_batches = 2*runs\n",
    "    num_mcmc_batches = runs\n",
    "    # Total number of iid batches (needs a fresh iid batch for each mcmc run)\n",
    "    num_total_iid_batches = num_iid_vs_iid_batches + num_mcmc_batches\n",
    "\n",
    "    # Define required parameters for each posterior type\n",
    "    required_parameters = {\n",
    "        \"Mixture\": [\"component_types\", \"component_params\", \"weights\"],\n",
    "        \"Cauchy\": [\"alpha\", \"beta\"],\n",
    "        \"Beta\": [\"a\", \"b\"],\n",
    "        \"Normal\": [\"mu\", \"sigma\"],\n",
    "        \"SkewNormal\": [\"mu\", \"sigma\", \"alpha\"],\n",
    "        \"StudentT\": [\"nu\", \"mu\", \"sigma\"],\n",
    "        \"SkewStudentT\": [\"a\", \"b\", \"mu\", \"sigma\"],\n",
    "        \"Laplace\": [\"mu\", \"b\"],\n",
    "        \"MvNormal\": [\"mu\", \"cov\"],\n",
    "        \"MvStudentT\": [\"nu\", \"mu\", \"scale\"],\n",
    "        \"Custom\": []\n",
    "    }\n",
    "\n",
    "    # Validate that required keys exist (except for varying attribute)\n",
    "    required_keys = [k for k in required_parameters.get(posterior_type) if k != varying_attribute]\n",
    "    if not all(k in posterior_kwargs for k in required_keys):\n",
    "        raise ValueError(f\"{posterior_type} posterior requires {required_keys}\")\n",
    "\n",
    "    # Create keyword arguments for IID sample generation\n",
    "    iid_kwargs = {key: posterior_kwargs.get(key, \"varies\") for key in required_parameters.get(posterior_type)}\n",
    " \n",
    "    logger.debug(f\"Using IID sample settings: {iid_kwargs}\")\n",
    "\n",
    "    # Create configuration and histogram folders inside the experiment root\n",
    "    group_folder = os.path.join(results_folder, group_name, config_descr)\n",
    "    init_folder = os.path.join(group_folder, f\"init_info\")\n",
    "    runs_folder = os.path.join(group_folder, f\"runs ({runs})\")\n",
    "    png_folder_kde = os.path.join(png_folder, \"IID_KDE_and_Histograms\")\n",
    "    create_directories(group_folder, init_folder, runs_folder, png_folder_kde)\n",
    "\n",
    "    if varying_attribute in [\"dimension\", \"correlation\", \"circle_radius\", \"circle_modes\"]:\n",
    "        posterior_kwargs_original = copy.deepcopy(posterior_kwargs)\n",
    "        iid_kwargs_original = copy.deepcopy(iid_kwargs)\n",
    "        iid_posteriors_folder  = os.path.join(init_folder, \"iid_posteriors\")\n",
    "        regular_posteriors_folder = os.path.join(init_folder, \"regular_posteriors\")\n",
    "        create_directories(iid_posteriors_folder, regular_posteriors_folder)\n",
    "    else:\n",
    "        posterior_kwargs_original = None\n",
    "        iid_kwargs_original = None\n",
    "        iid_posteriors_folder = None\n",
    "        regular_posteriors_folder = None\n",
    "\n",
    "\n",
    "    experiment_metadata = {\n",
    "        \"config_descr\": config_descr,\n",
    "        \"runs\": runs,\n",
    "        \"total_iid_batches\": num_total_iid_batches,\n",
    "        \"iid_vs_iid_comparisons\": num_iid_vs_iid_batches // 2,  \n",
    "        \"mcmc_vs_iid_comparisons\": num_mcmc_batches,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"num_chains\": num_chains,\n",
    "        \"samples_per_chain\": samples_per_chain,\n",
    "        \"posterior_type\": posterior_type,\n",
    "        \"varying_attribute\": varying_attribute,\n",
    "        \"varying_values\": varying_values,\n",
    "        \"init_scheme\": init_scheme,\n",
    "        \"base_random_seed\": base_random_seed,\n",
    "        \"git_tag\": get_git_tag(),\n",
    "    }\n",
    "\n",
    "    # Add posterior-specific parameters\n",
    "    experiment_metadata.update(iid_kwargs)  \n",
    "\n",
    "    # Save metadata\n",
    "    metadata_filename = os.path.join(group_folder, f\"metadata_config_{config_descr}.json\")\n",
    "    safe_json_dump(experiment_metadata, metadata_filename)\n",
    "\n",
    "    # generate iid batches (not needed for Custom posterior, since no iid samples available)\n",
    "    if posterior_type != \"Custom\":\n",
    "\n",
    "        iid_batches_dict, iid_ref_stats_dict = generate_all_iid_batches(\n",
    "            posterior_type=posterior_type,\n",
    "            posterior_kwargs=posterior_kwargs,\n",
    "            iid_kwargs_original=iid_kwargs_original,\n",
    "            iid_kwargs=iid_kwargs,\n",
    "            iid_posteriors_folder=iid_posteriors_folder,\n",
    "            varying_attribute=varying_attribute,\n",
    "            varying_values=varying_values,\n",
    "            num_total_iid_batches=num_total_iid_batches,\n",
    "            num_iid_vs_iid_batches=num_iid_vs_iid_batches,\n",
    "            num_samples=num_samples,\n",
    "            num_chains=num_chains,\n",
    "            rng=rng,\n",
    "            group_folder=group_folder,\n",
    "            png_folder=png_folder_kde,\n",
    "            required_parameters=required_parameters \n",
    "        )       \n",
    "\n",
    "    # Define fixed colors for each sampler\n",
    "    sampler_colors = {\n",
    "        \"Metro\": \"blue\",\n",
    "        \"HMC\": \"red\",\n",
    "        \"DEMetro\": \"green\",\n",
    "        \"DEMetro_Z\": \"purple\",\n",
    "        \"SMC\": \"orange\",\n",
    "    }\n",
    "\n",
    "    # move SMC to the end of the samplers list for efficient building of PyMC models\n",
    "    samplers = list(experiment_settings[\"samplers\"]) \n",
    "    if \"SMC\" in samplers:\n",
    "        samplers.remove(\"SMC\")\n",
    "        samplers.append(\"SMC\")\n",
    "\n",
    "    chain_eval_mode = \"chain\"\n",
    "    pooled_eval_mode = \"pooled\"\n",
    "\n",
    "    png_folder_init_chain = os.path.join(png_folder, \"chain_init\")\n",
    "    png_folder_init_pooled = os.path.join(png_folder, \"pooled_init\")\n",
    "    \n",
    "    create_directories(png_folder_init_chain, png_folder_init_pooled)\n",
    "\n",
    "    plot_first_sample = experiment_settings.get(\"plot_first_sample\", False)\n",
    "    df_all_runs = []\n",
    "\n",
    "    # === Run the Experiment ===\n",
    "    for run_id in range(1, runs + 1):\n",
    "        logger.info(f\"Running {config_descr} - Run {run_id}\")\n",
    "\n",
    "        run_random_seed = int(rng.integers(1_000_000))\n",
    "        run_rng = np.random.default_rng(run_random_seed)\n",
    "\n",
    "        run_folder = os.path.join(runs_folder, f\"run_{run_id}\")\n",
    "        traces_folder = os.path.join(run_folder, \"trace_plots\")\n",
    "        create_directories(run_folder,traces_folder)\n",
    "\n",
    "        if experiment_settings.get(\"save_plots_and_csv_per_run\", False):\n",
    "            csv_folder = os.path.join(run_folder, \"result CSVs\")\n",
    "            csv_pool   = os.path.join(csv_folder,  \"pooled\")\n",
    "            csv_chain  = os.path.join(csv_folder,  \"chain\")\n",
    "            plots_folder = os.path.join(run_folder, \"plots_of_run\")\n",
    "            plot_pool  = os.path.join(plots_folder, \"pooled_global_plots\")\n",
    "            plot_chain = os.path.join(plots_folder, \"chain_global_plots\")\n",
    "            \n",
    "            create_directories( csv_folder, plots_folder,\n",
    "                               csv_pool, csv_chain, plot_pool, plot_chain)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for value in varying_values:\n",
    "\n",
    "            var_attr_folder = os.path.join(traces_folder, f\"{varying_attribute}_{value}\")\n",
    "            create_directories(var_attr_folder)\n",
    "\n",
    "            if run_id == 1:\n",
    "                # create subfolder for value in init folder\n",
    "                init_value_folder = os.path.join(init_folder, f\"{varying_attribute}_{value}\")\n",
    "                create_directories(init_value_folder)\n",
    "\n",
    "            # Handle parameter changes for Mixture case\n",
    "            if posterior_type == \"Mixture\":\n",
    "\n",
    "                is_studentt = posterior_kwargs[\"component_types\"][0] == \"MvStudentT\"\n",
    "                cov_param_key = \"scale\" if is_studentt else \"cov\"\n",
    "\n",
    "                if varying_attribute == \"weights\":\n",
    "                    posterior_kwargs[\"weights\"] = value\n",
    "                elif varying_attribute == \"dimension\":\n",
    "                    adjust_dimension_of_kwargs(posterior_type, posterior_kwargs_original, posterior_kwargs, target_dim=value, required_parameters=required_parameters)\n",
    "                    if run_id == 1:\n",
    "                            save_adjusted_posterior_config(\n",
    "                                posterior_kwargs,\n",
    "                                folder=regular_posteriors_folder,\n",
    "                                dim_value=value\n",
    "                            )\n",
    "                elif varying_attribute == \"circle_radius\":\n",
    "                    posterior_kwargs[\"component_params\"], \\\n",
    "                    posterior_kwargs[\"component_types\"], \\\n",
    "                    posterior_kwargs[\"weights\"] = adjust_circle_layout(\n",
    "                        posterior_kwargs[\"component_number\"],\n",
    "                        posterior_kwargs[\"component_type\"],\n",
    "                        value,\n",
    "                        posterior_kwargs[cov_param_key],\n",
    "                        posterior_kwargs[\"weight_type\"]\n",
    "                    )\n",
    "                    if run_id == 1:\n",
    "                        save_adjusted_posterior_config(\n",
    "                            posterior_kwargs,\n",
    "                            folder=regular_posteriors_folder,\n",
    "                            dim_value=value\n",
    "                        )\n",
    "                elif varying_attribute == \"circle_modes\":\n",
    "                    posterior_kwargs[\"component_params\"], \\\n",
    "                    posterior_kwargs[\"component_types\"], \\\n",
    "                    posterior_kwargs[\"weights\"] = adjust_circle_layout(\n",
    "                        value,\n",
    "                        posterior_kwargs[\"component_type\"],\n",
    "                        posterior_kwargs[\"radius\"],\n",
    "                        posterior_kwargs[cov_param_key],\n",
    "                        posterior_kwargs[\"weight_type\"]\n",
    "                    )\n",
    "                    if run_id == 1:\n",
    "                        save_adjusted_posterior_config(\n",
    "                            posterior_kwargs,\n",
    "                            folder=regular_posteriors_folder,\n",
    "                            dim_value=value\n",
    "                        )\n",
    "                elif varying_attribute == \"init_scheme\":\n",
    "                    init_scheme = value\n",
    "                elif varying_attribute == \"correlation\":\n",
    "                    posterior_dim = get_posterior_dim(posterior_type, posterior_kwargs)\n",
    "\n",
    "                    for i in range(len(posterior_kwargs[\"component_params\"])):\n",
    "                        \n",
    "                        posterior_kwargs[\"component_params\"][i][cov_param_key] = build_correlation_cov_matrix(posterior_dim, value)\n",
    " \n",
    "                    \n",
    "                    if run_id == 1:\n",
    "                        save_adjusted_posterior_config(\n",
    "                                posterior_kwargs,\n",
    "                                folder=regular_posteriors_folder,\n",
    "                                dim_value=value\n",
    "                        )\n",
    "                elif varying_attribute == \"num_samples\":\n",
    "                    num_samples = value\n",
    "                elif varying_attribute == \"num_chains\":\n",
    "                    num_chains = value\n",
    "                else:\n",
    "                    # Vary only the selected component's parameter\n",
    "                    posterior_kwargs[\"component_params\"][component_index][varying_attribute] = value\n",
    "\n",
    "            else:\n",
    "\n",
    "                is_studentt = posterior_type == \"MvStudentT\"\n",
    "                cov_param_key = \"scale\" if is_studentt else \"cov\"\n",
    "\n",
    "                # Handle parameter changes for single posteriors\n",
    "                if varying_attribute == \"dimension\":\n",
    "                    adjust_dimension_of_kwargs(posterior_type, posterior_kwargs_original, posterior_kwargs, target_dim=value, required_parameters=required_parameters)\n",
    "                    if run_id == 1:\n",
    "                            save_adjusted_posterior_config(\n",
    "                                posterior_kwargs,\n",
    "                                folder=regular_posteriors_folder,\n",
    "                                dim_value=value\n",
    "                            )                     \n",
    "                elif varying_attribute == \"init_scheme\":\n",
    "                    init_scheme = value\n",
    "                elif varying_attribute == \"correlation\":\n",
    "                    posterior_dim = get_posterior_dim(posterior_type, posterior_kwargs)\n",
    "\n",
    "                    posterior_kwargs[cov_param_key] = build_correlation_cov_matrix(posterior_dim, value)\n",
    "\n",
    "                    if run_id == 1:\n",
    "                        save_adjusted_posterior_config(\n",
    "                            posterior_kwargs,\n",
    "                            folder=regular_posteriors_folder,\n",
    "                            dim_value=value\n",
    "                        )\n",
    "                elif varying_attribute == \"num_samples\":\n",
    "                    num_samples = value\n",
    "                elif varying_attribute == \"num_chains\":\n",
    "                    num_chains = value\n",
    "                else:\n",
    "                    # Vary only the specific parameter\n",
    "                    posterior_kwargs[varying_attribute] = value\n",
    "\n",
    "            # Ensure num_samples is normalized in case of varying num_chains or num_samples\n",
    "            samples_per_chain = num_samples // num_chains\n",
    "            num_samples = samples_per_chain*num_chains\n",
    "            \n",
    "            # base_posterior used for all samplers but SMC\n",
    "            if posterior_type == \"Mixture\":\n",
    "                base_posterior = MixturePosterior(\n",
    "                    component_types=posterior_kwargs[\"component_types\"],\n",
    "                    component_params=posterior_kwargs[\"component_params\"],\n",
    "                    weights=posterior_kwargs[\"weights\"],\n",
    "                )\n",
    "            elif posterior_type == \"Custom\":\n",
    "                logp_func = posterior_kwargs[\"logp_func\"]\n",
    "                base_posterior = CustomPosterior(logp_func=logp_func)\n",
    "            else:\n",
    "                base_posterior = SinglePosterior(dist_name=posterior_type, dist_params=posterior_kwargs)\n",
    "\n",
    "            means = None\n",
    "            init_pooled = None\n",
    "            init_chain = None\n",
    "  \n",
    "            if init_scheme is not None:\n",
    "                    means = extract_means_from_posterior(posterior_type, posterior_kwargs)\n",
    "                    init_pooled = get_initvals(init_scheme, means, pooled_eval_mode, num_chains, rng, run_id, init_value_folder, png_folder_init_pooled, varying_attribute, value, unimodal_init_margin=unimodal_init_margin)\n",
    "                    init_chain = get_initvals(init_scheme, means, chain_eval_mode, 1, rng, run_id, init_value_folder, png_folder_init_chain, varying_attribute, value, unimodal_init_margin=unimodal_init_margin)\n",
    "        \n",
    "            # Get IID samples for the current varying value\n",
    "            if posterior_type != \"Custom\" and varying_attribute not in [\"init_scheme\", \"num_chains\"]:\n",
    "                iid_batches = iid_batches_dict[value]\n",
    "            elif posterior_type == \"Custom\":\n",
    "                iid_batches = None\n",
    "\n",
    "\n",
    "            # Run sampling for all samplers\n",
    "            for sampler_name in samplers:\n",
    "\n",
    "                use_smc = sampler_name == \"SMC\"\n",
    "\n",
    "                 # Reuse model if not SMC\n",
    "                if not use_smc:\n",
    "                    posterior = base_posterior\n",
    "                else:\n",
    "                     # Rebuild for SMC \n",
    "                    if np.isscalar(means[0]):\n",
    "                            # shape (n_modes, 1)\n",
    "                            means_array = np.array(means)[:, None] \n",
    "                    else:\n",
    "                            means_array = np.array(means)\n",
    "\n",
    "                    if posterior_type == \"Mixture\":\n",
    "                        #compute higher and lower bound for init prior\n",
    "                        low, high,_,_,_  = get_uniform_prior_bounds(means_array=means_array, expansion_factor=0.25)   \n",
    "                        posterior = MixturePosterior(\n",
    "                            component_types=posterior_kwargs[\"component_types\"],\n",
    "                            component_params=posterior_kwargs[\"component_params\"],\n",
    "                            weights=posterior_kwargs[\"weights\"],\n",
    "                            use_smc=True,\n",
    "                            low=low,\n",
    "                            high=high\n",
    "                        )         \n",
    "\n",
    "                    else :\n",
    "                        # compute higher and lower bound for init prior\n",
    "                        low, high, _,_,_ = get_uniform_prior_bounds(means_array=means_array, expansion_factor=0.25, unimodal_init_margin=unimodal_init_margin)\n",
    "                        posterior = SinglePosterior(dist_name=posterior_type, dist_params=posterior_kwargs, use_smc=True, low=low, high=high)\n",
    "\n",
    "\n",
    "                pooled_seed = int(run_rng.integers(1_000_000))\n",
    "\n",
    "                # **Measure Computation Time**\n",
    "                start_time = time.time()\n",
    "\n",
    "                pooled_trace = posterior.run_sampling(\n",
    "                    sampler_name=sampler_name,\n",
    "                    num_samples=samples_per_chain,\n",
    "                    num_chains=num_chains, \n",
    "                    eval_mode=pooled_eval_mode,\n",
    "                    initvals=init_pooled, \n",
    "                    run_id=run_id, \n",
    "                    plot_first_sample=plot_first_sample,\n",
    "                    init_folder=init_value_folder, \n",
    "                    value=value, \n",
    "                    means=means, \n",
    "                    posterior_type=posterior_type, \n",
    "                    run_random_seed=pooled_seed\n",
    "                )\n",
    "\n",
    "\n",
    "                # 2*runs have already been used for iid vs iid comparison\n",
    "                fresh_iid_index = num_iid_vs_iid_batches + run_id-1\n",
    "                iid_batch = ensure_2d(iid_batches[fresh_iid_index]) \n",
    "\n",
    "\n",
    "                end_time = time.time()\n",
    "                pooled_runtime = end_time - start_time\n",
    "\n",
    "\n",
    "                eval_trace(trace=pooled_trace, runtime=pooled_runtime, eval_level=\"pooled\", run_id=run_id, sampler_name=sampler_name, value=value,\n",
    "                           posterior_type=posterior_type, iid_batch=iid_batch,\n",
    "                           experiment_settings=experiment_settings, folders={ \"var_attr_folder\": var_attr_folder},\n",
    "                           varying_attribute=varying_attribute, results=results, rng=run_rng)\n",
    "\n",
    "\n",
    "                chain_seed = int(run_rng.integers(1_000_000))\n",
    "\n",
    "                # **Measure Computation Time**\n",
    "                start_time = time.time()\n",
    "\n",
    "                chain_trace = posterior.run_sampling(\n",
    "                    sampler_name=sampler_name,\n",
    "                    num_samples=num_samples,\n",
    "                    num_chains=1,\n",
    "                    eval_mode=chain_eval_mode, \n",
    "                    initvals=init_chain, \n",
    "                    run_id=run_id, \n",
    "                    plot_first_sample=plot_first_sample,\n",
    "                    init_folder=init_value_folder, \n",
    "                    value=value, \n",
    "                    means=means, \n",
    "                    posterior_type=posterior_type, \n",
    "                    run_random_seed=chain_seed\n",
    "                )\n",
    "\n",
    "                end_time = time.time()\n",
    "                chain_runtime = end_time - start_time\n",
    "\n",
    "                eval_trace(trace=chain_trace, runtime=chain_runtime, eval_level=\"chain\", run_id=run_id, sampler_name=sampler_name, value=value,\n",
    "                        posterior_type=posterior_type, iid_batch=iid_batch,\n",
    "                        experiment_settings=experiment_settings, folders={ \"var_attr_folder\": var_attr_folder},\n",
    "                        varying_attribute=varying_attribute, results=results, rng=run_rng)\n",
    "                \n",
    "\n",
    "            # Now increments the TQDM progress bar if it's provided\n",
    "            if progress_bar is not None:\n",
    "                progress_bar.update(1)\n",
    "\n",
    "        # Convert results to DataFrame and save\n",
    "        df_results = pd.DataFrame(results)\n",
    "\n",
    "        var_attr_is_tuple = False\n",
    "\n",
    "        # Handle tuple-based attributes consistently\n",
    "        if isinstance(df_results[varying_attribute].iloc[0], tuple):\n",
    "            var_attr_is_tuple = True\n",
    "            df_results[varying_attribute] = df_results[varying_attribute].apply(str)\n",
    "\n",
    "        df_results = df_results.sort_values(varying_attribute, ascending=True)\n",
    "\n",
    "        df_pooled = df_results.query(\"eval_level == 'pooled'\").copy()\n",
    "        df_chain  = df_results.query(\"eval_level == 'chain'\").copy()\n",
    "\n",
    "        if experiment_settings.get(\"save_plots_and_csv_per_run\", False):\n",
    "\n",
    "            # pooled\n",
    "            plot_and_save_all_metrics(\n",
    "                df_results=df_pooled,\n",
    "                sampler_colors=sampler_colors,\n",
    "                varying_attribute=varying_attribute,\n",
    "                varying_attribute_for_plot=varying_attribute,\n",
    "                csv_folder=csv_pool,\n",
    "                plots_folder=plot_pool,\n",
    "                run_id=run_id,\n",
    "                config_descr=f\"{config_descr}_pooled\"\n",
    "            )\n",
    "\n",
    "            # chain\n",
    "            plot_and_save_all_metrics(\n",
    "                df_results=df_chain,\n",
    "                sampler_colors=sampler_colors,\n",
    "                varying_attribute=varying_attribute,\n",
    "                varying_attribute_for_plot=varying_attribute,\n",
    "                csv_folder=csv_chain,\n",
    "                plots_folder=plot_chain,\n",
    "                run_id=run_id,\n",
    "                config_descr=f\"{config_descr}_chain\"\n",
    "            )\n",
    "\n",
    "        df_all_runs.append(df_results)\n",
    "\n",
    "    logger.info(\"All runs completed successfully.\")\n",
    "\n",
    "    # ===== GLOBAL RESULTS FOLDER =====\n",
    "    global_folder = os.path.join(group_folder, \"global_results\")\n",
    "    create_directories(global_folder)\n",
    "    \n",
    "    # Combine all results into a single data frame \n",
    "    df_all_runs = pd.concat(df_all_runs, ignore_index=True)\n",
    "\n",
    "    df_pooled = df_all_runs[df_all_runs[\"eval_level\"] == \"pooled\"]   \n",
    "    df_chain = df_all_runs[df_all_runs[\"eval_level\"] == \"chain\"]\n",
    "\n",
    "\n",
    "    if var_attr_is_tuple:\n",
    "        iid_ref_stats_dict = {str(k): v for k, v in iid_ref_stats_dict.items()}\n",
    "\n",
    "    pooled_results_folder = os.path.join(global_folder, \"pooled_results\")\n",
    "    pooled_plots_folder   = os.path.join(global_folder, \"pooled_plots\")\n",
    "    png_folder_pooled = os.path.join(png_folder, \"pooled_global_plots\")\n",
    "    create_directories(pooled_results_folder, pooled_plots_folder, png_folder_pooled)\n",
    "\n",
    "    scatter_overlay = experiment_settings.get(\"scatter_overlay\", False)\n",
    "    save_extra_scatter = experiment_settings.get(\"save_extra_scatter\", False)\n",
    "    \n",
    "    compute_and_save_global_metrics(\n",
    "        df_all_runs=df_pooled,\n",
    "        sampler_colors=sampler_colors,\n",
    "        varying_attribute=varying_attribute,\n",
    "        varying_values=varying_values,\n",
    "        runs=runs,\n",
    "        num_chains=num_chains,\n",
    "        config_descr=config_descr + \"_pooled\",\n",
    "        global_results_folder=pooled_results_folder,\n",
    "        global_plots_folder=pooled_plots_folder,\n",
    "        png_folder=png_folder_pooled,\n",
    "        iid_ref_stats_dict=iid_ref_stats_dict,\n",
    "        scatter_overlay=scatter_overlay, \n",
    "        save_extra_scatter=save_extra_scatter\n",
    "    )\n",
    "\n",
    "    chain_results_folder = os.path.join(global_folder, \"chain_results\")\n",
    "    chain_plots_folder   = os.path.join(global_folder, \"chain_plots\")\n",
    "    png_folder_chain = os.path.join(png_folder, \"chain_global_plots\")\n",
    "    create_directories(chain_results_folder, chain_plots_folder, png_folder_chain) \n",
    "\n",
    "    compute_and_save_global_metrics(\n",
    "        df_all_runs=df_chain,\n",
    "        sampler_colors=sampler_colors,\n",
    "        varying_attribute=varying_attribute,\n",
    "        varying_values=varying_values,\n",
    "        runs=runs,\n",
    "        num_chains=num_chains,\n",
    "        config_descr=config_descr + \"_chain\",\n",
    "        global_results_folder= chain_results_folder,\n",
    "        global_plots_folder= chain_plots_folder,\n",
    "        png_folder=png_folder_chain,\n",
    "        iid_ref_stats_dict=iid_ref_stats_dict,\n",
    "        scatter_overlay= scatter_overlay,\n",
    "        save_extra_scatter= save_extra_scatter\n",
    "    )\n",
    "\n",
    "    logger.info(f\"===== Config {config_descr} completed successfully. =====\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations: 1\n",
      "Posterior Variants: 4\n",
      "Default Runs per Posterior: 3\n",
      "Total Runs: 12\n",
      "All configurations are valid. Starting experiments with the following settings and defaults:\n",
      "\n",
      "Experiment Settings:\n",
      "{\n",
      "  \"save_plots_and_csv_per_run\": false,\n",
      "  \"save_traces\": false,\n",
      "  \"trace_plots\": \"first_run_only\",\n",
      "  \"save_individual_traceplots_per_dim\": true,\n",
      "  \"plot_traces_in_notebook\": false,\n",
      "  \"samplers\": [\n",
      "    \"HMC\",\n",
      "    \"SMC\"\n",
      "  ],\n",
      "  \"logging_level\": \"ERROR\",\n",
      "  \"plot_first_sample\": true,\n",
      "  \"scatter_overlay\": false,\n",
      "  \"save_extra_scatter\": false\n",
      "}\n",
      "\n",
      "Defaults:\n",
      "{\n",
      "  \"num_samples\": 4000,\n",
      "  \"num_chains\": 4,\n",
      "  \"base_random_seed\": 42,\n",
      "  \"runs\": 3,\n",
      "  \"init_scheme\": \"thesis_scheme\",\n",
      "  \"unimodal_init_margin\": 100\n",
      "} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:   0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:   8%|▊         | 1/12 [00:18<03:25, 18.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  17%|█▋        | 2/12 [00:40<03:22, 20.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  25%|██▌       | 3/12 [01:14<04:01, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  33%|███▎      | 4/12 [03:18<08:40, 65.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  42%|████▏     | 5/12 [03:33<05:28, 46.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  50%|█████     | 6/12 [03:50<03:41, 36.99s/it]The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-05 17:01:13,042 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  58%|█████▊    | 7/12 [04:13<02:42, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  67%|██████▋   | 8/12 [05:06<02:35, 38.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  75%|███████▌  | 9/12 [05:42<01:53, 37.93s/it]The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-05 17:03:05,529 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  83%|████████▎ | 10/12 [06:01<01:04, 32.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  92%|█████████▏| 11/12 [06:20<00:28, 28.18s/it]The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-05 17:03:42,937 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress: 100%|██████████| 12/12 [06:42<00:00, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data for sampler 'HMC' and metric 'r_hat' – skipping.\n",
      "No data for sampler 'SMC' and metric 'r_hat' – skipping.\n",
      "No data for sampler 'HMC' and metric 'ess' – skipping.\n",
      "No data for sampler 'SMC' and metric 'ess' – skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress: 100%|██████████| 12/12 [06:45<00:00, 33.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment-level HTML report saved to: experiments/exp_SMC_cheater/exp_SMC_cheater_report.html\n",
      "\n",
      "============================\n",
      "Experiment Summary\n",
      "============================\n",
      "Started at:                   2025-07-05 16:57:16\n",
      "Finished at:                  2025-07-05 17:04:02\n",
      "Total duration:               0h 6m 45.5s\n",
      "Output folder:                experiments/exp_SMC_cheater\n",
      "Output folder size:           10.1 MB\n",
      "Total configurations:         1\n",
      "Successful configuration:     1\n",
      "Failed configurations:        0\n",
      "Summary saved to: experiments/exp_SMC_cheater/results/summary.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "experiment_name = \"SMC_cheater\"\n",
    "config_names = [\"Dim_3_runs\"]\n",
    "\n",
    "#config_names = [\"Base\", \"Skew\", \"Corr\", \"Dim\", \"Tails\", \"Multi_Distance\", \"Multi_Location\"]\n",
    "# Define the root directory for all experiments\n",
    "experiment_root_folder = os.path.join(\"experiments\", f\"exp_{experiment_name}\")\n",
    "results_folder = os.path.join(experiment_root_folder, \"results\")\n",
    "configs_folder = os.path.join(experiment_root_folder, \"configs\")\n",
    "report_pngs_folder = os.path.join(results_folder, \"z_html_pngs\")\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(experiment_root_folder):\n",
    "    user_input = input(\n",
    "        f\"Folder '{experiment_root_folder}' already exists and will be overwritten.\\n\"\n",
    "        \"Do you want to continue? (yes/no): \"\n",
    "    ).strip().lower()\n",
    "\n",
    "    if user_input not in [\"yes\", \"y\"]:\n",
    "        print(\"Operation aborted. No files were deleted.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    shutil.rmtree(results_folder)\n",
    "else:\n",
    "    create_directories(experiment_root_folder)\n",
    " \n",
    "create_directories(results_folder)\n",
    "create_directories(report_pngs_folder)\n",
    "\n",
    "# Copy experiment_template folders into experiment_root\n",
    "for subfolder in [\"default_vals\", \"settings\"]:\n",
    "    src = os.path.join(\"experiment_template\", subfolder)\n",
    "    dst = os.path.join(experiment_root_folder, subfolder)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copytree(src, dst)\n",
    "\n",
    "if not os.path.exists(configs_folder):\n",
    "    os.makedirs(configs_folder)\n",
    "\n",
    "# Copy only the configs that are defined in config_names\n",
    "for config_name in config_names:\n",
    "    src = os.path.join(\"experiment_template\", \"configs\", f\"{config_name}.yaml\")\n",
    "    dst = os.path.join(experiment_root_folder, \"configs\", f\"{config_name}.yaml\")\n",
    "    if os.path.exists(src):\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy(src, dst)\n",
    "    else:\n",
    "        print(f\"Warning: Config file {src} not found!\")\n",
    "\n",
    "\n",
    "experiment_paths = get_experiment_paths(config_names, base_dir=os.path.join(experiment_root_folder, \"configs\"))\n",
    "settings_path = os.path.join(experiment_root_folder, \"settings\", \"experiment_settings.yaml\")\n",
    "defaults_path = os.path.join(experiment_root_folder, \"default_vals\", \"attribute_default_vals.yaml\")\n",
    "\n",
    "experiment_settings = load_experiment_settings(settings_path)\n",
    "defaults = load_default_values(defaults_path)\n",
    "\n",
    "experiments = []\n",
    "for path in experiment_paths:\n",
    "    group_name, config_list = load_config_file(path)\n",
    "    config_list = [apply_defaults_to_config(cfg, defaults) for cfg in config_list]\n",
    "    experiments.append((group_name, config_list))\n",
    "\n",
    "total_configs = sum(len(config_list) for _, config_list in experiments)\n",
    "total_posterior_variants = sum(len(config[\"varying_values\"]) for _, config_list in experiments for config in config_list)\n",
    "runs_per_config = defaults.get(\"runs\", 1)\n",
    "total_runs = sum(config[\"runs\"] * len(config[\"varying_values\"]) for _, exp_group in experiments for config in exp_group)\n",
    "\n",
    "\n",
    "print(f\"Configurations: {total_configs}\")\n",
    "print(f\"Posterior Variants: {total_posterior_variants}\")\n",
    "print(f\"Default Runs per Posterior: {runs_per_config}\")\n",
    "print(f\"Total Runs: {total_runs}\")\n",
    "# Validate all configurations before running the experiments\n",
    "for group_name, exp_group in experiments:\n",
    "    for config in exp_group:\n",
    "        validate_config(config)\n",
    "\n",
    "print(\"All configurations are valid. Starting experiments with the following settings and defaults:\")\n",
    "print(\"\\nExperiment Settings:\")\n",
    "print(json.dumps(experiment_settings, indent=2))\n",
    "print(\"\\nDefaults:\")\n",
    "print(json.dumps(defaults, indent=2), \"\\n\")\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "start_dt = datetime.now()\n",
    "\n",
    "failed_configs = []\n",
    "\n",
    "with tqdm(total=total_runs, desc=\"Total experiment progress\") as pbar:\n",
    "    for group_name, exp_group in experiments:\n",
    "        for config in exp_group:\n",
    "            try:\n",
    "                # png folder for html report for each group and config\n",
    "                config_png_folder = os.path.join(results_folder, \"z_html_pngs\", group_name, config[\"config_descr\"])\n",
    "                create_directories(config_png_folder)\n",
    "\n",
    "                run_experiment(\n",
    "                    results_folder,\n",
    "                    config_png_folder,\n",
    "                    experiment_settings,\n",
    "                    posterior_type=config[\"posterior_type\"],\n",
    "                    config_descr=config[\"config_descr\"],\n",
    "                    runs=config[\"runs\"],\n",
    "                    varying_attribute=config[\"varying_attribute\"],\n",
    "                    varying_values=config[\"varying_values\"],\n",
    "                    init_scheme=\"varies\" if config[\"varying_attribute\"] == \"init_scheme\" else config.get(\"init_scheme\"),\n",
    "                    num_samples=\"varies\" if config[\"varying_attribute\"] == \"num_samples\" else config[\"num_samples\"],\n",
    "                    num_chains=\"varies\" if config[\"varying_attribute\"] == \"num_chains\" else config[\"num_chains\"],\n",
    "                    base_random_seed=config.get(\"base_random_seed\"),\n",
    "                    unimodal_init_margin=config.get(\"unimodal_init_margin\"),\n",
    "                    group_name=group_name,\n",
    "                    progress_bar=pbar, \n",
    "                    # Pass remaining keys as posterior_kwargs\n",
    "                    **{k: v for k, v in config.items() if k not in [\n",
    "                        \"config_descr\", \"runs\", \"varying_attribute\", \"varying_values\", \n",
    "                        \"num_samples\", \"num_chains\", \"init_scheme\", \n",
    "                        \"base_random_seed\", \"posterior_type\", \"unimodal_init_margin\"\n",
    "                    ]} \n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in config '{config['config_descr']}': {e}\")\n",
    "                traceback.print_exc()\n",
    "                failed_configs.append((config['config_descr'], str(e)))\n",
    "                \n",
    "\n",
    "end_time = time.time()\n",
    "end_dt = datetime.now()\n",
    "duration = end_time - start_time\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = round(duration % 60, 1)\n",
    "\n",
    "generate_html_report(\n",
    "        experiment_root_folder=experiment_root_folder,\n",
    "        report_pngs_folder=report_pngs_folder,\n",
    "        experiments=experiments,\n",
    "        output_path=os.path.join(experiment_root_folder, f\"exp_{experiment_name}_report.html\")\n",
    "    )\n",
    "\n",
    "def get_folder_size(path='.'):\n",
    "    \"\"\"Compute total size of all files in directory.\"\"\"\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.isfile(fp):\n",
    "                total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "size_bytes = get_folder_size(experiment_root_folder)\n",
    "\n",
    "summary_lines = [\n",
    "    \"\\n============================\",\n",
    "    \"Experiment Summary\",\n",
    "    \"============================\",\n",
    "    f\"Started at:                   {start_dt.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"Finished at:                  {end_dt.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"Total duration:               {hours}h {minutes}m {seconds}s\",\n",
    "    f\"Output folder:                {experiment_root_folder}\",\n",
    "    f\"Output folder size:           {humanize.naturalsize(size_bytes)}\",\n",
    "    f\"Total configurations:         {total_configs}\",\n",
    "    f\"Successful configuration:     {total_configs - len(failed_configs)}\",\n",
    "    f\"Failed configurations:        {len(failed_configs)}\"\n",
    "]\n",
    "\n",
    "if failed_configs:\n",
    "    # summary_lines.append(\"\\n Failed Configurations:\")\n",
    "    for cfg, msg in failed_configs:\n",
    "        summary_lines.append(f\" - {cfg}: {msg}\")\n",
    "\n",
    "# Print to console\n",
    "print(\"\\n\".join(summary_lines))\n",
    "\n",
    "# Also save to summary.txt\n",
    "summary_path = os.path.join(results_folder, \"summary.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_immo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
