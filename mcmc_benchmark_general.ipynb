{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import scipy.stats as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pytensor.tensor as pt\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import shutil \n",
    "import subprocess\n",
    "import traceback\n",
    "import time\n",
    "from datetime import datetime\n",
    "import humanize \n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "logging.getLogger(\"arviz\").setLevel(logging.CRITICAL)\n",
    "\n",
    "logger = logging.getLogger(\"pymc\")\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def set_logging_level(level_name):\n",
    "    level = getattr(logging, level_name.upper(), logging.INFO)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # remove all existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    handler= logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    handler.setFormatter(formatter)\n",
    "    handler.setLevel(level)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "# Function to get the current git tag\n",
    "def get_git_tag():\n",
    "        try:\n",
    "            tag = subprocess.check_output([\"git\", \"describe\", \"--tags\"], stderr=subprocess.DEVNULL).strip().decode()\n",
    "            return tag\n",
    "        except subprocess.CalledProcessError:\n",
    "            return \"No tag found\"\n",
    "        \n",
    "\n",
    "def create_directories(*paths):\n",
    "    \"\"\"Creates multiple directories if they don't exist.\"\"\"\n",
    "    for path in paths:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def ensure_2d(arr):\n",
    "    \"\"\"Ensures array shape is (N, d), even if 1D.\"\"\"\n",
    "    arr = np.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr[:, np.newaxis]\n",
    "    else:\n",
    "        return arr.reshape(-1, arr.shape[-1])\n",
    "\n",
    "def get_posterior_dim(posterior_type, params):\n",
    "    \"\"\"\n",
    "    Robustly determines the dimensionality of a posterior from its parameters.\n",
    "    \"\"\"\n",
    "    if posterior_type == \"Mixture\":\n",
    "        # Check only the first component (assuming all have same dimension)\n",
    "        comp_type = params[\"component_types\"][0]\n",
    "        comp_params = params[\"component_params\"][0]\n",
    "        return get_posterior_dim(comp_type, comp_params)\n",
    "\n",
    "    if \"mu\" in params:\n",
    "        mu = np.array(params[\"mu\"])\n",
    "        return mu.shape[0] if mu.ndim > 0 else 1\n",
    "    elif \"loc\" in params:\n",
    "        loc = np.array(params[\"loc\"])\n",
    "        return loc.shape[0] if loc.ndim > 0 else 1\n",
    "    elif posterior_type == \"Cauchy\" and \"alpha\" in params:\n",
    "        alpha = np.array(params[\"alpha\"])\n",
    "        return alpha.shape[0] if alpha.ndim > 0 else 1\n",
    "    elif posterior_type == \"Beta\":\n",
    "        a = np.array(params[\"a\"])\n",
    "        return a.shape[0] if a.ndim > 0 else 1\n",
    "    elif posterior_type == \"MvNormal\" and \"mu\" in params:\n",
    "        return len(params[\"mu\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine dimensionality for posterior type '{posterior_type}' with parameters: {params}\")\n",
    "\n",
    "\n",
    "def plot_and_save_all_metrics(df_results, sampler_colors, varying_attribute, varying_attribute_for_plot, results_folder, plots_folder, run_id, config_descr):\n",
    "    \"\"\"\n",
    "    Generates and saves multiple metric plots for different samplers.\n",
    "\n",
    "    Parameters:\n",
    "    - df_results: DataFrame containing experiment results.\n",
    "    - sampler_colors: Dictionary mapping sampler names to colors.\n",
    "    - varying_attribute: The attribute that varies.\n",
    "    - varying_attribute_for_plot: The attribute used for plotting.\n",
    "    - plots_folder: Folder where plots should be saved.\n",
    "    - run_id: ID of the current run.\n",
    "    - config_descr: Description of the configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define metric labels\n",
    "    metrics = [\"wasserstein_distance\" , \"swd_zscore\",\"r_hat\", \"ess\", \"runtime\"]\n",
    "\n",
    "    # Initialize plots for all metrics\n",
    "    fig_ax_pairs = {key: plt.subplots(figsize=(10, 6)) for key in metrics}\n",
    "\n",
    "    # Iterate over samplers and plot all metrics\n",
    "    for sampler in df_results[\"sampler\"].unique():\n",
    "        df_sampler = df_results[df_results[\"sampler\"] == sampler]\n",
    "        csv_filename = os.path.join(results_folder, f\"{sampler}_results.csv\")\n",
    "        df_sampler.to_csv(csv_filename, index=False)\n",
    "\n",
    "        for metric in metrics:\n",
    "            fig, ax = fig_ax_pairs[metric]\n",
    "            ax.plot(df_sampler[varying_attribute_for_plot], df_sampler[metric], \n",
    "                    marker=\"o\", linestyle=\"-\", label=sampler, \n",
    "                    color=sampler_colors.get(sampler, \"black\"))\n",
    "\n",
    "    # Set dynamic axis labels and save plots\n",
    "    attribute_label = \"Mode Distance\" if varying_attribute == \"mu\" else varying_attribute.replace(\"_\", \" \").title()\n",
    "\n",
    "    for metric in metrics:\n",
    "        fig, ax = fig_ax_pairs[metric]\n",
    "        finalize_and_save_plot(fig,ax, attribute_label, metric, \n",
    "                               f\"{metric} for Samplers (config =_{config_descr})\",\n",
    "                               os.path.join(plots_folder, f\"{metric}_run_{run_id}.pdf\"))\n",
    "\n",
    "\n",
    "def compute_and_save_global_metrics(df_all_runs, sampler_colors, varying_attribute, runs, config_descr, global_results_folder, global_plots_folder, iid_ref_stats_dict):\n",
    "    \"\"\"\n",
    "    Computes and saves global metric plots (averaged across runs) for different samplers.\n",
    "\n",
    "    Parameters:\n",
    "    - df_all_runs: DataFrame containing results from all runs.\n",
    "    - sampler_colors: Dictionary mapping sampler names to colors.\n",
    "    - varying_attribute: The attribute that varies.\n",
    "    - runs: Number of experiment runs.\n",
    "    - config_descr: Configuration description.\n",
    "    - global_results_folder: Folder to save CSVs.\n",
    "    - global_plots_folder: Folder to save plots.\n",
    "    \"\"\"\n",
    "\n",
    "    df_all_runs = df_all_runs.sort_values(varying_attribute, ascending=True)\n",
    "\n",
    "    # Define metrics for aggregation\n",
    "    metrics = [\"wasserstein_distance\", \"mmd_rff\", \"swd_zscore\",\"r_hat\", \"ess\", \"runtime\"]\n",
    "\n",
    "    # New figure set (line + fill)\n",
    "    fig_ax_pairs_shaded = {metric: plt.subplots(figsize=(10, 6)) for metric in metrics}\n",
    "    fig_d, ax_d = plt.subplots(figsize=(10, 6))  # Cohen's d\n",
    "    fig_g, ax_g = plt.subplots(figsize=(10, 6))  # Glass's Δ\n",
    "    fig_g_mmd, ax_g_mmd = plt.subplots(figsize=(10, 6))  # Glass's Δ for MMD\n",
    "\n",
    "    global_avg_dfs = {}\n",
    "\n",
    "    # Load IID reference statistics\n",
    "    iid_means_dict_swd = {}\n",
    "    iid_stds_dict_swd = {}\n",
    "    iid_means_dict_mmd = {}\n",
    "    iid_stds_dict_mmd = {}\n",
    "\n",
    "\n",
    "    for key in df_all_runs[varying_attribute].unique():\n",
    "        k = tuple(key) if isinstance(key, np.ndarray) else key\n",
    "        iid_entry = iid_ref_stats_dict.get(k)\n",
    "        if iid_entry is None:\n",
    "            raise KeyError(f\"Missing IID reference stats for varying attribute value: {k}\")\n",
    "        iid_means_dict_swd[k] = iid_entry[\"mean_swd\"]\n",
    "        iid_stds_dict_swd[k] = iid_entry[\"std_swd\"]\n",
    "        iid_means_dict_mmd[k] = iid_entry[\"mean_mmd\"]\n",
    "        iid_stds_dict_mmd[k] = iid_entry[\"std_mmd\"]\n",
    "\n",
    "\n",
    "    for metric in metrics:\n",
    "        fig_shaded, ax_shaded = fig_ax_pairs_shaded[metric]\n",
    "\n",
    "        # For each sampler, plot its line for this metric\n",
    "        for sampler in df_all_runs[\"sampler\"].unique():\n",
    "            df_sampler = df_all_runs[df_all_runs[\"sampler\"] == sampler]\n",
    "            color = sampler_colors.get(sampler, \"black\")\n",
    "\n",
    "            # Pivot: rows = varying_attribute, columns = run_id, values = metric\n",
    "            df_pivot = df_sampler.pivot_table(\n",
    "                index=varying_attribute, columns=\"run_id\", values=metric\n",
    "            )\n",
    "\n",
    "            # Compute mean and standard deviation across runs\n",
    "            means = df_pivot.mean(axis=1)\n",
    "            stds = df_pivot.std(axis=1)\n",
    "\n",
    "            # Plot mean line\n",
    "            ax_shaded.plot(means.index, means, \"o-\", label=sampler, color=color)\n",
    "\n",
    "            # Plot uncertainty: shaded std\n",
    "            if len(means.index) > 1:\n",
    "                ax_shaded.fill_between(means.index, means - stds, means + stds, color=color, alpha=0.2)\n",
    "            else:\n",
    "                ax_shaded.errorbar(means.index, means, yerr=stds, fmt=\"o\", color=color, capsize=5)\n",
    "\n",
    "            # Save global avg for CSV\n",
    "            if sampler not in global_avg_dfs:\n",
    "                global_avg_dfs[sampler] = {}\n",
    "            global_avg_dfs[sampler][metric] = (means, stds)\n",
    "\n",
    "            # Compute Cohen's d for wasserstein_distance only\n",
    "            if metric == \"wasserstein_distance\":\n",
    "                # Get IID mean and std for this varying attribute value\n",
    "                iid_means_swd = np.array([iid_means_dict_swd[k] for k in means.index])\n",
    "                iid_stds_swd = np.array([iid_stds_dict_swd[k] for k in means.index])\n",
    "            \n",
    "                # Avoid zero in denominator\n",
    "                pooled_std = np.sqrt((stds.values**2 + iid_stds_swd**2) / 2)\n",
    "                pooled_std_safe = np.where(pooled_std == 0, np.nan, pooled_std)\n",
    "                iid_stds_safe = np.where(iid_stds_swd == 0, np.nan, iid_stds_swd)\n",
    "\n",
    "                # Compute Cohen's d\n",
    "                cohens_d = (means.values - iid_means_swd) / pooled_std_safe\n",
    "                glass_delta = (means.values - iid_means_swd) / iid_stds_safe\n",
    "        \n",
    "                #for i in range(len(cohens_d)):\n",
    "                #    print(f\"[{sampler}] index {i}: mean_mcmc = {means.values[i]:.4f}, mean_iid = {iid_means_swd[i]:.4f}, \"f\"std_mcmc = {stds.values[i]:.4f}, std_iid = {iid_stds_swd[i]:.4f}, cohen's d = {cohens_d[i]:.4f}\")\n",
    "\n",
    "                global_avg_dfs[sampler][\"ws_dist_cohens_d\"] = cohens_d\n",
    "                global_avg_dfs[sampler][\"ws_dist_glass_delta\"] = glass_delta\n",
    "\n",
    "                # Plot Cohen's d for this sampler\n",
    "                ax_d.plot(means.index, cohens_d, \"o-\", label=sampler, color=color)\n",
    "                ax_g.plot(means.index, glass_delta, \"o-\", label=sampler, color=color)\n",
    "            \n",
    "            elif metric == \"mmd_rff\":\n",
    "                # Get IID mean and std for this varying attribute value\n",
    "                iid_means_mmd = np.array([iid_means_dict_mmd[k] for k in means.index])\n",
    "                iid_stds_mmd = np.array([iid_stds_dict_mmd[k] for k in means.index])\n",
    "\n",
    "                # Avoid zero in denominator\n",
    "                pooled_std = np.sqrt((stds.values**2 + iid_stds_mmd**2) / 2)\n",
    "                pooled_std_safe = np.where(pooled_std == 0, np.nan, pooled_std)\n",
    "                iid_stds_safe = np.where(iid_stds_mmd == 0, np.nan, iid_stds_mmd)\n",
    "\n",
    "                # Compute Cohen's d\n",
    "                cohens_d_mmd = (means.values - iid_means_mmd) / pooled_std_safe\n",
    "                glass_delta_mmd = (means.values - iid_means_mmd) / iid_stds_safe\n",
    "\n",
    "                global_avg_dfs[sampler][\"mmd_rff_cohens_d\"] = cohens_d_mmd\n",
    "                global_avg_dfs[sampler][\"mmd_rff_glass_delta\"] = glass_delta_mmd\n",
    "\n",
    "                # Plot Cohen's d for this sampler\n",
    "                ax_g_mmd.plot(means.index, glass_delta_mmd, \"o-\", label=sampler, color=color)\n",
    "                #ax_g_mmd.plot(means.index, cohens_d_mmd, \"o-\", label=sampler, color=color)\n",
    "\n",
    "        # Only for wasserstein_distance: Plot IID baseline once\n",
    "        if metric == \"wasserstein_distance\":\n",
    "            \n",
    "            iid_means = np.array([iid_means_dict_swd[k] for k in means.index])\n",
    "            iid_stds = np.array([iid_stds_dict_swd[k] for k in means.index])\n",
    "\n",
    "            ax_shaded.plot(means.index, iid_means, \"o--\", label=\"IID Reference\", color=\"black\")\n",
    "            ax_shaded.fill_between(\n",
    "                means.index,\n",
    "                iid_means - iid_stds,\n",
    "                iid_means + iid_stds,\n",
    "                color=\"black\",\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "        elif metric == \"mmd_rff\":\n",
    "\n",
    "            iid_means = np.array([iid_means_dict_mmd[k] for k in means.index])\n",
    "            iid_stds = np.array([iid_stds_dict_mmd[k] for k in means.index])\n",
    "\n",
    "            ax_shaded.plot(means.index, iid_means, \"o--\", label=\"IID Reference\", color=\"black\")\n",
    "            ax_shaded.fill_between(\n",
    "                means.index,\n",
    "                iid_means - iid_stds,\n",
    "                iid_means + iid_stds,\n",
    "                color=\"black\",\n",
    "                alpha=0.1,\n",
    "            )\n",
    "\n",
    "    # Save Global Averages per Sampler to CSV\n",
    "    for sampler, metrics_dict in global_avg_dfs.items():\n",
    "        df_global_avg = pd.DataFrame({\n",
    "            varying_attribute: metrics_dict[\"wasserstein_distance\"][0].index,\n",
    "            **{f\"global_avg_{metric}\": metrics_dict[metric][0].values for metric in metrics},\n",
    "            **{f\"global_avg_{metric}_std\": metrics_dict[metric][1].values for metric in metrics},\n",
    "        })\n",
    "\n",
    "        if \"ws_dist_cohens_d\" in metrics_dict:\n",
    "            df_global_avg[\"ws_dist_cohens_d\"] = metrics_dict[\"ws_dist_cohens_d\"]\n",
    "        if \"ws_dist_glass_delta\" in metrics_dict:\n",
    "            df_global_avg[\"ws_dist_glass_delta\"] = metrics_dict[\"ws_dist_glass_delta\"]\n",
    "        if \"mmd_rff_cohens_d\" in metrics_dict:\n",
    "            df_global_avg[\"mmd_rff_cohens_d\"] = metrics_dict[\"mmd_rff_cohens_d\"]\n",
    "        if \"mmd_rff_glass_delta\" in metrics_dict:\n",
    "            df_global_avg[\"mmd_rff_glass_delta\"] = metrics_dict[\"mmd_rff_glass_delta\"]\n",
    "\n",
    "        csv_filename = os.path.join(global_results_folder, f\"Global_results_{sampler}.csv\")\n",
    "        df_global_avg.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Save plots\n",
    "    attribute_label = \"Mode Distance\" if varying_attribute == \"mu\" else varying_attribute.replace(\"_\", \" \").title()\n",
    "    for metric in metrics:\n",
    "      \n",
    "        fig_shaded, ax_shaded = fig_ax_pairs_shaded[metric]\n",
    "\n",
    "        finalize_and_save_plot(fig_shaded, ax_shaded, attribute_label, metric,\n",
    "                               f\"Averaged {metric.replace('_', ' ').title()} ({runs} Runs, config = {config_descr})\",\n",
    "                               os.path.join(global_plots_folder, f\"{metric}_global_plot_shaded.pdf\"))\n",
    "        \n",
    "    # Plot Cohen's d for wasserstein_distance\n",
    "    finalize_and_save_plot(fig_d, ax_d, xlabel=attribute_label, ylabel=\"Cohen's d\", title=f\"Cohen's d for Wasserstein Distance ({runs} Runs, config = {config_descr})\",\n",
    "    save_path=os.path.join(global_plots_folder, \"cohens_d_ws_dist.pdf\"))\n",
    "\n",
    "    # Plot Glass's Δ for wasserstein_distance\n",
    "    finalize_and_save_plot(fig_g, ax_g, xlabel=attribute_label, ylabel=\"Glass's Δ\", title=f\"Glass's Δ for Wasserstein Distance ({runs} Runs, config = {config_descr})\",\n",
    "    save_path=os.path.join(global_plots_folder, \"glass_delta_ws_dist.pdf\"))\n",
    "\n",
    "    # Plot Glass's Δ for MMD\n",
    "    finalize_and_save_plot(fig_g_mmd, ax_g_mmd, xlabel=attribute_label, ylabel=\"Glass's Δ\", title=f\"Glass's Δ for MMD-RFF ({runs} Runs, config = {config_descr})\",save_path=os.path.join(global_plots_folder, \"glass_delta_mmd.pdf\"))\n",
    "\n",
    "\n",
    "\n",
    "def finalize_and_save_plot(fig, ax, xlabel, ylabel, title, save_path):\n",
    "    \"\"\"\n",
    "    Finalizes the plot with labels, grid, and saves it to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - fig: Matplotlib figure\n",
    "    - ax: Matplotlib axis\n",
    "    - xlabel: Label for x-axis\n",
    "    - ylabel: Label for y-axis\n",
    "    - title: Title of the plot\n",
    "    - save_path: Path to save the figure.\n",
    "    \"\"\"\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.legend(title=\"Sampler\")\n",
    "    ax.grid(True)\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def plot_histogram(samples, title, save_path=None, posterior_type=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram and KDE of the given samples.\n",
    "\n",
    "    Parameters:\n",
    "    - samples: 1D or 2D array of samples.\n",
    "    - title: Title of the plot.\n",
    "    - save_path: If provided, saves the figure to this path.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    if samples.ndim == 2:\n",
    "        # Handle multivariate case\n",
    "        if samples.shape[1] == 2:\n",
    "            plt.scatter(samples[:, 0], samples[:, 1], alpha=0.3, label=\"2D Samples\")\n",
    "            plt.xlabel(\"Dimension 1\")\n",
    "            plt.ylabel(\"Dimension 2\")\n",
    "            plt.title(title)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "        elif posterior_type == \"MvNormal\" and samples.shape[1] > 2:\n",
    "            logger.info(f\"Skipping plotting: Multivariate Normal with dimension {samples.shape[1]}.\")\n",
    "            return\n",
    "        \n",
    "    else:\n",
    "        # Standard 1D histogram + KDE\n",
    "        plt.hist(samples, bins=50, alpha=0.5, density=True, color='blue', edgecolor='black', label=\"Histogram\")\n",
    "        sns.kdeplot(samples, color='red', lw=2, label=\"KDE\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Sample Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def extract_means_from_posterior(posterior_type, posterior_kwargs):\n",
    "    \"\"\"\n",
    "    Generalized function to extract central tendency (mean/loc) for initialization.\n",
    "    - For Mixture: returns list of all component means.\n",
    "    - For single-posteriors: returns list with one mean value or vector.\n",
    "    \"\"\"\n",
    "    if posterior_type == \"Mixture\":\n",
    "        return extract_means_from_components(posterior_type, posterior_kwargs[\"component_params\"])\n",
    "\n",
    "    elif \"mu\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"mu\"]]\n",
    "\n",
    "    elif \"loc\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"loc\"]]\n",
    "\n",
    "    elif posterior_type == \"Cauchy\" and \"alpha\" in posterior_kwargs:\n",
    "        return [posterior_kwargs[\"alpha\"]] \n",
    "\n",
    "    elif posterior_type == \"Beta\":\n",
    "        a = posterior_kwargs[\"a\"]\n",
    "        b = posterior_kwargs[\"b\"]\n",
    "        return [a / (a + b)]  # Expected value\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot extract central location (mu or loc) for posterior type '{posterior_type}'.\")\n",
    "\n",
    "\n",
    "def extract_means_from_components(posterior_type, component_params):\n",
    "    \"\"\"\n",
    "    Extracts central tendency (mu or loc) from each component's parameters.\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    for params in component_params:\n",
    "        if \"mu\" in params:\n",
    "            means.append(params[\"mu\"])\n",
    "        elif \"loc\" in params:\n",
    "            means.append(params[\"loc\"])\n",
    "\n",
    "        elif posterior_type == \"Cauchy\" and \"alpha\" in params:\n",
    "            means.append(params[\"alpha\"])\n",
    "            \n",
    "        elif posterior_type == \"Beta\":\n",
    "            a = params[\"a\"]\n",
    "            b = params[\"b\"]\n",
    "            means.append([a / (a + b)])  # Expected value\n",
    "        else:\n",
    "            raise ValueError(\"Component missing a central tendency parameter (mu or loc).\")\n",
    "    return means\n",
    "\n",
    "\n",
    "def get_initvals(init_scheme, means, num_chains, rng=None, run_id=None, init_folder=None, value=None):\n",
    "    \"\"\"Generates initialization values based on the chosen scheme.\"\"\" \n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "    if np.isscalar(means[0]):\n",
    "        dim = 1\n",
    "        means_array = np.array(means)[:, None]  # shape (n_modes, 1)\n",
    "    else:\n",
    "        means_array = np.array(means)\n",
    "        dim = means_array.shape[1]\n",
    "\n",
    "\n",
    "    if init_scheme == \"thesis_scheme\":\n",
    "        # If multimodal posterior, use the means of the components, else spawn them randomly around the mean\n",
    "        if len(means_array) >= 2:\n",
    "            # Multimodal case\n",
    "            # Compute bounding box across all dimensions\n",
    "            min_mode = np.min(means_array, axis=0)\n",
    "            max_mode = np.max(means_array, axis=0)\n",
    "            border = 0.25 * (max_mode - min_mode)\n",
    "\n",
    "            low = min_mode - border\n",
    "            high = max_mode + border\n",
    "\n",
    "            initvals = [{\"posterior\": rng.uniform(low, high).item() if dim == 1 else rng.uniform(low, high)} for _ in range(num_chains)]\n",
    "\n",
    "            if run_id == 1:\n",
    "                init_info = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"case\": \"multimodal\",\n",
    "                    \"dim\": dim,\n",
    "                    \"means_array\": means_array.tolist(),\n",
    "                    \"min_mode\": min_mode,\n",
    "                    \"max_mode\": max_mode,\n",
    "                    \"border\": border,\n",
    "                    \"low\": low,\n",
    "                    \"high\": high,\n",
    "                    \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "                }  \n",
    "        else:\n",
    "            # Unimodal case\n",
    "            center = means_array[0]\n",
    "            center = center.item() if dim == 1 else center\n",
    "            noise = 0.5\n",
    "            # samples have 1D shape\n",
    "            if np.isscalar(center):\n",
    "                initvals = [{\"posterior\": center + rng.normal(scale=noise)} for _ in range(num_chains)]\n",
    "            else:\n",
    "                initvals = [{\"posterior\": center + rng.normal(scale=noise, size=center.shape)} for _ in range(num_chains)]\n",
    "\n",
    "            if run_id == 1:          \n",
    "                init_info = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"case\": \"unimodal\",\n",
    "                    \"dim\": dim,\n",
    "                    \"means_array\": means_array.tolist(),\n",
    "                    \"center\": center.tolist() if hasattr(center, \"tolist\") else center,\n",
    "                    \"noise\": noise,\n",
    "                    \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "                }\n",
    "\n",
    "    elif init_scheme == \"equal_per_mode\":\n",
    "        noise = 0.5\n",
    "        initvals =[]\n",
    "        for i in range(num_chains):\n",
    "            mean = means_array[i % len(means_array)]\n",
    "            value = mean + rng.normal(scale=noise)\n",
    "            if dim == 1:\n",
    "                value = value.item()\n",
    "            initvals.append({\"posterior\": value})\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": \"equal_per_mode\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    elif init_scheme == \"all_in_middle\":\n",
    "        middle_point = np.mean(means_array, axis=0)\n",
    "        middle_point = middle_point.item() if dim == 1 else middle_point\n",
    "        noise = 0.5\n",
    "        initvals = [{\"posterior\": middle_point + rng.normal(scale=noise)} for _ in range(num_chains)]\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": \"all_in_middle\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"middle_point\": middle_point.tolist() if hasattr(middle_point, \"tolist\") else middle_point,\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    elif init_scheme.startswith(\"all_near_mode_\"):\n",
    "\n",
    "        mode_index = int(init_scheme.split(\"_\")[-1])\n",
    "        if mode_index >= len(means):\n",
    "            raise IndexError(f\"Mode index {mode_index} out of bounds for available means.\")\n",
    "        \n",
    "        target_mode = means_array[mode_index]\n",
    "        target_mode = target_mode.item() if dim == 1 else target_mode\n",
    "        noise = 0.5\n",
    "        initvals = [{\"posterior\": target_mode + rng.normal(scale=noise)} for _ in range(num_chains)]\n",
    "\n",
    "        if run_id == 1:\n",
    "            init_info = {\n",
    "                \"run_id\": run_id,\n",
    "                \"case\": f\"all_near_mode{mode_index}\",\n",
    "                \"dim\": dim,\n",
    "                \"means_array\": means_array.tolist(),\n",
    "                \"mode_index\": mode_index,\n",
    "                \"samples\": [{k: v.tolist() if isinstance(v, np.ndarray) else v for k, v in d.items()} for d in initvals],\n",
    "            }\n",
    "\n",
    "    if run_id == 1:\n",
    "        parent_folder= os.path.join(init_folder, \"chain initvals\")\n",
    "        create_directories(parent_folder)\n",
    "        chain_info_path = os.path.join(parent_folder, f\"init_{value}.json\")\n",
    "        chain_info_plot_path = os.path.join(parent_folder, f\"init_{value}.pdf\")\n",
    "        save_sample_info(sample_info=init_info, json_path=chain_info_path, plot_path=chain_info_plot_path, label=\"Init Values\")\n",
    "\n",
    "    logger.debug(f\"Generated initvals: {initvals}\")\n",
    "    return initvals\n",
    "\n",
    "\n",
    "def save_sample_info(sample_info, json_path, plot_path, label=\"Samples\", case=None):\n",
    "    \"\"\"\n",
    "    General utility to save sample info (e.g., init values, warmup samples) as JSON and plot if dim ≤ 2.\n",
    "    \n",
    "    Parameters:\n",
    "    - sample_info: dict containing\n",
    "        - \"samples\": list of dicts like [{\"posterior\": ...}, ...]\n",
    "        - \"means_array\": list of means (e.g. from init or components)\n",
    "        - \"dim\": int, dimensionality\n",
    "        - optionally: \"low\", \"high\", \"case\"\n",
    "    - json_path: path to save JSON info\n",
    "    - plot_path: path to save the plot\n",
    "    - label: label for sample points (e.g., \"Init Values\", \"Warmup Samples\")\n",
    "    - case: override case type (for optional bounding box display)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Convert numpy objects to lists for JSON serialization ---\n",
    "    def convert_numpy(obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, np.generic):\n",
    "            return obj.item()\n",
    "        return obj\n",
    "\n",
    "    # --- Save JSON ---\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(sample_info, f, indent=4, default=convert_numpy)\n",
    "\n",
    "    # --- Extract data ---\n",
    "    dim = sample_info[\"dim\"]\n",
    "    means_array = np.array(sample_info.get(\"means_array\", []))\n",
    "\n",
    "    if label == \"Init Values\":\n",
    "        samples = np.array([list(v.values())[0] for v in sample_info[\"samples\"]])\n",
    "    elif label == \"Samples\":\n",
    "        samples = np.array(sample_info[\"samples\"])\n",
    "\n",
    "    # --- Skip plotting for dim > 2 ---\n",
    "    if dim > 2:\n",
    "        return\n",
    "\n",
    "    # --- Start plot ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 2) if dim == 1 else (8, 6))\n",
    "\n",
    "    # 1D case\n",
    "    if dim == 1:\n",
    "        samples_flat = samples.flatten()\n",
    "        ax.scatter(samples_flat, np.zeros_like(samples_flat), color='blue', label=label, alpha=0.7)\n",
    "        means_flat = means_array.flatten()\n",
    "        ax.scatter(means_flat, np.zeros_like(means_flat), color='red', marker='x', s=100, label='Means')\n",
    "\n",
    "        if sample_info.get(\"case\") == \"multimodal\":\n",
    "            # Handle scalar or list storage\n",
    "            low = sample_info[\"low\"]\n",
    "            high = sample_info[\"high\"]\n",
    "\n",
    "            ax.axvline(low, color=\"black\", linestyle=\"--\", label=\"Init Box\")\n",
    "            ax.axvline(high, color=\"black\", linestyle=\"--\")\n",
    "                    \n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel(\"Value\")\n",
    "\n",
    "\n",
    "    # 2D case\n",
    "    elif dim == 2:\n",
    "        ax.scatter(samples[:, 0], samples[:, 1], color='blue', label=label, alpha=0.7)\n",
    "        ax.scatter(means_array[:, 0], means_array[:, 1], color='red', marker='x', s=100, label='Means')\n",
    "\n",
    "        if sample_info.get(\"case\") == \"multimodal\":\n",
    "            low = np.array(sample_info[\"low\"])\n",
    "            high = np.array(sample_info[\"high\"])\n",
    "\n",
    "            rect = plt.Rectangle(low, *(high - low), linewidth=1, edgecolor='black',\n",
    "                                 facecolor='none', linestyle='--', label='Init Box')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        ax.set_xlabel(\"Dim 1\")\n",
    "        ax.set_ylabel(\"Dim 2\")\n",
    "        ax.set_aspect(\"equal\")\n",
    "\n",
    "    # --- Finalize ---\n",
    "    if label == \"Init Values\":\n",
    "        ax.set_title(f\"{label} and Means\")\n",
    "    elif label == \"Samples\":\n",
    "        sampler = sample_info.get(\"sampler\", \"Unknown\")\n",
    "        case = sample_info.get(\"case\", \"Unknown\")\n",
    "        ax.set_title(f\"First {case} from {sampler}\")\n",
    "    ax.grid(True)\n",
    "\n",
    "    if dim == 1:\n",
    "        ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "        fig.subplots_adjust(right=0.75)  \n",
    "    else:\n",
    "        ax.legend()  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def sliced_wasserstein_distance(X, Y, L=100):\n",
    "    \"\"\"\n",
    "    Computes the sliced Wasserstein distance (SWD_p) between two sets of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: numpy array of shape (N, d) -> first sample set\n",
    "    - Y: numpy array of shape (N, d) -> second sample set\n",
    "    - L: int, number of random projections\n",
    "    - p: int, order of Wasserstein distance (default: 1)\n",
    "    \n",
    "    Returns:\n",
    "    - SWD_p: float, the sliced Wasserstein distance\n",
    "    \"\"\"\n",
    "\n",
    "    N, d = X.shape  # Assuming X and Y have the same shape\n",
    "    S = 0  # Accumulation variable\n",
    "\n",
    "    for _ in range(L):\n",
    "        # Sample a random unit vector (projection direction)\n",
    "        theta = np.random.randn(d)\n",
    "        theta /= np.linalg.norm(theta)  # Normalize to unit sphere\n",
    "\n",
    "        # Compute projections\n",
    "        alpha = X @ theta\n",
    "        beta = Y @ theta\n",
    "\n",
    "        # Compute 1D Wasserstein distance\n",
    "        W_i = sp.wasserstein_distance(alpha, beta)\n",
    "\n",
    "        # Accumulate\n",
    "        S += W_i\n",
    "\n",
    "    # Compute final SWD\n",
    "    SWD_p = (S / L) \n",
    "\n",
    "    return SWD_p\n",
    "\n",
    "def compute_mmd_rff(X, Y, D=500, sigma=1.0, seed=None):\n",
    "    \"\"\"\n",
    "    Computes the approximate Maximum Mean Discrepancy (MMD) using Random Fourier Features (RFF)\n",
    "    between two sample sets X and Y.\n",
    "\n",
    "    Parameters:\n",
    "    - X: np.ndarray of shape (n, d) – sample set from distribution p(x)\n",
    "    - Y: np.ndarray of shape (m, d) – sample set from distribution q(x)\n",
    "    - D: int – number of random Fourier features\n",
    "    - sigma: float – bandwidth of the Gaussian kernel\n",
    "    - seed: int or None – random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "    - mmd_rff: float – approximate MMD value\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    n, d = X.shape\n",
    "    m, _ = Y.shape\n",
    "\n",
    "    # Step 1: Generate random frequencies and offsets\n",
    "    omega = rng.normal(loc=0.0, scale=1.0 / sigma, size=(D, d))\n",
    "    b = rng.uniform(0, 2 * np.pi, size=D)\n",
    "\n",
    "    # Step 2: Compute random Fourier features\n",
    "    def z(x):\n",
    "        projection = np.dot(x, omega.T) + b\n",
    "        return np.sqrt(2.0 / D) * np.cos(projection)\n",
    "\n",
    "    Z_X = z(X)  # shape (n, D)\n",
    "    Z_Y = z(Y)  # shape (m, D)\n",
    "\n",
    "    # Step 3: Calculate mean embeddings\n",
    "    mu_p = Z_X.mean(axis=0)\n",
    "    mu_q = Z_Y.mean(axis=0)\n",
    "\n",
    "    # Step 4: Calculate MMD^2 (Euclidean distance between embeddings)\n",
    "    mmd_rff = np.linalg.norm(mu_p - mu_q)\n",
    "\n",
    "    return mmd_rff\n",
    "\n",
    "\n",
    "def generate_iid_samples(posterior_type = None, num_samples=2000, rng=None,**params):\n",
    "    \"\"\"\n",
    "    Generate IID samples from a mixture distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - component_types: List of strings specifying the type of each component (e.g., [\"normal\", \"beta\"]).\n",
    "    - component_params: List of dictionaries with parameters for each component.\n",
    "    - num_samples: Number of samples to generate.\n",
    "    - weights: List of weights for the components.\n",
    "    - rng: Random number generator.\n",
    "\n",
    "    Returns:\n",
    "    - iid_samples: Array of generated IID samples.\n",
    "    \"\"\"\n",
    "\n",
    "    rng = rng or np.random.default_rng()\n",
    "\n",
    "    # Mapping from string names to scipy sampling functions\n",
    "    scipy_distributions = {\n",
    "        \"Normal\": lambda p: sp.norm.rvs(loc=p[\"mu\"], scale=p[\"sigma\"], size=num_samples, random_state=rng),\n",
    "        \"StudentT\": lambda p: sp.t.rvs(df=p[\"nu\"], loc=p[\"mu\"], scale=p[\"sigma\"], size=num_samples, random_state=rng),\n",
    "        \"Beta\": lambda p: sp.beta.rvs(a=p[\"a\"], b=p[\"b\"], size=num_samples, random_state=rng),\n",
    "        \"Cauchy\": lambda p: sp.cauchy.rvs(loc=p[\"alpha\"], scale=p[\"beta\"], size=num_samples, random_state=rng),\n",
    "        \"Laplace\": lambda p: sp.laplace.rvs(loc=p[\"mu\"], scale=p[\"b\"], size=num_samples, random_state=rng),\n",
    "        \"MvNormal\": lambda p: rng.multivariate_normal(mean=np.array(p[\"mu\"]), cov=np.array(p[\"cov\"]), size=num_samples),\n",
    "    }\n",
    "\n",
    "    # Handle Skewed Student-T (which needs PyMC)\n",
    "    if posterior_type == \"SkewStudentT\":\n",
    "        with pm.Model():\n",
    "            skewed_t = pm.SkewStudentT.dist(a=params[\"a\"], b=params[\"b\"], mu=params[\"mu\"], sigma=params[\"sigma\"])\n",
    "            return pm.draw(skewed_t, draws=num_samples, random_seed=rng)\n",
    "\n",
    "    # Handle single distributions\n",
    "    if posterior_type in scipy_distributions:\n",
    "        logger.debug(f\"Generating {posterior_type} samples with parameters: {params}\")\n",
    "        return scipy_distributions[posterior_type](params)\n",
    "\n",
    "    elif posterior_type == \"Mixture\":\n",
    "        component_types = params[\"component_types\"]\n",
    "        component_params = params[\"component_params\"]\n",
    "        weights = params[\"weights\"]\n",
    "\n",
    "        if len(component_types) != len(component_params):\n",
    "            raise ValueError(\"Each component type must have a corresponding parameter dictionary.\")\n",
    "\n",
    "        # normalize weights\n",
    "        weights = np.array(weights) / np.sum(weights)\n",
    "\n",
    "        # Choose which component each sample belongs to based on weights\n",
    "        chosen_components = rng.choice(len(component_types), size=num_samples, p=weights)\n",
    "\n",
    "        posterior_dim = None  \n",
    "\n",
    "        posterior_dim = get_posterior_dim(\"Mixture\", {\n",
    "            \"component_types\": component_types,\n",
    "            \"component_params\": component_params,\n",
    "            \"weights\": weights\n",
    "        })\n",
    "\n",
    "\n",
    "        if posterior_dim > 1:\n",
    "            iid_samples = np.empty((num_samples, posterior_dim))  # Multivariate case\n",
    "        else:\n",
    "            iid_samples = np.empty(num_samples)\n",
    "\n",
    "        for i, (comp_type, comp_params) in enumerate(zip(component_types, component_params)):\n",
    "            mask = chosen_components == i  # Select samples for this component\n",
    "            num_selected = mask.sum()\n",
    "            if num_selected > 0:\n",
    "                if comp_type in scipy_distributions or comp_type == \"SkewStudentT\":\n",
    "                    iid_samples[mask] = generate_iid_samples(posterior_type=comp_type, num_samples=num_selected, rng=rng, **comp_params)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported component type in IID sampling: {comp_type}\")\n",
    "                \n",
    "        return iid_samples\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported posterior type: {posterior_type}\")\n",
    "\n",
    "\n",
    "def generate_all_iid_batches(\n",
    "    posterior_type,\n",
    "    posterior_kwargs,\n",
    "    iid_kwargs,\n",
    "    varying_attribute,\n",
    "    varying_values,\n",
    "    num_total_iid_batches,\n",
    "    num_iid_vs_iid_batches,\n",
    "    num_samples,\n",
    "    rng=None,\n",
    "    config_folder=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates all IID batches for the given posterior type and varying attribute.\n",
    "\n",
    "    Parameters:\n",
    "    - posterior_type: Type of the posterior (e.g., \"Mixture\", \"Normal\").\n",
    "    - posterior_kwargs: Dictionary of parameters for the posterior.\n",
    "    - varying_attribute: The attribute that varies (e.g., \"mu\", \"sigma\").\n",
    "    - varying_values: List of values for the varying attribute.\n",
    "    - num_total_iid_batches: Total number of IID batches to generate.\n",
    "    - num_iid_vs_iid_batches: Number of IID vs IID batches.\n",
    "    - num_samples: Number of samples per batch.\n",
    "    \n",
    "    Returns:\n",
    "    - iid_batches_dict: Dictionary of generated IID batches.\n",
    "    - iid_ref_stats_dict: Dictionary of reference statistics for SWD and MMD.\n",
    "    \"\"\"\n",
    "    \n",
    "    iid_histogram_folder = os.path.join(config_folder, \"KDE and Histograms of IID Samples\")\n",
    "    create_directories(iid_histogram_folder)\n",
    "\n",
    "    # === Handle Precomputed IID Samples for Varying Attributes ===\n",
    "    # Dictionary to store generated IID batches\n",
    "    iid_batches_dict = {}\n",
    "    # Dictionary to store reference SWD statistics\n",
    "    iid_ref_stats_dict = {}\n",
    "\n",
    "    if posterior_type == \"Mixture\":\n",
    "        component_index = posterior_kwargs.get(\"varying_component\")  # Get the selected component\n",
    "\n",
    "        # Loop through all varying values for Mixture posterior\n",
    "        for value in varying_values:\n",
    "\n",
    "            iid_kwargs[\"component_params\"][component_index][varying_attribute] = value\n",
    "            logger.debug(f\"Updating component {component_index} with {varying_attribute} = {value}\")\n",
    "\n",
    "            iid_batches = [generate_iid_samples(\n",
    "                posterior_type=posterior_type,\n",
    "                component_types=iid_kwargs[\"component_types\"],\n",
    "                component_params=iid_kwargs[\"component_params\"], \n",
    "                weights=iid_kwargs[\"weights\"],\n",
    "                num_samples= num_samples,\n",
    "                rng=rng\n",
    "            ) for _ in range(num_total_iid_batches)]\n",
    "\n",
    "            iid_batches_dict[value] = iid_batches\n",
    "\n",
    "            ref_swd_values = []\n",
    "            ref_mmd_values = []\n",
    "\n",
    "            for i in range(0, num_iid_vs_iid_batches, 2):  # 0–1, 2–3, 4–5, ...\n",
    "                x = ensure_2d(iid_batches[i])\n",
    "                y = ensure_2d(iid_batches[i+1])\n",
    "                swd = sliced_wasserstein_distance(x, y)\n",
    "                mmd_rff = compute_mmd_rff(x, y, D=500, sigma=1.0, seed=None)\n",
    "                ref_mmd_values.append(mmd_rff)\n",
    "                ref_swd_values.append(swd)\n",
    "\n",
    "            mean_ref_swd = np.mean(ref_swd_values)\n",
    "            std_ref_swd = np.std(ref_swd_values, ddof=1)\n",
    "\n",
    "            mean_ref_mmd = np.mean(ref_mmd_values)\n",
    "            std_ref_mmd = np.std(ref_mmd_values, ddof=1)\n",
    "\n",
    "            # Speichern\n",
    "            iid_ref_stats_dict[value] = {\"mean_swd\": mean_ref_swd, \"std_swd\": std_ref_swd, \"mean_mmd\": mean_ref_mmd, \"std_mmd\": std_ref_mmd}\n",
    "\n",
    "            # Plot histogram and KDE for each varying value\n",
    "            plot_histogram(\n",
    "                samples=iid_batches_dict[value][0],\n",
    "                title=f\"IID Samples Histogram & KDE ({varying_attribute}={value})\",\n",
    "                save_path=os.path.join(iid_histogram_folder, f\"iid_hist_kde_{varying_attribute}_{value}.pdf\"),\n",
    "                posterior_type=posterior_type\n",
    "            )\n",
    "        \n",
    "    # Single posterior case\n",
    "    elif varying_attribute in iid_kwargs or varying_attribute == \"num_samples\":\n",
    "        for value in varying_values:\n",
    "            if varying_attribute == \"num_samples\":\n",
    "                current_num_samples = value  \n",
    "            else:\n",
    "                iid_kwargs[varying_attribute] = value  \n",
    "                current_num_samples = num_samples      \n",
    "            \n",
    "            iid_batches = [generate_iid_samples(    \n",
    "                posterior_type=posterior_type,\n",
    "                **iid_kwargs,\n",
    "                num_samples= num_samples,\n",
    "                rng=rng) for _ in range(num_total_iid_batches)]\n",
    "\n",
    "            iid_batches_dict[value] = iid_batches\n",
    "\n",
    "            ref_swd_values = []\n",
    "            ref_mmd_values = []\n",
    "            for i in range(0, num_iid_vs_iid_batches, 2):  # 0–1, 2–3, 4–5, ...\n",
    "                x = ensure_2d(iid_batches[i])\n",
    "                y = ensure_2d(iid_batches[i+1])\n",
    "                swd = sliced_wasserstein_distance(x, y)\n",
    "                mmd_rff = compute_mmd_rff(x, y, D=500, sigma=1.0, seed=None)\n",
    "                ref_swd_values.append(swd)\n",
    "                ref_mmd_values.append(mmd_rff)\n",
    "\n",
    "            mean_ref_swd = np.mean(ref_swd_values)\n",
    "            std_ref_swd = np.std(ref_swd_values, ddof=1)\n",
    "\n",
    "            mean_ref_mmd = np.mean(ref_mmd_values)\n",
    "            std_ref_mmd = np.std(ref_mmd_values, ddof=1)\n",
    "\n",
    "            # Speichern\n",
    "            iid_ref_stats_dict[value] = {\"mean_swd\": mean_ref_swd, \"std_swd\": std_ref_swd, \"mean_mmd\": mean_ref_mmd, \"std_mmd\": std_ref_mmd}\n",
    "\n",
    "            # Plot histogram and KDE for each varying value\n",
    "            plot_histogram(\n",
    "                samples=iid_batches_dict[value][0],\n",
    "                title=f\"IID Samples Histogram & KDE ({varying_attribute}={value})\",\n",
    "                save_path=os.path.join(iid_histogram_folder, f\"iid_hist_kde_{varying_attribute}_{value}.pdf\"),\n",
    "                posterior_type=posterior_type\n",
    "            )\n",
    "\n",
    "    # Fixed posterior case (no varying attribute in posterior_kwargs)\n",
    "    else:\n",
    "        iid_batches = [generate_iid_samples(\n",
    "            posterior_type=posterior_type,\n",
    "            **iid_kwargs,\n",
    "            num_samples=num_samples,\n",
    "            rng=rng\n",
    "        ) for _ in range(num_total_iid_batches)]\n",
    "\n",
    "\n",
    "        ref_swd_values = []\n",
    "        ref_mmd_values = []\n",
    "        for i in range(0, num_iid_vs_iid_batches, 2):  # 0–1, 2–3, ...\n",
    "            swd = sliced_wasserstein_distance(iid_batches[i], iid_batches[i+1])\n",
    "            mmd_rff = compute_mmd_rff(iid_batches[i], iid_batches[i+1], D=500, sigma=1.0, seed=None)\n",
    "            ref_swd_values.append(swd)\n",
    "            ref_mmd_values.append(mmd_rff)\n",
    "\n",
    "        mean_ref_swd = np.mean(ref_swd_values)\n",
    "        std_ref_swd = np.std(ref_swd_values, ddof=1)\n",
    "\n",
    "        mean_ref_mmd = np.mean(ref_mmd_values)\n",
    "        std_ref_mmd = np.std(ref_mmd_values, ddof=1)\n",
    "\n",
    "        iid_ref_stats_dict[\"fixed\"] = {\"mean\": mean_ref_swd, \"std\": std_ref_swd, \"mean_mmd\": mean_ref_mmd, \"std_mmd\": std_ref_mmd}\n",
    "\n",
    "        plot_histogram(\n",
    "            samples=iid_batches[0],\n",
    "            title=\"IID Samples Histogram & KDE (fixed posterior)\",\n",
    "            save_path=os.path.join(iid_histogram_folder, \"iid_hist_kde.pdf\"),\n",
    "            posterior_type=posterior_type\n",
    "        )\n",
    "    \n",
    "    return iid_batches_dict, iid_ref_stats_dict\n",
    "\n",
    "\n",
    "class PosteriorExample:\n",
    "    \"\"\"Base class for different posterior types.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None  # Placeholder for the PyMC model\n",
    "    \n",
    "    def _define_posterior(self):\n",
    "        \"\"\"Subclasses should implement this method to define the posterior.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement _define_posterior()\")\n",
    "\n",
    "    def run_sampling(self, sampler_name, num_samples=2000, tune=1000, num_chains=2, initvals=None,run_id=None, plot_first_sample=None, init_folder=None, value=None, means=None, run_random_seed=None):\n",
    "        \"\"\"Runs MCMC sampling using the chosen sampler.\"\"\"\n",
    "\n",
    "        with self.model:\n",
    "\n",
    "            if sampler_name == \"SMC\":\n",
    "                trace = pm.sample_smc(num_samples, chains=num_chains, progressbar=False, random_seed=run_random_seed)\n",
    "            else:\n",
    "                \n",
    "                # Define which sampler to use\n",
    "                if sampler_name == \"Metro\":\n",
    "                    sampler = pm.Metropolis()\n",
    "                elif sampler_name == \"HMC\":\n",
    "                    sampler = pm.NUTS()\n",
    "                elif sampler_name == \"DEMetro\":\n",
    "                    sampler = pm.DEMetropolis()\n",
    "                elif sampler_name == \"DEMetro_Z\":\n",
    "                    sampler = pm.DEMetropolisZ()\n",
    "                elif sampler_name == \"Slice\":\n",
    "                    sampler = pm.Slice()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown sampler: {sampler_name}\")\n",
    "\n",
    "                if run_id == 1:\n",
    "                    discard_tuned_samples = False\n",
    "                else:\n",
    "                    discard_tuned_samples = True\n",
    "\n",
    "                if initvals != None:\n",
    "                    trace = pm.sample(num_samples, tune=tune, step=sampler,initvals=initvals, chains=num_chains, return_inferencedata=True, discard_tuned_samples=discard_tuned_samples, progressbar=False, random_seed=run_random_seed)   \n",
    "                else:\n",
    "                    trace = pm.sample(num_samples, tune=tune, step=sampler, chains=num_chains, return_inferencedata=True, discard_tuned_samples=discard_tuned_samples, progressbar=False, random_seed=run_random_seed)\n",
    "\n",
    "                if run_id == 1 and plot_first_sample:\n",
    "                    first_warmup_samples = trace.warmup_posterior[\"posterior\"].isel(draw=0).values\n",
    "                    dim = first_warmup_samples.shape[1] if first_warmup_samples.ndim > 1 else 1\n",
    "\n",
    "                    warmup_info = {\n",
    "                        \"sampler\": sampler_name,\n",
    "                        \"value\": value,\n",
    "                        \"means_array\": means,\n",
    "                        \"case\": \"Warmup Samples\",\n",
    "                        \"dim\": dim,\n",
    "                        \"samples\": first_warmup_samples.tolist(),\n",
    "                    }\n",
    "\n",
    "                    # Define file paths\n",
    "                    parent_folder = os.path.join(init_folder, f\"{sampler_name}\")\n",
    "                    create_directories(parent_folder)\n",
    "                    warmup_base = os.path.join(parent_folder, \"first warum up samples\")\n",
    "                    warmup_json_path = f\"{warmup_base}.json\"\n",
    "                    warmup_plot_path = f\"{warmup_base}.pdf\"\n",
    "\n",
    "                    save_sample_info(sample_info=warmup_info, json_path=warmup_json_path, plot_path=warmup_plot_path, label=\"Samples\")\n",
    "\n",
    "                    # also plot first posterior sample\n",
    "                    first_posterior_samples = trace.posterior[\"posterior\"].isel(draw=0).values\n",
    "                    posterior_info = {\n",
    "                        \"sampler\": sampler_name,\n",
    "                        \"value\": value,\n",
    "                        \"means_array\": means,\n",
    "                        \"case\": \"Posterior Samples\",\n",
    "                        \"dim\": dim,\n",
    "                        \"samples\": first_posterior_samples.tolist(),\n",
    "                    }\n",
    "                    # Define file paths\n",
    "              \n",
    "                    posterior_base = os.path.join(parent_folder, \"first posterior samples\")\n",
    "                    posterior_json_path = f\"{posterior_base}.json\"\n",
    "                    posterior_plot_path = f\"{posterior_base}.pdf\"\n",
    "                    save_sample_info(sample_info=posterior_info, json_path=posterior_json_path, plot_path=posterior_plot_path, label=\"Samples\")\n",
    "                    \n",
    "        return trace\n",
    "\n",
    "\n",
    "class SinglePosterior(PosteriorExample):\n",
    "    def __init__(self, dist_name, dist_params):\n",
    "        \"\"\"\n",
    "        A flexible class for defining unimodal posteriors.\n",
    "\n",
    "        Parameters:\n",
    "        - dist_name: String specifying the name of the PyMC distribution (e.g., \"Normal\", \"StudentT\").\n",
    "        - dist_params: Dictionary containing the parameters for the distribution.\n",
    "        \"\"\"\n",
    "        self.dist_name = dist_name\n",
    "        self.dist_params = dist_params\n",
    "        super().__init__()\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "    def _define_posterior(self):\n",
    "        \n",
    "        dist_class = getattr(pm, self.dist_name)   # Retrieve the distribution class from PyMC\n",
    "        \n",
    "        with pm.Model() as model:\n",
    "            dist_class(\"posterior\", **self.dist_params)\n",
    "        return model\n",
    "\n",
    "\n",
    "class MixturePosterior(PosteriorExample):\n",
    "    \n",
    "    def __init__(self, component_types, component_params, weights=None, varying_component=None): \n",
    "        \"\"\"\n",
    "        A flexible mixture posterior allowing any number of components and arbitrary distributions.\n",
    "\n",
    "        Parameters:\n",
    "        - component_types: List of strings specifying the type of each component (e.g., [\"normal\", \"beta\"]).\n",
    "        - component_params: List of dictionaries, where each dictionary contains the parameters for the corresponding distribution.\n",
    "        - weights: List of weights for the mixture components (defaults to uniform).\n",
    "        \"\"\"\n",
    "        if len(component_types) != len(component_params):\n",
    "            raise ValueError(\"Each component type must have a corresponding parameter dictionary.\")\n",
    "\n",
    "        if weights is None:\n",
    "            weights = np.ones(len(component_types))  # Default: Equal weights\n",
    "\n",
    "        if len(weights) != len(component_types):\n",
    "            raise ValueError(\"Number of weights must match number of components.\")\n",
    "\n",
    "        self.component_types = component_types\n",
    "        self.component_params = component_params\n",
    "        self.weights = weights\n",
    "\n",
    "        # Normalize weights\n",
    "        self.weights = np.array(self.weights) / np.sum(self.weights)\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "\n",
    "    def _define_posterior(self):\n",
    "        \n",
    "        # Construct component distributions dynamically\n",
    "        components = []\n",
    "        for dist_type, params in zip(self.component_types, self.component_params):\n",
    "            try:\n",
    "                dist_class = getattr(pm, dist_type)  # Retrieve PyMC distribution dynamically\n",
    "                components.append(dist_class.dist(**params))  # Use `.dist()` to create distribution\n",
    "            except AttributeError:\n",
    "                raise ValueError(f\"Unsupported distribution type: {dist_type}\")\n",
    "            \n",
    "        # Define the mixture model    \n",
    "        with pm.Model() as model:\n",
    "            # Mixture model\n",
    "            pm.Mixture(\"posterior\", w=self.weights, comp_dists=components)\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "class CustomPosterior(PosteriorExample):\n",
    "    \"\"\"\n",
    "    A flexible class to define custom posteriors using a user-specified log-probability function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logp_func):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - logp_func: Callable function that defines the log-probability.\n",
    "                     Must accept PyMC symbolic variables.\n",
    "        - param_names: List of parameter names required by logp_func.\n",
    "        - initvals: Optional dictionary for initial values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logp_func = logp_func\n",
    "        self.model = self._define_posterior()\n",
    "\n",
    "    def _define_posterior(self):\n",
    "        with pm.Model() as model:\n",
    "\n",
    "            # Define the custom distribution using pm.CustomDist\n",
    "            pm.CustomDist(\"posterior\", logp=self.logp_func)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "def run_experiment(\n",
    "    experiment_settings,\n",
    "    posterior_type,\n",
    "    config_descr,\n",
    "    runs,\n",
    "    varying_attribute, \n",
    "    varying_values,      \n",
    "    num_samples,\n",
    "    num_chains,\n",
    "    init_scheme=None,\n",
    "    base_random_seed=None,\n",
    "    progress_bar=None,\n",
    "    **posterior_kwargs\n",
    "):\n",
    "    \n",
    "    set_logging_level(experiment_settings.get(\"logging_level\", \"INFO\"))\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    logger.info(f\"===== Config {config_descr} started! =====\")\n",
    "\n",
    "    # Initialize random number generator\n",
    "    rng = np.random.default_rng(base_random_seed)\n",
    "\n",
    "    # Samples\n",
    "    samples_per_chain = \"varies\" if varying_attribute in [\"num_samples\", \"num_chains\"] else num_samples // num_chains\n",
    "    # Adjust total to match per-chain sample count\n",
    "    num_samples = samples_per_chain*num_chains\n",
    "\n",
    "    # Number of IID bacthes for the IID vs IID comparison\n",
    "    num_iid_vs_iid_batches = 2*runs\n",
    "    num_mcmc_batches = runs\n",
    "    # Total number of iid batches (needs a fresh iid batch for each mcmc run)\n",
    "    num_total_iid_batches = num_iid_vs_iid_batches + num_mcmc_batches\n",
    "\n",
    "    # Define required parameters for each posterior type\n",
    "    required_parameters = {\n",
    "        \"Mixture\": [\"component_types\", \"component_params\", \"weights\"],\n",
    "        \"Cauchy\": [\"alpha\", \"beta\"],\n",
    "        \"Beta\": [\"a\", \"b\"],\n",
    "        \"Normal\": [\"mu\", \"sigma\"],\n",
    "        \"StudentT\": [\"nu\", \"mu\", \"sigma\"],\n",
    "        \"SkewStudentT\": [\"a\", \"b\", \"mu\", \"sigma\"],\n",
    "        \"Laplace\": [\"mu\", \"b\"],\n",
    "        \"MvNormal\": [\"mu\", \"cov\"],\n",
    "        \"Custom\": []\n",
    "    }\n",
    "\n",
    "    # Validate that required keys exist (except for varying attributes)\n",
    "    required_keys = [k for k in required_parameters.get(posterior_type) if k != varying_attribute]\n",
    "    if not all(k in posterior_kwargs for k in required_keys):\n",
    "        raise ValueError(f\"{posterior_type} posterior requires {required_keys}\")\n",
    "\n",
    "    # Create keyword arguments for IID sample generation\n",
    "    iid_kwargs = {key: posterior_kwargs.get(key, \"varies\") for key in required_parameters.get(posterior_type)}\n",
    "\n",
    "    logger.debug(f\"Using IID sample settings: {iid_kwargs}\")\n",
    "\n",
    "    # Create configuration and histogram folders inside the experiment root\n",
    "    config_folder = os.path.join(experiment_root_folder, f\"{config_descr}_with_{runs}_runs\")\n",
    "    init_folder = os.path.join(config_folder, f\"init_info\")\n",
    "    create_directories(config_folder, init_folder)\n",
    "\n",
    "\n",
    "    if posterior_type != \"Custom\":\n",
    "        iid_batches_dict, iid_ref_stats_dict = generate_all_iid_batches(\n",
    "            posterior_type=posterior_type,\n",
    "            posterior_kwargs=posterior_kwargs,\n",
    "            iid_kwargs=iid_kwargs,\n",
    "            varying_attribute=varying_attribute,\n",
    "            varying_values=varying_values,\n",
    "            num_total_iid_batches=num_total_iid_batches,\n",
    "            num_iid_vs_iid_batches=num_iid_vs_iid_batches,\n",
    "            num_samples=num_samples,\n",
    "            rng=rng,\n",
    "            config_folder=config_folder\n",
    "        )       \n",
    "\n",
    "    experiment_metadata = {\n",
    "        \"config_descr\": config_descr,\n",
    "        \"runs\": runs,\n",
    "        \"posterior_type\": posterior_type,\n",
    "        \"varying_attribute\": varying_attribute,\n",
    "        \"varying_values\": varying_values,\n",
    "        \"init_scheme\": init_scheme,\n",
    "        \"base_random_seed\": base_random_seed,\n",
    "        \"git_tag\": get_git_tag(),\n",
    "    }\n",
    "    experiment_metadata.update(iid_kwargs)  # Add posterior-specific parameters\n",
    "\n",
    "    # Save metadata\n",
    "    metadata_filename = os.path.join(config_folder, f\"metadata_config_{config_descr}.json\")\n",
    "    with open(metadata_filename, \"w\") as f:\n",
    "        json.dump(experiment_metadata, f, indent=4)\n",
    "\n",
    "    # Define fixed colors for each sampler\n",
    "    sampler_colors = {\n",
    "        \"Metro\": \"blue\",\n",
    "        \"HMC\": \"red\",\n",
    "        \"DEMetro\": \"green\",\n",
    "        \"Slice\": \"orange\",\n",
    "    }\n",
    "\n",
    "    plot_first_sample = experiment_settings.get(\"plot_first_sample\", False)\n",
    "\n",
    "    df_all_runs = []\n",
    "\n",
    "    # === Run the Experiment ===\n",
    "    for run_id in range(1, runs + 1):\n",
    "        logger.info(f\"Running {config_descr} - Run {run_id}\")\n",
    "\n",
    "        run_random_seed = int(rng.integers(1_000_000))\n",
    "\n",
    "        run_folder = os.path.join(config_folder, f\"run_{run_id}\")\n",
    "        results_folder = os.path.join(run_folder, \"results\")\n",
    "        traces_folder = os.path.join(run_folder, \"traces_and_trace_plots\")\n",
    "        plots_folder = os.path.join(run_folder, \"plots_of_run\")\n",
    "        \n",
    "        create_directories(run_folder, results_folder, traces_folder, plots_folder)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for value in varying_values:\n",
    "\n",
    "            var_attr_folder = os.path.join(traces_folder, f\"{varying_attribute}_{value}\")\n",
    "            create_directories(var_attr_folder)\n",
    "\n",
    "            if run_id == 1:\n",
    "                # create subfolder for value in init folder\n",
    "                init_value_folder = os.path.join(init_folder, f\"{varying_attribute}_{value}\")\n",
    "                create_directories(init_value_folder)\n",
    "\n",
    "            # Handle parameter changes for Mixture case\n",
    "            if posterior_type == \"Mixture\":\n",
    "                component_index = posterior_kwargs.get(\"varying_component\")\n",
    "                if component_index is None and varying_attribute not in [\"num_samples\", \"num_chains\", \"init_scheme\"]:\n",
    "                    raise ValueError(f\"`varying_component` must be specified when varying '{varying_attribute}' in a Mixture.\")\n",
    "\n",
    "                # Modify only the selected component\n",
    "                posterior_kwargs[\"component_params\"][component_index][varying_attribute] = value\n",
    "            \n",
    "            else:\n",
    "                if varying_attribute in iid_kwargs:\n",
    "                    posterior_kwargs[varying_attribute] = value\n",
    "            \n",
    "            if varying_attribute == \"num_samples\":\n",
    "                num_samples = value\n",
    "                samples_per_chain = num_samples // num_chains\n",
    "            elif varying_attribute == \"num_chains\":\n",
    "                num_chains = value\n",
    "                samples_per_chain = num_samples // num_chains\n",
    "            elif varying_attribute == \"init_scheme\":\n",
    "                init_scheme = value\n",
    "\n",
    "            if posterior_type == \"Mixture\":\n",
    "                model = MixturePosterior(**posterior_kwargs)\n",
    "            elif posterior_type == \"Custom\":\n",
    "                logp_func = posterior_kwargs[\"logp_func\"]\n",
    "                model = CustomPosterior(logp_func=logp_func)\n",
    "            else:\n",
    "                model = SinglePosterior(dist_name=posterior_type, dist_params=posterior_kwargs)\n",
    "\n",
    "            means = None\n",
    "            initvals = None\n",
    "            \n",
    "            if init_scheme is not None:\n",
    "                    means = extract_means_from_posterior(posterior_type, posterior_kwargs)\n",
    "                    initvals = get_initvals(init_scheme, means, num_chains, rng, run_id, init_value_folder, value)\n",
    "        \n",
    "            # Get IID samples for the current varying value\n",
    "            if posterior_type != \"Custom\" and varying_attribute not in [\"init_scheme\", \"num_chains\"]:\n",
    "                iid_batches = iid_batches_dict[value]\n",
    "            elif posterior_type == \"Custom\":\n",
    "                iid_batches = None\n",
    "\n",
    "            # Run sampling for all samplers\n",
    "            for sampler_name in experiment_settings[\"samplers\"]:\n",
    "                \n",
    "                if posterior_type == \"Mixture\":\n",
    "                    logger.info(f\"Running {sampler_name} with {varying_attribute} = {value} (Component {component_index})\")\n",
    "                else:\n",
    "                    logger.info(f\"Running {sampler_name} with {varying_attribute} = {value}\")\n",
    "\n",
    "                # **Measure Computation Time**\n",
    "                start_time = time.time()\n",
    "                trace = model.run_sampling(\n",
    "                    sampler_name, num_samples=samples_per_chain, num_chains=num_chains,\n",
    "                    initvals = initvals, run_id=run_id, plot_first_sample=plot_first_sample, init_folder= init_value_folder, value=value, means=means, run_random_seed=run_random_seed)\n",
    "                end_time = time.time()\n",
    "                runtime = end_time - start_time\n",
    "                \n",
    "                # Plot trace plots in notebook if requested\n",
    "                if experiment_settings.get(\"plot_traces_in_notebook\", False):\n",
    "                    az.plot_trace(trace, compact=True)\n",
    "                    plt.title(f\"Trace Plot ({sampler_name}, {varying_attribute} = {value})\")\n",
    "                    plt.show()\n",
    "                \n",
    "                # Save trace to NetCDF file if requested\n",
    "                if experiment_settings.get(\"save_traces\", False):\n",
    "                    trace_filename = os.path.join(var_attr_folder, f\"{sampler_name}_trace.nc\")\n",
    "                    az.to_netcdf(trace, trace_filename)\n",
    "\n",
    "                trace_plot_mode = experiment_settings.get(\"trace_plots\", \"none\")\n",
    "\n",
    "                # Save trace plots to PDF if requested\n",
    "                if trace_plot_mode == \"all\" or (trace_plot_mode == \"first_run_only\" and run_id == 1):\n",
    "                    trace_plot_filename = os.path.join(var_attr_folder, f\"{sampler_name}_trace_plot.pdf\")\n",
    "                    az.plot_trace(trace, compact=True)\n",
    "                    plt.savefig(trace_plot_filename, bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "\n",
    "                posterior_samples = trace.posterior[\"posterior\"].values\n",
    "\n",
    "                # Ensure posterior_samples always has shape (N, dims)\n",
    "                if posterior_samples.ndim == 2:\n",
    "                    posterior_samples = posterior_samples.reshape(-1, 1) \n",
    "                else:\n",
    "                    posterior_samples = posterior_samples.reshape(-1, posterior_samples.shape[-1])\n",
    "\n",
    "                    \n",
    "                # Only compute Wasserstein distance if we have iid_samples\n",
    "                if posterior_type != \"Custom\":\n",
    "                    #ws_distance = sliced_wasserstein_distance(posterior_samples, iid_samples, L=5)\n",
    "\n",
    "                    # 2*runs have already been used for iid vs iid comparison\n",
    "                    fresh_iid_index = num_iid_vs_iid_batches + run_id-1\n",
    "                    iid_batch = ensure_2d(iid_batches[fresh_iid_index]) \n",
    "                    mcmc_vs_iid_swd = sliced_wasserstein_distance(posterior_samples, iid_batch, L=5)\n",
    "\n",
    "                    mmd_rff_value = compute_mmd_rff(posterior_samples, iid_batch, D=500, sigma=1.0, seed=run_random_seed)\n",
    "                    \n",
    "                    # get reference values\n",
    "                    ref_stats = iid_ref_stats_dict[value]\n",
    "                    iid_vs_iid_mean = ref_stats[\"mean_swd\"]\n",
    "                    iid_vs_iid_std = ref_stats[\"std_swd\"]\n",
    "\n",
    "                    relative_swd = mcmc_vs_iid_swd - iid_vs_iid_mean\n",
    "                    if iid_vs_iid_std > 0:\n",
    "                        # Compute z-score\n",
    "                        swd_zscore = relative_swd / iid_vs_iid_std\n",
    "                    else: \n",
    "                        swd_zscore = np.nan\n",
    "              \n",
    "                else:\n",
    "                    mcmc_vs_iid_swd = np.nan\n",
    "                    relative_swd = np.nan\n",
    "                    swd_zscore = np.nan\n",
    "\n",
    "                # Compute R-hat and ESS\n",
    "                r_hat = az.rhat(trace)[\"posterior\"].max().item()\n",
    "                ess = az.ess(trace)[\"posterior\"].min().item()\n",
    "\n",
    "                print(f\"R-hat for sampler {sampler_name}: {r_hat}\")\n",
    "\n",
    "                #print(f\"ESS for sampler {sampler_name}: {ess}\")\n",
    "\n",
    "                results.append({\n",
    "                    \"run_id\": run_id,\n",
    "                    varying_attribute: value,\n",
    "                    \"sampler\": sampler_name,\n",
    "                    \"wasserstein_distance\": mcmc_vs_iid_swd,\n",
    "                    \"mmd_rff\": mmd_rff_value,\n",
    "                    \"swd_zscore\": swd_zscore,\n",
    "                    \"r_hat\": r_hat,\n",
    "                    \"ess\": ess,\n",
    "                    \"runtime\": runtime\n",
    "                })\n",
    "\n",
    "\n",
    "        # Convert results to DataFrame and save\n",
    "        df_results = pd.DataFrame(results)\n",
    "\n",
    "        var_attr_is_tuple = False\n",
    "\n",
    "        # Handle tuple-based attributes consistently\n",
    "        if isinstance(df_results[varying_attribute].iloc[0], tuple):\n",
    "            var_attr_is_tuple = True\n",
    "            df_results[varying_attribute] = df_results[varying_attribute].apply(str)\n",
    "            varying_attribute_for_plot = varying_attribute\n",
    "        else:\n",
    "            varying_attribute_for_plot = varying_attribute\n",
    "\n",
    "        df_results = df_results.sort_values(varying_attribute_for_plot, ascending=True)\n",
    "\n",
    "        if experiment_settings.get(\"save_plots_per_run\", False):\n",
    "            plot_and_save_all_metrics(\n",
    "                df_results=df_results,\n",
    "                sampler_colors=sampler_colors,\n",
    "                varying_attribute=varying_attribute,\n",
    "                varying_attribute_for_plot=varying_attribute_for_plot,\n",
    "                results_folder=results_folder,\n",
    "                plots_folder=plots_folder,\n",
    "                run_id=run_id,\n",
    "                config_descr=config_descr\n",
    "            )\n",
    "\n",
    "        df_all_runs.append(df_results)\n",
    "\n",
    "        # Now increments the TQDM progress bar if it's provided\n",
    "        if progress_bar is not None:\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    logger.info(\"All runs completed successfully.\")\n",
    "\n",
    "    # ===== GLOBAL RESULTS FOLDER =====\n",
    "    global_folder = os.path.join(config_folder, \"global_results\")\n",
    "    global_results_folder = os.path.join(global_folder, \"results\")\n",
    "    global_plots_folder = os.path.join(global_folder, \"plots\")\n",
    "    create_directories(global_folder, global_results_folder, global_plots_folder)\n",
    "    \n",
    "    # Combine all results into a single data frame \n",
    "    df_all_runs = pd.concat(df_all_runs, ignore_index=True)\n",
    "\n",
    "    if var_attr_is_tuple:\n",
    "        iid_ref_stats_dict = {str(k): v for k, v in iid_ref_stats_dict.items()}\n",
    "\n",
    "    compute_and_save_global_metrics(\n",
    "        df_all_runs=df_all_runs,\n",
    "        sampler_colors=sampler_colors,\n",
    "        varying_attribute=varying_attribute,\n",
    "        runs=runs,\n",
    "        config_descr=config_descr,\n",
    "        global_results_folder=global_results_folder,\n",
    "        global_plots_folder=global_plots_folder,\n",
    "        iid_ref_stats_dict=iid_ref_stats_dict\n",
    "    )\n",
    "\n",
    "    logger.info(f\"===== Config {config_descr} completed successfully. =====\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# posterior_type = \"Cauchy\", \"Beta\", \"Normal\", \"StudentT\", \"Laplace\", \"SkewstudentT\"\n",
    "# varying_attribute = \"num_samples\", \"num_chains\", \"init_scheme\" or posterior specific attribute\n",
    "# mixture specific attributes = \"component_types\", component_params\", \"weights\"\n",
    "# cauchy specific attributes = \"alpha\", \"beta\"\n",
    "# beta specific attributes = \"alpha\", \"beta\"\n",
    "# normal specific attributes = \"mu\", \"sigma\"\n",
    "# student_t specific attributes = \"nu\", \"mu\", \"sigma\"\n",
    "# laplace specific attributes = \"mu\", \"b\"\n",
    "# skewed_student_t specific attributes = \"a\", \"b\", \"mu\", \"sigma\"\n",
    "# all but the varying attribute must be fixed and present in the config\n",
    "\n",
    "\n",
    "# default attributes\n",
    "default_num_samples = 2000\n",
    "default_num_chains = 4\n",
    "default_base_random_seed = 420\n",
    "default_runs = 3\n",
    "default_init_scheme = \"thesis_scheme\"\n",
    "\n",
    "\n",
    "test = [\n",
    "    {\n",
    "        \"config_descr\": \"test\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [(5, 5), (10, -10), (-10, 10)],\n",
    "        \"varying_component\": 1,\n",
    "        \"component_types\": [\"MvNormal\", \"MvNormal\", \"MvNormal\"],\n",
    "        \"component_params\": [\n",
    "                {\"mu\": [0, 0], \"cov\": [[1, 0.5], [0.5, 1]]},  \n",
    "                {\"cov\": [[2, 0.3], [0.3, 2]]},  \n",
    "                {\"mu\": [-10, -10], \"cov\": [[1, -0.2], [-0.2, 1]]}  \n",
    "        ],\n",
    "        \"weights\": [0.3, 0.4, 0.3],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "]\n",
    "\n",
    "cauchy = [\n",
    "    {\n",
    "        \"config_descr\": \"Cauchy\",\n",
    "        \"posterior_type\": \"Cauchy\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"alpha\",\n",
    "        \"varying_values\": [2],\n",
    "        \"beta\": 2,\n",
    "        \"init_scheme\": default_init_scheme,\n",
    "    }\n",
    "]\n",
    "\n",
    "unimodal = [\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Normal\",\n",
    "        \"posterior_type\": \"Normal\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [2,3,4],\n",
    "        \"sigma\": 1,\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "temp = [\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Student_t\",\n",
    "        \"posterior_type\": \"StudentT\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\":  default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"nu\",\n",
    "        \"varying_values\": [1, 2, 3, 5, 30],\n",
    "        \"mu\": 0,\n",
    "        \"sigma\": 1,\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Laplace_test\",\n",
    "        \"posterior_type\": \"Laplace\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"b\",\n",
    "        \"varying_values\": [0.5, 1, 2, 5],\n",
    "        \"mu\": 0,\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "high_dim_and_correlated = [\n",
    "\n",
    "        {\n",
    "        \"config_descr\": \"Mv_normal_3d_high_corr\",\n",
    "        \"posterior_type\": \"MvNormal\",\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"runs\": default_runs,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [\n",
    "            (-5, 0, 5),\n",
    "            (0, 0, 0),\n",
    "            (-10, 20, -30),\n",
    "            (50, -50, 100)\n",
    "        ],\n",
    "        \"cov\": [[1, 0.9, 0.85], \n",
    "              [0.9, 1, 0.88], \n",
    "              [0.85, 0.88, 1]],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Mv_normal_2d_low_corr\",\n",
    "        \"posterior_type\": \"MvNormal\",\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"runs\": default_runs,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [\n",
    "            (0, 0),\n",
    "            (-10, 10),\n",
    "            (20, -20),\n",
    "            (50, -50)\n",
    "        ],\n",
    "        \"cov\": [[1, 0.1], [0.1, 1]],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Mv_normal_2d_high_corr\",\n",
    "        \"posterior_type\": \"MvNormal\",\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"runs\": default_runs,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [\n",
    "            (0, 0),\n",
    "            (-10, 10),\n",
    "            (20, -20),\n",
    "            (50, -50)\n",
    "        ],\n",
    "        \"cov\": [[1, 0.95], [0.95, 1]],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "\n",
    "    {\n",
    "        \"config_descr\": \"Mv_normal_3d_low_corr\",\n",
    "        \"posterior_type\": \"MvNormal\",\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"runs\": default_runs,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [\n",
    "            (-5, 0, 5),\n",
    "            (0, 0, 0),\n",
    "            (-10, 20, -30),\n",
    "            (50, -50, 100)\n",
    "        ],\n",
    "        \"cov\": [[1, 0.2, 0.1], \n",
    "                [0.2, 1, 0.15], \n",
    "                [0.1, 0.15, 1]],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "]\n",
    "\n",
    "multimodal = [\n",
    "\n",
    "        {\n",
    "        \"config_descr\": \"Mv_normal_2d_mixture_3_comp\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [(5, 5), (10, -10), (-10, 10)],\n",
    "        \"varying_component\": 1,\n",
    "        \"component_types\": [\"MvNormal\", \"MvNormal\", \"MvNormal\"],\n",
    "        \"component_params\": [\n",
    "                {\"mu\": [0, 0], \"cov\": [[1, 0.5], [0.5, 1]]},  \n",
    "                {\"mu\": [0, 0], \"cov\": [[2, 0.3], [0.3, 2]]},  \n",
    "                {\"mu\": [-10, -10], \"cov\": [[1, -0.2], [-0.2, 1]]}  \n",
    "        ],\n",
    "        \"weights\": [0.3, 0.4, 0.3],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "]\n",
    "\n",
    "easy_2d_multimodal = [\n",
    "    {\n",
    "        \"config_descr\": \"Mixture_of_Normal_2d\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [(3, 3), (10, 10)],\n",
    "        \"varying_component\": 1,\n",
    "        \"component_types\": [\"MvNormal\", \"MvNormal\"],\n",
    "        \"component_params\": [\n",
    "            {\"mu\": [0, 0], \"cov\": [[1, 0.5], [0.5, 1]]},\n",
    "            {\"cov\": [[2, 0.3], [0.3, 2]]}\n",
    "        ],\n",
    "        \"weights\": [0.7, 0.3],\n",
    "        \"init_scheme\": \"all_in_middle\"\n",
    "    },\n",
    "\n",
    "        {   \n",
    "        \"config_descr\": \"Normal_and_student_t\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"component_types\": [\"Normal\", \"StudentT\"],\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"nu\",\n",
    "        \"varying_values\": [1, 2],\n",
    "        \"varying_component\": 1,\n",
    "        \"component_params\": [{\"mu\": 0, \"sigma\": 1}, {\"nu\": 3, \"mu\": 10, \"sigma\": 2}],\n",
    "        \"weights\": [0.6, 0.4],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "]\n",
    "\n",
    "easy_multimodal = [\n",
    "    {\n",
    "        \"config_descr\": \"Mixture_of_Normal\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [0],\n",
    "        \"varying_component\": 0,\n",
    "        \"component_types\": [\"Normal\", \"Normal\"],\n",
    "        \"component_params\": [\n",
    "            {\"mu\": 0, \"sigma\": 1},\n",
    "            {\"mu\": 10, \"sigma\": 1}\n",
    "        ],\n",
    "        \"weights\": [0.7, 0.3],\n",
    "        \"init_scheme\": \"all_in_middle\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "difficult_geometries = [\n",
    "\n",
    "        {\n",
    "        \"config_descr\": \"SkewStudentT\",\n",
    "        \"posterior_type\": \"SkewStudentT\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"varying_attribute\": \"a\",\n",
    "        \"varying_values\": [1, 2, 3, 5],\n",
    "        \"b\": 1,\n",
    "        \"mu\": 0,\n",
    "        \"sigma\": 1,\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    },\n",
    "    {\n",
    "        \"config_descr\": \"Mixture_of_SkewStudentT\",\n",
    "        \"posterior_type\": \"Mixture\",\n",
    "        \"runs\": default_runs,\n",
    "        \"num_chains\": default_num_chains,\n",
    "        \"base_random_seed\": default_base_random_seed,\n",
    "        \"num_samples\": default_num_samples,\n",
    "        \"varying_attribute\": \"mu\",\n",
    "        \"varying_values\": [0, 3, 6, 10],\n",
    "        \"varying_component\": 0,\n",
    "        \"component_types\": [\"SkewStudentT\", \"SkewStudentT\"],\n",
    "        \"component_params\": [\n",
    "            {\"a\": 3, \"b\": 1, \"sigma\": 1},\n",
    "            {\"a\": 9, \"b\": 3, \"mu\": 3, \"sigma\": 4}\n",
    "        ],\n",
    "        \"weights\": [0.5, 0.5],\n",
    "        \"init_scheme\": default_init_scheme\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def validate_config(config):\n",
    "    \"\"\"Checks if the config correctly defines one varying attribute and all other attributes are fixed.\"\"\"\n",
    "    \n",
    "    REQUIRED_ATTRIBUTES = {\n",
    "    \"config_descr\",\n",
    "    \"posterior_type\",\n",
    "    \"runs\",\n",
    "    \"num_samples\",\n",
    "    \"num_chains\",\n",
    "    \"varying_attribute\",\n",
    "    \"varying_values\",\n",
    "    }\n",
    "\n",
    "    # Posterior-specific required attributes\n",
    "    POSTERIOR_ATTRIBUTES = {\n",
    "        \"Cauchy\": {\"alpha\", \"beta\"},\n",
    "        \"Beta\": {\"a\", \"b\"},\n",
    "        \"Normal\": {\"mu\", \"sigma\"},\n",
    "        \"StudentT\": {\"nu\", \"mu\", \"sigma\"},\n",
    "        \"Laplace\": {\"mu\", \"b\"},\n",
    "        \"SkewStudentT\": {\"a\", \"b\", \"mu\", \"sigma\"},\n",
    "        \"Mixture\": {\"component_types\", \"component_params\", \"weights\"},\n",
    "        \"MvNormal\": {\"mu\", \"cov\"},\n",
    "        \"Custom\": {\"logp_func\"}\n",
    "    }\n",
    "\n",
    "    OPTIONAL_ATTRIBUTES = {\"base_random_seed\", \"init_scheme\", \"varying_component\"}\n",
    "\n",
    "    if \"config_descr\" not in config:\n",
    "        raise ValueError(\"Config is missing 'config_descr'.\")\n",
    "    \n",
    "    config_descr = config[\"config_descr\"]\n",
    "\n",
    "    if \"varying_attribute\" not in config:\n",
    "        raise ValueError(f\"Config '{config_descr}' is missing 'varying_attribute'.\")\n",
    "    \n",
    "    varying_attr = config[\"varying_attribute\"]\n",
    "\n",
    "    # Ensure all required attributes are present\n",
    "    missing_attrs = REQUIRED_ATTRIBUTES - config.keys() - {varying_attr}\n",
    "\n",
    "    if missing_attrs:\n",
    "        raise ValueError(f\"Config '{config_descr}' is missing required attributes: {missing_attrs}.\")\n",
    "    \n",
    "    posterior_type = config[\"posterior_type\"]\n",
    "\n",
    "    if posterior_type not in POSTERIOR_ATTRIBUTES:\n",
    "        raise ValueError(f\"Config '{config_descr}' has an invalid 'posterior_type': '{posterior_type}'.\")\n",
    "\n",
    "    if posterior_type == \"Mixture\" and \"varying_component\" in config:\n",
    "        varying_index = config[\"varying_component\"]\n",
    "        varying_component = config[\"component_types\"][varying_index]\n",
    "        all_valid_attributes = REQUIRED_ATTRIBUTES.union(POSTERIOR_ATTRIBUTES[posterior_type], POSTERIOR_ATTRIBUTES[varying_component], OPTIONAL_ATTRIBUTES)\n",
    "        \n",
    "    else:\n",
    "        # Ensure varying_attribute is a recognized attribute\n",
    "        all_valid_attributes = REQUIRED_ATTRIBUTES.union(POSTERIOR_ATTRIBUTES[posterior_type], OPTIONAL_ATTRIBUTES)\n",
    "\n",
    "    if varying_attr not in all_valid_attributes:\n",
    "        raise ValueError(f\"Config '{config_descr}' has an invalid 'varying_attribute': '{varying_attr}'.\")\n",
    "        \n",
    "    if posterior_type == \"Mixture\" and varying_attr not in (\"num_samples\", \"num_chains\", \"init_scheme\"):\n",
    "        if \"varying_component\" not in config:\n",
    "            raise ValueError(\n",
    "                f\"Config '{config_descr}' must have 'varying_component' defined \"\n",
    "                f\"when varying '{varying_attr}' for a Mixture.\"\n",
    "            )\n",
    "        \n",
    "    vc = config.get(\"varying_component\")    \n",
    "    if vc is not None and not (0 <= vc < len(config[\"component_types\"])):\n",
    "        raise ValueError(\n",
    "            f\"Config '{config_descr}' has invalid 'varying_component' index {vc}, \"\n",
    "            f\"but 'component_types' has length {len(config['component_types'])}.\"\n",
    "        )\n",
    "    \n",
    "    VALID_INIT_SCHEMES = {\"equal_per_mode\",\"all_in_middle\", \"all_near_mode\", \"thesis_scheme\"} \n",
    "\n",
    "    if \"init_scheme\" in config:\n",
    "        if config[\"init_scheme\"] not in VALID_INIT_SCHEMES and not config[\"init_scheme\"].startswith(\"all_near_mode_\"):\n",
    "            raise ValueError(\n",
    "                f\"Config '{config_descr}' has invalid 'init_scheme': \"\n",
    "                f\"'{config['init_scheme']}'. Must be one of {VALID_INIT_SCHEMES} \"\n",
    "                \"or 'all_near_mode_<int>'.\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of runs: 3\n",
      "All configurations are valid. Starting experiments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:   0%|          | 0/3 [00:00<?, ?it/s]The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:51:45,003 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.8659156202037976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:52:29,762 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.763903246492107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:53:04,331 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 1.466746333482559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:53:20,261 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.7454846683242111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:53:37,459 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.7329131984898911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:53:48,827 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 14.996476643173146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:53:59,385 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.7479467443234282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:54:19,615 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.5318517205063962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:54:37,476 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  33%|███▎      | 1/3 [03:24<06:48, 204.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-hat for sampler DEMetro: 1.0894281133296944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:54:46,639 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.2445877681693704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:54:58,948 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.044070065361121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:09,372 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 1.5651547404424369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:16,609 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.7444561352855885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:27,899 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.738616294923158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:37,971 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 1.0173792001296935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:44,998 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.7425198723430542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:55:56,012 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 2.0982568545934015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:04,975 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress:  67%|██████▋   | 2/3 [04:50<02:14, 134.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-hat for sampler DEMetro: 1.0524341050825123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:11,881 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.258024539010437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:22,035 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.1996844892988066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:31,350 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 1.0415927015903634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:38,157 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 2.429180650163072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:56:48,631 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 2.410558676650063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:01,933 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler DEMetro: 1.0233534215682505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:11,545 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler Metro: 1.5359256675385882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:25,875 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "R-hat for sampler HMC: 1.5300189421612937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 00:57:37,231 - ERROR - The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress: 100%|██████████| 3/3 [06:22<00:00, 115.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-hat for sampler DEMetro: 1.1525380950807647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Total experiment progress: 100%|██████████| 3/3 [06:24<00:00, 128.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "Experiment Summary\n",
      "============================\n",
      "Started at:               2025-05-13 00:51:14\n",
      "Finished at:              2025-05-13 00:57:38\n",
      "Total duration:           0h 6m 24.6s\n",
      "Output folder:            exp_test_1\n",
      "Output folder size:       1.3 MB\n",
      "Total configurations:     1\n",
      "Successful runs:          1\n",
      "Failed configurations:    0\n",
      "Summary saved to: exp_test_1/summary.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the list of experiments to run\n",
    "#experiments = [cauchy, unimodal, high_dim_and_correlated, multimodal, difficult_geometries]\n",
    "\n",
    "experiments = [test]\n",
    "experiment_name = \"test_1\"\n",
    "\n",
    "# Define the root directory for all experiments\n",
    "experiment_root_folder = f\"exp_{experiment_name}\"\n",
    "\n",
    "# Check if the folder already exists\n",
    "if os.path.exists(experiment_root_folder):\n",
    "    user_input = input(\n",
    "        f\"Folder '{experiment_root_folder}' already exists and will be overwritten.\\n\"\n",
    "        \"Do you want to continue? (yes/no): \"\n",
    "    ).strip().lower()\n",
    "\n",
    "    if user_input not in [\"yes\", \"y\"]:\n",
    "        print(\"Operation aborted. No files were deleted.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    shutil.rmtree(experiment_root_folder)\n",
    "\n",
    "create_directories(experiment_root_folder)\n",
    "\n",
    "experiment_settings = {\n",
    "    \"save_plots_per_run\": False,                                                     # if True, save plots for each run \n",
    "    \"save_traces\": False,                                                           # if True, save traces to NetCDF files\n",
    "    \"trace_plots\": \"first_run_only\",                                                # \"none\", \"first_run_only\", \"all\" \n",
    "    \"plot_traces_in_notebook\": False,                                               # if True, plot traces in the notebook\n",
    "    \"samplers\": [\"Metro\", \"HMC\", \"DEMetro\"],                                        # options: \"Metro\", \"HMC\", \"DEMetro\", \"DEMetro_Z\", \"Slice\"\n",
    "    \"logging_level\": \"ERROR\",                                                       # \"DEBUG\", \"INFO\", \"ERROR\"\n",
    "    \"plot_first_sample\": True,                                                      # if True, plot the first sample of each chain\n",
    "}\n",
    "\n",
    "failed_configs = []\n",
    "start_time = time.time()\n",
    "start_dt = datetime.now()\n",
    "\n",
    "# Validate all configurations before running the experiments\n",
    "for exp in experiments:\n",
    "    for config in exp:\n",
    "        validate_config(config)\n",
    "\n",
    "\n",
    "total_runs = sum(config[\"runs\"] for exp in experiments for config in exp)\n",
    "print(f\"Total number of runs: {total_runs}\")\n",
    "\n",
    "print(\"All configurations are valid. Starting experiments...\")\n",
    "with tqdm(total=total_runs, desc=\"Total experiment progress\") as pbar:\n",
    "    for exp in experiments:\n",
    "        for config in exp:\n",
    "            try:\n",
    "                run_experiment(\n",
    "                    experiment_settings,\n",
    "                    posterior_type=config[\"posterior_type\"],\n",
    "                    config_descr=config[\"config_descr\"],\n",
    "                    runs=config[\"runs\"],\n",
    "                    varying_attribute=config[\"varying_attribute\"],\n",
    "                    varying_values=config[\"varying_values\"],\n",
    "                    init_scheme=\"varies\" if config[\"varying_attribute\"] == \"init_scheme\" else config.get(\"init_scheme\", None),\n",
    "                    num_samples=\"varies\" if config[\"varying_attribute\"] == \"num_samples\" else config[\"num_samples\"],\n",
    "                    num_chains=\"varies\" if config[\"varying_attribute\"] == \"num_chains\" else config[\"num_chains\"],\n",
    "                    base_random_seed=config.get(\"base_random_seed\", None),\n",
    "                    progress_bar=pbar, \n",
    "                    **{k: v for k, v in config.items() if k not in [\n",
    "                        \"config_descr\", \"runs\", \"varying_attribute\", \"varying_values\", \n",
    "                        \"num_samples\", \"num_chains\", \"init_scheme\", \n",
    "                        \"base_random_seed\", \"posterior_type\"\n",
    "                    ]}  # Pass remaining keys as posterior_kwargs\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error in config '{config['config_descr']}': {e}\")\n",
    "                traceback.print_exc()\n",
    "                failed_configs.append((config['config_descr'], str(e)))\n",
    "                \n",
    "\n",
    "end_time = time.time()\n",
    "end_dt = datetime.now()\n",
    "duration = end_time - start_time\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = round(duration % 60, 1)\n",
    "\n",
    "def get_folder_size(path='.'):\n",
    "    \"\"\"Compute total size of all files in directory.\"\"\"\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if os.path.isfile(fp):\n",
    "                total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "# Prepare the summary text\n",
    "size_bytes = get_folder_size(experiment_root_folder)\n",
    "total_configs = sum(len(exp) for exp in experiments)\n",
    "\n",
    "summary_lines = [\n",
    "    \"\\n============================\",\n",
    "    \"Experiment Summary\",\n",
    "    \"============================\",\n",
    "    f\"Started at:               {start_dt.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"Finished at:              {end_dt.strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    f\"Total duration:           {hours}h {minutes}m {seconds}s\",\n",
    "    f\"Output folder:            {experiment_root_folder}\",\n",
    "    f\"Output folder size:       {humanize.naturalsize(size_bytes)}\",\n",
    "    f\"Total configurations:     {total_configs}\",\n",
    "    f\"Successful runs:          {total_configs - len(failed_configs)}\",\n",
    "    f\"Failed configurations:    {len(failed_configs)}\"\n",
    "]\n",
    "\n",
    "if failed_configs:\n",
    "    summary_lines.append(\"\\n Failed Configurations:\")\n",
    "    for cfg, msg in failed_configs:\n",
    "        summary_lines.append(f\" - {cfg}: {msg}\")\n",
    "\n",
    "# Print to console\n",
    "print(\"\\n\".join(summary_lines))\n",
    "\n",
    "# Also save to summary.txt\n",
    "summary_path = os.path.join(experiment_root_folder, \"summary.txt\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(summary_lines))\n",
    "\n",
    "print(f\"Summary saved to: {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_immo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
